# Data Analysis: Tidy to Scored

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

## Overview

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_dataanalysis.png"))
library(dplyr)
library(tidyr)
library(knitr)
library(englelab)
```

**Data Analysis** requires many steps and decisions that need to be made along the way. Often times the goal is to create a single data file that is ready for statistical analysis. But before we can get to this single data file a couple of steps are involved.

1. Create a scored data file for each task

    At this stage we also remove any subjects that are suspect or have too poor of performance, and remove univariate outliers.

2. Merge the scored data files into a single data file

    At this stage we also can create composite factors if needed.
    
In this chapter, I will use the visual arrays task as an example of how to do step 1.

## Score script template

The R script template to *score* raw data files was explained in Chapter 9. 

In this Chapter we will walk through a script to score a raw data file from a visual arrays task.

This is the R script template downloaded by: `workflow::template(type = "score")`

```{r eval = FALSE}
#### Setup ####
## Load Packages
library(here)
library(readr)
library(dplyr)

## Set Import/Output Directories
import_dir <- "Data Files/Raw Data"
output_dir <- "Data Files/Scored Data"

## Set Import/Output Filenames
task <- "taskname"
import_file <- paste(task, "raw.csv", sep = "_")
output_file <- paste(task, "Scores.csv", sep = "_")

## Set Data Cleaning Params

###############

#### Import ####
data_import <- read_csv(here(import_dir, import_file))
################

#### Score Data ####
data_scores <- data_import %>%
  filter() %>%
  group_by() %>%
  summarise()
####################

#### Clean Data ####

####################

#### Calculate Reliability ####

###############################

#### Output ####
write_csv(data_scores, here(output_dir, output_file))
################

rm(list=ls())
```

## Setup

- Load packages

    Any packages required for this script are loaded at the top. For this task we will need the `here`, `readr`, `dplyr` packages in addition to `tidyr` and `knitr`. Go ahead and add `library(tidyr)` and `library(knitr)` to this section.
    
- Set Import/Output Directories

    To make this example easier you will not have to actually import/output any files.
    
- Set Import/Output Filenames

    The only line we need to change here is the `task <- "taskname"` to `task <- "VAorient_S"`.
    
- Set Data Cleaning Params

    In this section of the script we can set certain data cleaning criteria to variables. This makes it easy to see what data cleaning criteria were used right at the top of the script rather than having to read through and try to interpret the script.
    
    For the visual arrays task we should remove subjects who had low accuracy scores - lets say those with accuracy lower than 3.5 SDs
    
    Add `acc_criteria <- -3.5` to this section of the script.
    
    Optionally it would be a good idea to add a comment about what this criteria is for.
    
    
## Import

This section can stay exactly the same. As long as you are using these templates this line of code can always remain untouched.

## Data Scoring

This is the meat of the script, where the action happens. It will also be different for every task - obviously. We will walk through each line of code for this section using the visual arrays task as an example. 

It will be easier if you have an example data set so you can follow along. Type the following lines of code in your console.

```{r eval = TRUE}
library(englelab)
data_import <- visualarrays_tidy
```

Now you should see the object (data frame) `data_import` in your environment window. Click on the data frame to view it. This is a *tidy* raw data set from the visual arrays task. You are starting as though you had already imported the data file and ready to score the visual arrays task.

### `filter()`

Notice how the data set has both `practice` and `real` trials:

```{r eval = TRUE}
unique(visualarrays_tidy$TrialProc)
```

We need to get rid of `practice` trials.

```{r}
data_import <- visualarrays_tidy %>%
  filter(TrialProc == "real")
```

### `group_by()` and `summarise()`

View the data frame. Notice how there are only `real` trials. The next step is to score the visual arrays data. 

There are set sizes of 5 and 7 in this visual arrays data:

```{r}
unique(data_import$SetSize)
```

Therefore, there will be two resulting *k* scores. We can average the two *k* scores to get one final *k* score for the visual arrays task. This means we need to calculate a *k* score for each subject and each set size. First let's talk a little more about how the visual arrays task is scored.

----

The visual arrays task is scored with the following formula.

*k* = N * (P(H) + P(CR) - 1)

Where N is the set size, P(H) is the probability of hits, and P(CR) is the probability of correct rejections.

To understand how to calculate P(H) and P(CR) the figure below will be useful. 

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics(rep("images/visualarrays.png"))
```

Any given trial can have a correct response as either "same" or "different" - not both so these probabilities are independent. However, on a "same" trial the response made can either be a correct rejection ("same" response made) or a false alarm ("different" response made). The probability of these responses are dependent such that: P(CR) = 1 - P(FA) and P(FA) = 1 - P(CR). If you rearrange these you get that 1 = P(CR) + P(FA).

We need to calculate P(CR), and we can do this by writing this formula in terms of number of trials rather than probability.

1 = (CR.n + FA.n) / Same.n, because CR.n + FA.n = Same.n

Therefore, the probability of a CR is the proportion of CR.n to Same.n

P(CR) = CR.n / Same.n, P(CR) = CR.n / (CR.n + FA.n)

The same logic can be applied to Hits and Misses for calculating P(H)

P(H) = H.n / Different.n, P(H) = H.n / (H.n + M.n)

----

This means we need to know the total number of trials that were correct rejections, false alarms, misses, and hits. We need to calculate this separately for each subject and each set size.

We can do this using `group_by()` and `summarise()`

View the data frame. You will notice there are columns for `CorrectRejection`, `FalseAlarm`, `Miss`, and `Hit`. The values in these columns are `1`'s or `0`'s. If a row has a value of `1` on the `CorrectionRejection` column that means the response was a Correct Rejection. We can simply calculate the number of Correct Rejection (and the other response options) by summing those columns by subject and set size.

```{r eval = TRUE}
data_scores <- data_import %>%
  group_by(Subject, SetSize) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup()
```

View the data frame. You can see it is now reduced to two rows per subject, one for each set size. We also have columns for the number of trials on correct rejections, false alarms, misses, and hits. Now we simply need to calculate P(CR) and P(H) using the formulas above.

```{r eval = TRUE}
data_scores <- data_import %>%
  group_by(Subject, SetSize) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n))
```

Now we can calculate *k* for each set size using the *k* formula

*k* = N * (P(H) + P(CR) - 1)

Where N is the set size.

```{r eval = FALSE}
data_scores <- data_import %>%
  group_by(Subject, SetSize) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1))
```

----

Let's take a moment to further understand this formula and how it is interpreted as a capacity score. If a subject gets a trial correct, either a correct rejection or a hit, it is assumed that they retained all items in memory from the target array (a BIG assumption). For instance, if the set size is 5 then on that trial they would assume to have a storage capacity of 5. 

P(H) and P(CR) are the proportion of correct trials on "different" and "same" trials, respectively. If a subject's true capacity or *k* is 5 then they would be expected to make a correct response on all trials or nearly all trials. Therefore, P(H) and P(CR) would both equal 1. 

Let's rearrange the *k* formula as:

*k* = (N * P(H)) + (N * P(CR)) - N

With this example we would get

*k* = (5 * 1) + (5 * 1) - 5 = **5**

What about if a subject was completely guessing, a true capacity of 0:

*k* = (5 * .5) + (5 * .5) - 5 = **0**

P(H) and P(CR) are at .5 when guessing

What if a subject's true capacity was 3:

*k* = (5 * .8) + (5 * .8) - 5 = **3**,

there are actually more than one combination of P(H) and P(CR) values to result in a *k* = 3, as long as (P(H) + P(CR)) / 2 = .8.

----

In the end, we want a data frame that has only **one** row per subject and columns corresponding to *k* scores on set size 5 and 7. In other words we need to make this data frame **wider**.

To make the data frame wider we will use the `pivot_wider()` function from the `tidyr` package. 


```{r eval = FALSE}
data_scores <- data_import %>%
  group_by(Subject, SetSize) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1)) %>%
  pivot_wider(id_cols = "Subject",
              names_from = "SetSize",
              values_from = c("CR", "H", "k"))
```

View the data frame. Now there is only one row per subject with k scores for each set size spread across columns. All that is left to do is calculate an **average** *k* score

```{r}
data_scores <- data_import %>%
  group_by(Subject, SetSize) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1)) %>%
  pivot_wider(id_cols = "Subject",
              names_from = "SetSize",
              values_from = c("CR", "H", "k")) %>%
  mutate(VA_k = (k_5 + k_7) / 2)
```

Yay! We calculated a single *k* score for the visual arrays task! We are done!... or are we? No we are not. There are a few more things we can do to make further analyses and reporting easier. This will also allow us to create a **log** of the data cleaning procedures we employed.

## Data Cleaning

### Remove problematic subjects

First, it would be a good idea to remove problematic subjects.

Problematic subjects are usually those that are performing so poorly, below chance, that they either are not really doing the task or completely misunderstood the instructions. 

In the past we have used a sort of blanket 3.5 SDs below mean accuracy - however I am not sure that is the best approach. Also for the sake of this example there are no subjects below 3.5 SDs, therefore, let's set it at **2.0 SDs**.

Before we get into the steps to perform, let's set a data cleaning parameter at the top of the script in the **Setup** section.

```{r}
## Set Data Cleaning Params
acc_criteria <- -2    # Remove subjects with accuracy 2 SDs below the mean
###############
```

We set `acc_criteria` to `-2` because we will want to remove subjects that had accuracy **2 SDs below the mean**.

The steps for this are pretty simple:

1. Calculate the mean accuracy for each subject - `summarise()`

2. Create z-scores on mean accuracy - `mutate()` and `scale()`

3. Create a data frame that contains only those that fall below the 2 SDs - `filter()`

4. Remove those subjects from the scored data file we just created - `filter()`

```{r}
data_remove <- data_import %>%
  group_by(Subject) %>% 
  summarise(Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(ACC.mean_z = scale(Accuracy.mean, center = TRUE, scale = TRUE)[,1]) %>%
  filter(ACC.mean_z < acc_criteria)

data_scores <- filter(data_scores, !(Subject %in% data_remove$Subject)) 
```

What we did was create a separate data frame to hold subjects that we want to remove because they are problematic. And then we simply applied a filter on the original data frame to keep only those subjects that are not present in the removed data frame.

### Remove univariate outliers

Next, we can remove univariate outliers at this stage too. If we remove it at a later stage it is harder to create a **log** of how many outliers were removed for this task. 

Before we get into the steps to perform, let's set another data cleaning parameter at the top of the script in the **Setup** section.

```{r}
## Set Data Cleaning Params
acc_criteria <- -2         # Remove subjects with accuracy 2 SDs below the mean
outlier_criteria <- 2.0    # Trim outliers 2 SDs from the mean
###############
```

Here I am deciding to use an outlier cutoff on `VA_k` scores at **+/- 2 SDs from the mean**. This is too small of a cutoff, but I am using it for the sake of this example because there are no subjects at a higher cutoff.

The steps to do this are real simple:

1. Calculate z-scores on the `VA_k` score - `mutate()` and `scale()`

2. Create a data frame that contains only those that are considered outliers - `filter()`

3. Remove those subjects from the scored data file we just created - `filter()`

```{r}
data_outliers <- data_scores %>%
  mutate(VA_k_z = scale(VA_k, center = TRUE, scale = TRUE)) %>%
  filter(VA_k_z >= outlier_criteria | VA_k_z <= -1 * outlier_criteria)

data_scores <- filter(data_scores, !(Subject %in% data_outliers$Subject)) 
```

Great! Now we have cleaned up the data a bit. At the end of the script we will go over how to write code to create a **log** of this data cleaning. 

## Calculate reliability

Finally, we might as well go ahead and calculate reliability here seeing as how we have both the raw and cleaned data set loaded in the environment already. To calculate split-half reliability for this task we need to split on even and odd items for both set-size 5 and set-size 7. Then we can calculate a visual arrays *k* score for even items and for odd items. 

We basically just need to repeat the process for calculating k scores but first split between even and odd items.

First, let's make sure we are only calculating reliability from the raw data on those Subjects that did not get removed in the previous steps (Subjects that have made it through data cleaning).

```{r eval = FALSE}
reliability <- data_import %>%
  filter(Subject %in% data_scores$Subject)
```

This next code block way is how to split trials into "even" and "odd". In this case we want to split separately on each set size so we group by both Subject and SetSize. 

```{r eval = FALSE}
reliability <- data_import %>%
  filter(Subject %in% data_scores$Subject) %>%
  group_by(Subject, SetSize) %>%
  mutate(Index = row_number(),
         Split = ifelse(Index %% 2, "odd", "even"))
```

Since the data frame is split by Subject and SetSize, `row_number()` provides the occurrence of that trial within the set size (e.g. 1 = the first occurrence of set size 5 for a given subject, 2 = the second occurrence of set size 5 for that subject, and so on). This information is then stored in the column `Index`. 

We can then evaluate whether the value in `Index` is divisible by two. `Index %% 2` will return `TRUE` if there is a remainder (if it is not divisible by two) and `FALSE` if there is no remainder (if it is divisible by two). We can then set the value of `Split` to "odd" when it evaluates as `TRUE` and "even" when it evaluates as `FALSE`. Now we have a column specifying whether a trial is an even or odd trial within each set size for a given subject.

Now we just need to calculate `k` scores like we did above. Copy and paste the previous code up until `pivot_wider()`

```{r eval = FALSE}
reliability <- data_import %>%
  filter(Subject %in% data_scores$Subject) %>%
  group_by(Subject, SetSize) %>%
  mutate(Index = row_number(),
         Split = ifelse(Index %% 2, "odd", "even")) %>%
  group_by(Subject, SetSize, Split) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1))
```

Take a look at what the data frame looks like `View(reliability)`

When we pivot_wider(), we need to make sure to also split on the `Split` column in addition to the `SetSize` column. Then we can calculate a `VA_k` score for both even and odd split-halves.

```{r eval = FALSE}
reliability <- data_import %>%
  filter(Subject %in% data_scores$Subject) %>%
  group_by(Subject, SetSize) %>%
  mutate(Index = row_number(),
         Split = ifelse(Index %% 2, "odd", "even")) %>%
  group_by(Subject, SetSize, Split) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1)) %>%
  pivot_wider(id_cols = "Subject",
              names_from = c("SetSize", "Split"),
              values_from = c("CR", "H", "k")) %>%
  mutate(VA_k_even = (k_5_even + k_7_even) / 2,
         VA_k_odd = (k_5_odd + k_7_odd) / 2)
```

Now let's calculate the correlation with a Spearman-Brown Correction using `summarise()` and `mutate()`

```{r}
reliability <- data_import %>%
  filter(Subject %in% data_scores$Subject) %>%
  group_by(Subject, SetSize) %>%
  mutate(Index = row_number(),
         Split = ifelse(Index %% 2, "odd", "even")) %>%
  group_by(Subject, SetSize, Split) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1)) %>%
  pivot_wider(id_cols = "Subject",
              names_from = c("SetSize", "Split"),
              values_from = c("CR", "H", "k")) %>%
  mutate(VA_k_even = (k_5_even + k_7_even) / 2,
         VA_k_odd = (k_5_odd + k_7_odd) / 2) %>%
  summarise(r = cor(VA_k_even, VA_k_odd)) %>%
  mutate(r = (2 * r) / (1 + r))
```

Finally let's add this as a column to the main data frame with the overall `VA_k` score.

```{r}
data_scores$VA_splithalf <- reliability$r
```

Now view the data frame - `View(data_scores)`. You should see an extra column with the split half reliability value. This will make it easier during data analysis to create a table output of the reliability values from every task.

## Output

The final section of the essential parts of the script is to save the *scored* data file. You do not need to change this section at all from the template.

```{r eval = FALSE}
#### Output ####
write_csv(data_scores, here(output_dir, output_file))
################
```

## Create a data cleaning log

**This addition is more optional but it can be nice to have.**

When reporting your findings in a manuscript it may be a good idea to report how many subjects were removed on each task due to being problematic and being an outlier. This creates more transparency for those that may read your manuscript.

It is also a good idea for you to just be aware of how many subjects are being removed. This can also be a good time to check that you didn't do anything completely wrong - like remove way too many subjects!

When we get to the **masterscript** in the next chapter - you will see that there is a line of code to create a *blank* log file - **Data Files/ Scored Data/log.txt**. We need to create this in the masterscript because we will have every **\_score.R** script append information to this log. That way we have one single file with information from all tasks. 

To append information to the log file, we will use a combination of `paste()`, `sink()`, and `cat()` functions. These are all base R functions so no need to load extra libraries - which is always nice.

A key part of this is making it easy to read because the more tasks we have the more cluttered this log file will be. Let's start out by creating the header and footer for this task in the log file.

```{r}
## Append to Log ####
log_header <- paste("===== Visual Arrays: 1_visualarrays_score.R =====\n\n",
                    format(Sys.Date(), "%B %d %Y"), "\n", sep = "")
log_footer <- "==================================================\n"
##################
```

The output in the **log.txt** file would look something like:

```{r comment = ""}
cat(log_header, log_footer, fill = FALSE, sep = "\n")
```

This is nice. The header clearly tells us what task this information is for and what script was ran. The date is when the script was ran - given by `format(Sys.Date(), "%B %d %Y")`. Then there is a clear line segment separating it from anything that comes below.

Note that the notation `"\n"` means *new line* - it is like hitting the enter/return key on the keyboard to move down to the next line. Creating this extra spacing will make it easier to scan the **log.txt** file.

Now let's put information between the header and footer. First, let's report the total number of initial subjects that we had raw data on.

```{r}
## Append to Log ####
log_init_n <- paste("-- ", length(unique(data_import$Subject)),
                    "\t Initial subject count", sep = "")
##################
```

`length(unique(data_import$Subject))` returns the number of unique subject IDs in `data_import`.

Now let's add information about the subjects that were removed because they were problematic and outliers.

```{r}
log_remove <- paste("-- ", length(unique(data_remove$Subject)),
                    "\t Number of problematic subjects removed", sep = "")
log_outlier <- paste("-- ", length(unique(data_outliers$Subject)),
                     "\t Number of outliers removed", sep = "")
```

Now we are evaluating the number of subjects in the `data_remove` and `data_outliers` data frames.

And a line for the final subject count:

```{r}
log_final_n <- paste("-- ", length(unique(data_scores$Subject)),
                     "\t Final subject count", "\n", sep = "")
```

Again, just to see what this will look like run the following line of code in your console:

```{r comment=""}
cat(log_header, log_init_n, log_remove, 
    log_outlier, log_final_n, log_footer, 
    fill = FALSE, sep = "\n")
```

Now let's add two tables - the `data_remove` and `data_outlier` tables. We need to convert the data frame to an easy to read format in a text file. We will use a markdown format. To do so we can use the `kable()` function from the `knitr` package.

```{r}
table_remove <- kable(data_remove, digits = 2, format = "markdown")
table_outlier <- kable(data_outliers, digits = 2, format = "markdown")
```

This is what the table will look like:

```{r comment=""}
cat(table_remove, fill = FALSE, sep = "\n")
```

```{r comment=""}
cat(table_outlier, fill = FALSE, sep = "\n")
```

So we have created the formatting for all the information we want to add. All that is left is to append it to the **log.txt** file. We can use a combination of `sink()` and `cat()`. 

`sink()` basically **opens** and **closes** the requested file. Like you would open and close it on your computer. Except it is doing it behind the scenes. We need to include `sink()` twice, once to open and once to close the file. In between them will be a `cat()` function that will concatenate all the formatted information together.

If we put this all together, we get:

```{r eval = FALSE}
## Append to Log ####
log_header <- paste("===== Visual Arrays: 1_visualarrays_score.R =====\n\n",
                    format(Sys.Date(), "%B %d %Y"), "\n", sep = "")
log_init_n <- paste("-- ", length(unique(data_import$Subject)),
                    "\t Initial subject count", sep = "")
log_remove <- paste("-- ", length(unique(data_remove$Subject)),
                    "\t Number of problematic subjects removed", sep = "")
log_outlier <- paste("-- ", length(unique(data_outliers$Subject)),
                     "\t Number of outliers removed", sep = "")
log_final_n <- paste("-- ", length(unique(data_scores$Subject)),
                     "\t Final subject count", "\n", sep = "")
table_remove <- kable(data_remove, digits = 2, format = "markdown")
table_outlier <- kable(data_outliers, digits = 2, format = "markdown")
log_footer <- "==================================================\n"

sink(file = here("log.txt"), append = FALSE, split = TRUE)
cat(log_header, log_init_n, log_remove, log_outlier, log_final_n, 
    print(table_remove), "\n", print(table_outlier), "\n", log_footer,
    fill = FALSE, sep = "\n")
sink()
##################
```

The resulting **log.txt** file would look like:

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics(rep("images/log.png"))
```

## Final 

The final script would look like. You can see it gets quite long... It would be ideal to add more comments describing how the data is being scored, cleaned, and reliability calculated.

```{r eval = FALSE}
#### Setup ####
## Load Packages
library(here)
library(readr)
library(dplyr)
library(tidyr)
library(knitr)

## Set Import/Output Directories
import_dir <- "Data Files/Raw Data"
output_dir <- "Data Files/Scored Data"
removed_dir <- "Data Files/Scored Data/removed"

## Set Import/Output Filenames
task <- "VisualArrays"
import_file <- paste(task, "raw.csv", sep = "_")
output_file <- paste(task, "Scores.csv", sep = "_")
removed_file <- paste(task, "_removed.csv", sep = "")

## Set Data Cleaning Params
acc_criteria <- -2         # Remove subjects with accuracy 2 SDs below the mean
outlier_criteria <- 2.0    # Trim outliers 2 SDs from the mean
###############

#### Import ####
data_import <- read_csv(here(import_dir, import_file)) %>%
  filter(TrialProc == "real")
################

#### Score Data ####
data_scores <- data_import %>%
  group_by(Subject, SetSize) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1)) %>%
  pivot_wider(id_cols = "Subject",
              names_from = "SetSize",
              values_from = c("CR", "H", "k")) %>%
  mutate(VA_k = (k_5 + k_7) / 2)
####################

#### Clean Data ####
data_remove <- data_import %>%
  group_by(Subject) %>% 
  summarise(Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(ACC.mean_z = scale(Accuracy.mean, center = TRUE, scale = TRUE)[,1]) %>%
  filter(ACC.mean_z < acc_criteria)

data_scores <- filter(data_scores, !(Subject %in% data_remove$Subject)) 

data_outliers <- data_scores %>%
  mutate(VA_k_z = scale(VA_k, center = TRUE, scale = TRUE)) %>%
  filter(VA_k_z >= outlier_criteria | VA_k_z <= -1 * outlier_criteria)

data_scores <- filter(data_scores, !(Subject %in% data_outliers$Subject)) 
####################

#### Calculate Reliability ####
reliability <- data_import %>%
  filter(Subject %in% data_scores$Subject) %>%
  group_by(Subject, SetSize) %>%
  mutate(Index = row_number(),
         Split = ifelse(Index %% 2, "odd", "even")) %>%
  group_by(Subject, SetSize, Split) %>%
  summarise(CR.n = sum(CorrectRejection, na.rm = TRUE),
            FA.n = sum(FalseAlarm, na.rm = TRUE),
            M.n = sum(Miss, na.rm = TRUE),
            H.n = sum(Hit, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(CR = CR.n / (CR.n + FA.n),
         H = H.n / (H.n + M.n),
         k = SetSize * (H + CR -1)) %>%
  pivot_wider(id_cols = "Subject",
              names_from = c("SetSize", "Split"),
              values_from = c("CR", "H", "k")) %>%
  mutate(VA_k_even = (k_5_even + k_7_even) / 2,
         VA_k_odd = (k_5_odd + k_7_odd) / 2) %>%
  summarise(r = cor(VA_k_even, VA_k_odd)) %>%
  mutate(r = (2 * r) / (1 + r))

data_scores$VA_splithalf <- reliability$r
###############################

#### Output ####
write_csv(data_scores, here(output_dir, output_file))
################

#### Append to Log ####
log_header <- paste("===== Visual Arrays: 1_visualarrays_score.R =====\n\n",
                    format(Sys.Date(), "%B %d %Y"), "\n", sep = "")
log_init_n <- paste("-- ", length(unique(data_import$Subject)),
                    "\t Initial subject count", sep = "")
log_remove <- paste("-- ", length(unique(data_remove$Subject)),
                    "\t Number of problematic subjects removed", sep = "")
log_outlier <- paste("-- ", length(unique(data_outliers$Subject)),
                     "\t Number of outliers removed", sep = "")
log_final_n <- paste("-- ", length(unique(data_scores$Subject)),
                     "\t Final subject count", "\n", sep = "")
table_remove <- kable(data_remove, digits = 2, format = "markdown")
table_outlier <- kable(data_outliers, digits = 2, format = "markdown")
log_footer <- "==================================================\n"

sink(file = here("log.txt"), append = FALSE, split = TRUE)
cat(log_header, log_init_n, log_remove, log_outlier, log_final_n, 
    print(table_remove), "\n", print(table_outlier), "\n", log_footer,
    fill = FALSE, sep = "\n")
sink()
#######################

rm(list=ls())
```


----

```{r echo=FALSE, message=FALSE, warning=FALSE}
rm(list=ls())
```


