# R Script Templates

```{r include=FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
```

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_all.png"))
```

<br>

One of the nice features of using R scripts to analyze your data is that you can use a lot of the same code from one step or one project to another. 

For each of these steps, the script organization from one task, analysis, or project to the next will be more or less the same. This means we can take advantage of using R script templates. Using R Script templates will not just save time but help you stay organized and use reproducibility practices talked about in the previous chapters.

## Install `workflow` package

```{r eval = FALSE}
devtools::install_github("dr-JT/workflow")
```

I have created some templates you can easily download using a single R function `workflow::template()`

There are different types of templates you can download using `workflow::template()`. You can always use `?workflow::template` to see help documentation.

### Data Preparation

```{r echo=FALSE, eval = TRUE, out.width='40%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_messy-tidy.png"))
```

*Data Preparation* will occur in a Data Collection directory, separately from *Data Analysis*. It will have its own **.RProj** and **mastercript.R** files. 

The main purpose of data preparation is converting *messy* raw data files to *tidy* raw data files. 

### messy-to-tidy script

If you want to download a template script for the step of converting *messy* raw data files to *tidy* raw data files during data preparation you can simply specify:

```{r eval = FALSE}
workflow::template(rawscript = TRUE)
```

Let us take a look at what the **rawscript** template looks like

```{r eval = FALSE}
#### Setup ####
## Load packages
library(here)
library(readr)
library(dplyr)

## Set Import/Output Directories
import_dir <- "Data Files/Merged"
output_dir <- "Data Files"

## Set Import/Output Filenames
task <- "taskname"
import_file <- paste(task, ".txt", sep = "")
output_file <- paste(task, "raw.csv", sep = "_")
################

#### Import ####
data_import <- read_delim(here(import_dir, import_file), "\t", 
                          escape_double = FALSE, trim_ws = TRUE,
                          guess_max = 10000)
################

#### Tidy raw data ####
data_raw <- data_import %>%
  filter() %>%
  rename() %>%
  mutate() %>%
  select()
#######################

#### Output ####
write_csv(data_raw, here(output_dir, output_file))
################

rm(list=ls())
```

There are 4 main blocks of R code:

1) **Set up** the script 

    * Load required packages using `library()`
    * Set the import/output directories and filenames
    
    Doing these steps at the top of the script makes it obvious, without having to read the rest of the script, what packages the script will require and what data file it is importing and outputing.

2) **Import** a data file using `read_delim()` from the `readr` package

3) **Tidy** the imported data frame using `dplyr` functions, such as

   `filter()`, `rename()`, `mutate()`, `case_when()`, and `select()`
   
4) **Output** a "tidy" data file using `write_csv()` from `readr`

Last, remove all objects from the environment with `rm(list=ls())`.

<br>

To me this template is beautiful. The only thing you need to change is 

1) `task <- "taskname"` to the name of the task used in the file name

2) Fill in what happens in the *Tidy raw data* block. 

The rest can LITERALLY stay the same. How easy!

### masterscript: Data Preparation

In the EngleLab, we often have up to 40 tasks for a single Data Collection study. When you have a lot of R Scripts it is quite tedious to open, **Source**, and exit, each R script one at a time. The **masterscript** allows you to **Source** all of your scripts at once. 

The `source()` function is a way to execute all the lines of code in a script file. Rather than having to manually open each script file and sourcing it from there you can control your entire data processing workflow from the masterscript using `source()`. The argument `echo = TRUE` will print the results of the script to the console that way you can still see what the script is doing.

This allows you to run each script from the masterscript and control the order in which you run them. For Data Preparation the order does not really matter, but once you get to Data Analysis the order is crucial.

The masterscript template for *Data Preparation* can be downloaded with:

```{r eval = FALSE}
workflow::template(masterscript = "data preparation")
```

This masterscript is simple:

```{r eval = FALSE}
## Data Preparation for Study Name

#############################################
#------ 0. "messy" to "tidy" raw data ------#
#############################################

source("R Scripts/0_taskname_raw.R", echo=TRUE)

rm(list=ls())
#############################################
```

## Data Analysis

```{r echo=FALSE, eval = TRUE, out.width='45%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_dataanalysis.png"))
```

*Data Analysis* will occur in it's own directory separate from the Data Collection directory and separate from *Data Preparation*. The tidy raw data files created during *Data Preparation* will be copy and pasted over to **Data Files/Raw Data** in the *Data Analysis* directory

Typically there are at least two steps that need to be taken in order to create a final data file that is ready for statistical analysis. 

1. **Clean and Score** the data from each task by aggregating performance over trials and removing any outlier trials or problematic/poor performing subjects.

2. **Merge** the scored data file from each task into one final data file ready for statistical analysis.

There may be some additional steps that are required but these are the basic ones. 

### clean and score script

If you want to download a template script for the step of cleaning and scoring a raw data file you can type in the console:

```{r eval = FALSE}
workflow::template(scorescript = TRUE)
```

Let us take a look at what the **scorescript** template looks like

```{r eval = FALSE}
#### Setup ####
## Load Packages
library(here)
library(readr)
library(dplyr)

## Set Import/Output Directories
import_dir <- "Data Files/Raw Data"
output_dir <- "Data Files/Scored Data"

## Set Import/Output Filenames
task <- "taskname"
import_file <- paste(task, "raw.csv", sep = "_")
output_file <- paste(task, "Scores.csv", sep = "_")

## Set Data Cleaning Params

###############

#### Import ####
data_import <- read_csv(here(import_dir, import_file))
################

#### Data Cleaning and Scoring ####
data_scores <- data_import %>%
  filter() %>%
  group_by() %>%
  summarise()
###################################

#### Output ####
write_csv(data_scores, here(output_dir, output_file))
################

rm(list=ls())
```

Like the rawscript, there are 4 main blocks of R code:

1) **Set up** the script 

    * Load required packages using `library()`
    * Set the import/output directories and filenames
    * Set Data Cleaning Params
    
    You can set some optional data cleaning parameters at the top of the script. This will be explained in more detail in the next section.

2) **Import** a data file using `read_csv()` from the `readr` package

3) **Clean and Score** the imported data frame using `dplyr` functions, such as

   `filter()`, `mutate()`, `group_by()`, and `summarise()`
   
4) **Output** a data file with task *Scores* using `write_csv()` from `readr`

Last, remove all objects from the environment with `rm(list=ls())`.

<br>

The parts you may need to change are:

1. What packages are loaded, `library()`

2. task <- "taskname" 

3. Optional data cleaning parameters in the "Set Data Cleaning Params" section

4. The "Data Cleaning and Scoring" block

### merge script

To download a template script to merge several Scored data files together:

```{r eval = FALSE}
workflow::template(mergescript = TRUE)
```

The template looks like this:

```{r eval = FALSE}
#### Set up ####
## Load packages
library(here)
library(datawrangling) # for files_join() and trim()
library(dplyr)

## Set import/output directories
import_dir <- "Data Files/Scored Data"
output_dir <- "Data Files"
output_file <- "name_of_datafile.csv"
################

#### Import Files ####
data_import <- files_join(here(import_dir), pattern = "Scores", id = "Subject")
######################

#### Select only important variables ####
data_merge <- data_import %>%
  select()

## Create list of final subjects
subj.list <- select(data_merge, Subject)
#################################################################

#### Output ####
write_csv(data_merge, here(output_dir, output_file))
write_csv(subj.list, here(output_dir, "subjlist_final.csv"))
################

rm(list=ls())
```

Again, this template script has 4 main blocks: 

1) **Set up** the script 

    * Load required packages using `library()`
    * Set the import/output directories
    
    Notice that in the setup section, the import and output filenames are not specified. The import file names are not specified because 1) we may need to import quite a lot of Scored data files and 2) I created a function to do this without having to specify each individual filename. You could add a section to include an output filename if you want. 

2) **Import** the data files with task scores. 

    You can merge multiple files with the same rows (Subjects) and different columns (variables or task scores) using `datawrangling::files_join()`. This will be explained in more detail later on.

3) **Select** only relevant variables.

   It is likely that the individual Scored data files will have way more columns of scores than you are interested in.
   
4) **Output** a final data file that is ready for statistical analysis with `write_csv()` from `readr`

    Optional to also create a file with a list of all subjects that have made it through all these stages of processing. 

Last, remove all objects from the environment with `rm(list=ls())`.

### masterscript: Data Analysis

The masterscript template for *Data Analysis* can be downloaded with:

```{r eval = FALSE}
workflow::template(masterscript = "data analysis")
```

```{r eval = FALSE}
## Data Analysis for StudyName


#################################################
#------ 1. "tidy" raw data to Scored data ------#
#################################################

source("R Scripts/1_taskname_score.R", echo=TRUE)

rm(list=ls())
#############################################################
#------ 2. Create Final Merged Data File for Analysis ------#
#############################################################

source("R Scripts/2_merge.R", echo=TRUE)

rm(list=ls())
###############################
#------ 3. Data Analysis ------#
###############################
library(rmarkdown)

render("R Scripts/3_MainAnalyses.Rmd",
       output_dir = "Results", 
       output_file = "MainAnalyses.html")

rm(list=ls())
#################################################
```

You can see that it is organized in the order in which the scripts need to be ran and how the scripts are named.

The `render()`, function seen in the "3. Data Analysis" section, is how to knit an RMarkdown document. We will see later that this creates a flexible way to knit RMarkdown documents because you can specify the output file name and location, as well as certain parameters for what data set to import or analysis parameters to set. `render()` comes from the package `rmarkdown` which is why it is loaded at the top of the data analysis section.


----

```{r eval = TRUE, echo=FALSE}
rm(list=ls())
```

