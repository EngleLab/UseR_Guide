# (PART) Cleaning and Scoring Data {-}

# Cleaning and Scoring Data: Overview {-}

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

```{r echo=FALSE, eval = TRUE}
knitr::include_graphics(rep("images/workflow.png"))
```

This is the first step of actual data analysis. When performing statistical analyses we are asking questions about the relationship between two or more variables. In psychology, our variables are either experimental manipulations or measured observations. Variables that are measured observations are typically aggregated over multiple responses (e.g. multiple trials in a task) within an individual.

To continue with the Flanker example from previous chapters, we manipulate the congruency of the central target arrow with the flanking distractor arrows. 

```{r echo=FALSE, eval=TRUE, fig.align='center'}
knitr::include_graphics(rep("images/flanker_arrow.jpeg"))
```

A participant will perform many trials, some of which will be congruent and some of which will be incongruent. If we wanted to assess the effect of congruency on reaction time (RT) we would first need to calculate a mean RT on congruent trials and a mean RT on incongruent trials for each participant separately. This is because any one single trial will contain a lot of error (or noise) and may not accurately reflect that individuals RT. As an attempt to reduce some of that error in a single trial, we aggregate performance across many trials. In this case we aggregate by calculating a mean RT for each congruency condition and for each participant. This results in two aggregated variables per particpant, a mean RT on congruent and a mean RT on incongruent trials. Generally, the more trials the less error in the aggreagated variable. 

**Before you can conduct statistical analyses you need to calculate aggregated scores from the raw trial level data.** 

*If appropriate you may also need to do some data cleaning to remove trials and/or particpants that are clearly problematic.* Sometimes a participant will not understand the instructions, or does not care enough to correctly perform the task, and this will usually be reflected in extremely poor performance across the task. Or a particpant, for a single trial, may give an anticipatory response that results in an RT value that is shorter than possible (e.g. 100 ms). These participants and trials should be removed from further analysis. This is what is meant by data cleaning. 

In the previous Section, I covered how to create a *tidy* raw data file that had one row per trial (including practice trials). In this Section you will use that *tidy* raw data file to calculate the `FlankerEffect` for each Subject. The `FlankerEffect` is the difference of *mean RTs* on incongruent trials minus congruent trials. You will also do some data cleaning to remove problematic trials and participants.

## Data Analysis Repository

In the previous Section you created a **Data Collection** repository. The **Data Collection** repository is the directory where data collection occurs and where the **tidy raw data files** are stored. This repository eventually becomes the **Central Repository** after data collection has finished.

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/central_repository.png"))
```

The **Central Repository** is where both the *messy* and *tidy* raw data files are stored. **No data analysis steps occur in the Central Repository**. 

**Data analysis steps occur in the Data Analysis repositories**. 

In the EngleLab, we often conduct large-scale data collection studies in which there are many different research projects going on at once. Many of the tasks will be shared between these research projects. This means each separate project is using a lot of the same data. This can make it difficult to figure out how and at what stage to separate data processing and analysis between these studies.

If someone already created a data file, for their research project, with scored variables on a lot of the tasks that you need, **should you copy and paste that data file into your analysis repository?**. **NO!** 

This will undermine the reproducibility of your research project and analyses. You will also have no way of figuring out how they scored their variables, what data cleaning procedures they used, etc. There is no transparency of how they got from A (the raw data) to B (the scored data file). They simply gave you B. 

Whereas with the **Project Organization** method I outlined in **Chapter 7**, you can copy the **tidy** raw data files from the **Central Repository** into your **Data Analysis** repository. And if someone has already created scripts to score some of the tasks you need, you can also copy those R scripts over to your project. Now you will have A (the raw data) and the R script of how to get from A to B (the scored data file). This will give you full transparency and you can completely reproduce the analyses. 

----

To make your life even easier, I have automated the process for you to create these various repositories. This will make sure you (and any of your collegues) have the same directory organization across your different projects.

You will first have to install the `workflow` package. If you have not done so you can install it: `devtools::install_github("dr-JT/workflow")`. Then:

File --> New Project... --> New Directory --> Research Study

* __Directory Name__: UseRGuide_DataAnalysis

* __Create project as subdirectory of__: Your R_Tutorial directory

* __Repository Type__: data analysis

----

In the folder **UseRGuide_DataAnalysis**, you should see an organization that looks like this:

```{r echo=FALSE, eval = TRUE, out.width='35%', fig.align='center'}
knitr::include_graphics(rep("images/repository_analysis.png"))
```

You will notice that there are already some template scripts so you can quickly start using R for **Cleaning and Scoring** your data.

**When working on this Data Preparation section you should open RStudio by opening the *UseRGuide_DataAnalysis.Rproj* file**.

## Copy tidy raw data

In the previous Section you created a **tidy** raw data file in the **UseRGuide_DataCollection** repository. Copy the tidy raw data file, **Flanker_raw.csv**, to the **UseRGuide_DataAnalysis** repository - **Data Files/Raw Data/Flanker_raw.csv**.

Normally you would be copying several tidy raw files, however, for the sake of this tutorial we are only working with a single task right now.

**If you did not complete the previous section or do not have the Flanker_raw.csv file, then you can download it below**. Move the downloaded file to **Data Files/Raw Data/Flanker_raw.csv**.

----

<div style="text-align: center; font-size: 1.25em">
[<i class="fas fa-download" style="font-size: 3em"></i> ](http://englelab.gatech.edu/userguide/Flanker_raw.csv)

[**Download Example Flanker Data**](http://englelab.gatech.edu/userguide/Flanker_raw.csv)
</div>

----

********

```{r echo=FALSE, message=FALSE, warning=FALSE, eval = TRUE}
rm(list=ls())
```

********

**Are you excited? I am! First we need to learn how to create scored data files that are ready for analysis**
