# Tidy Raw to Scored Data

```{r echo=FALSE}
knitr::include_graphics(rep("images/workflow.png"))
```

This Chapter will cover the functions that will be used to convert a "tidy" raw data file to a scored data file.

You should save the code you write in this Chapter to a new R script. Do not use the same R script from the previous Chapter.

## Flanker Example

Again we will continue using the Flanker data set as an example. If you do not have the "tidy" raw data file from the previous Chapter, then download the example Flanker data and run the following script **(You will have to change the import and save directories, in `read_delim()` and `write_delim()`**. Save the "tidy" raw data file as "Flanker_raw.txt".

[Download Example Flanker Data](http://englelab.gatech.edu/R/example_data/Flanker_Example.zip)

```{r eval=FALSE}
data_flanker <- read_delim("data/Flanker.txt", "\t", escape_double = FALSE, trim_ws = TRUE) %>%
  duplicates.remove(taskname = "Flanker", output.folder = "data/duplicates") %>%
  filter(`Procedure[Trial]`=="TrialProc" | `Procedure[Trial]`=="PracTrialProc") %>%
  rename(TrialProc = `Procedure[Trial]`) %>%
  mutate(TrialProc = ifelse(TrialProc=="TrialProc", "real", "practice"),
         Trial = ifelse(TrialProc=="real", TrialList.Sample, PracTrialList.Sample),
         RT = ifelse(TrialProc=="real", SlideTarget.RT, PracSlideTarget.RT),
         ACC = ifelse(TrialProc=="real", SlideTarget.ACC, PracSlideTarget.ACC),
         Response = ifelse(TrialProc=="real", SlideTarget.RESP, PracSlideTarget.RESP),
         Response = ifelse(Response=="{LEFTARROW}", "left", ifelse(Response=="{RIGHTARROW}", "right", NA)),
         TargetArrowDirection = ifelse(TrialProc=="real", TargetDirection, TargerDirection)) %>%
  select(Subject, TrialProc, Trial, Condition = FlankerType, RT, ACC, Response, TargetArrowDirection, SessionDate, SessionTime)

write_delim(data_flanker, path = "data/Flanker_raw.txt", delim = "\t", na = "")
```

In the EngleLab we typically use one of two scores for the Flanker task. The Reaction time **FlankerEffect** or the **FlankerBinned** score. 

**FlankerEffect**: The difference between mean RTs on congruent and incongruent trials

**FlankerBinned**: A procedure to take into account both reaction time and accuracy in a single score. (We prefer this score since it avoids the problem of RT difference scores)

## Import "raw" data

```{r collapse=TRUE, message=FALSE, warning=FALSE}
library(readr)
import <- read_delim("data/Flanker_raw.txt", "\t", escape_double = FALSE, trim_ws = TRUE)
```

Before you start calculating any performance variable you need to remove the practice trials

```{r collapse=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
import <- filter(import, TrialProc!="practice")
```

## Aggregating Performance

Typically, the dependent measure in a cognitive task is some aggregate of performance across the trials in the task. The exception to this are adaptive tasks that arrives at a single score at the end of the task.

Some common aggregate performance measures we use in our lab are:

**Mean RT**: average reaction time on trials (will usually filter out inaccurate trials). 

**Mean Accuracy**: average accuracy on the task

**Sum of Correct Responses**: total number of correct responses

If you remember from Chapter 4, the `dplyr` package provides a convenient way to calculate aggregate scores with the `group_by()` and `summarise()` functions.

In the Flanker task, we will need to calculate, for each subject, their **Mean RT** seperately for congruent and incongruent trials. 

Essentially, we are aggregating performance grouped at two levels: subject and Condition. For each subject and each Condition we need to cacluate a **Mean RT**. This is where the `group_by()` function comes in. 

We can also calculate the **Standard deviation on RT** and **Mean Accuracy**.

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- import %>%
  group_by(Subject, Condition) %>%
  summarise(RT.mean = mean(RT, na.rm = TRUE),
            RT.sd = sd(RT, na.rm = TRUE),
            ACC.mean = mean(ACC, na.rm = TRUE))
```

The functions used to calculate the aggreate scores, `mean()` and `sd()` are part of the `basic` R functions. They are pretty self explanatory but I should mention the `na.rm = TRUE`. This simply specifies that trials with missing values should be removed when calculating the aggregate score. There really are not any scenarios I have ran into where I don't want to remove missing values, so every time I use any aggregate function I specify, `na.rm = TRUE`. 

Take a look at the dataframe you just created. It should have two rows for each Subject, one for congruent and the other for incongruent trials. You can see that this is a reduced dataframe from `import`, that is because we just aggregated performance across the trials. The only information that was preserved are the grouped variables (Subject and Condition) and the new aggregate variables.

**Now we actually need to do one extra thing before we calculate aggegate performance**

We want to calculate the **Flanker Effect** using only reaction times from accurate trials; therefore, we need to calcuate aggregate **Mean RT**s based on only accurate trials. 

There are several ways of doing this. The simplest is to set reaction times to missing `NA` on inaccurate trials. Remember the `na.rm = TRUE` agrument? This will come in handy here becuase now we only have values for reaction times on accurate trials, and want to remove (ignore) innaccurate trials.

To set reaction times to missing `NA` on inaccurate trials you can use the `mutate()` function that you learned about in the previous Chapter. 

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- import %>%
  mutate(RT = ifelse(ACC==0, NA, RT))
```

Now add the `group_by()` and `summarise()` function to get

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- import %>%
  mutate(RT = ifelse(ACC==0, NA, RT)) %>%
  group_by(Subject, Condition) %>%
  summarise(RT.mean = mean(RT, na.rm = TRUE),
            RT.sd = sd(RT, na.rm = TRUE),
            ACC.mean = mean(ACC, na.rm = TRUE))
```


## Reshape Dataframes

The next step is to calculate the difference between congruent and incongruent trials. The dataframe currently is not in the right format to do this. We need just one row per Subject, with columns for **congruent RT.mean** and **incongruent RT.mean**, as well as the other aggregate variables. In other wrods, we need to spread the values in the column `Condition` across the three aggregate variables.

There are various functions in R that allow us to **reshape** the dataframe. The `dplyr` package provides the handy `spread()` function. However, `spread()` will only work on one of the aggregated variables not all three. What I have gone and done is create a function (that uses `spread()`) to spread multiple columns across multiple aggregated variables. The function is `reshape()` and is part of my `datawrangling` package.

You will need to include the following arguments in `reshape()`:

**key**: The column used for spreading across other columns

**values**: The columns that contain the values to be spread on

**by**: What column is not being reorganized and needs to be preserved. Usually "Subject".

Recall that you can use `help()` to see documentation on how to use a function.

```{r collapse=TRUE, message=FALSE, warning=FALSE}
library(datawrangling)
data_flanker <- data_flanker %>%
  reshape(key = "Condition", values = c("ACC.mean", "RT.mean", "RT.sd"), by = "Subject")
```

If you compare the previous dataframe with the reshaped dataframe you will see that you have simply spread "congruent" and "incongruent" across the aggregate variables. There is now one row per Subject.

## Calculate Scores

### Flanker Effect

Now you can calculate the **Flanker Effect** by subtracting the two columns, **congruent_RT.mean** and **incongruent_RT.mean**. Use the `mutate()` function to do this.

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- data_flanker %>%
  mutate(FlankerEffect = incongruent_RT.mean - congruent_RT.mean)
```

Nice! You have now calculated scores on the Flanker task for each Subject!

You can feel free to pipe all the function together to get something like

```{r collapse=TRUE, message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(datawrangling)

import <- read_delim("data/Flanker_raw.txt", "\t", escape_double = FALSE, trim_ws = TRUE) %>%
  filter(TrialProc!="practice")

data_flanker <- import %>%
  mutate(RT = ifelse(ACC==0, NA, RT)) %>%
  group_by(Subject, Condition) %>%
  summarise(RT.mean = mean(RT, na.rm = TRUE),
            RT.sd = sd(RT, na.rm = TRUE),
            ACC.mean = mean(ACC, na.rm = TRUE)) %>%
  ungroup() %>%
  reshape(key = "Condition", values = c("ACC.mean", "RT.mean", "RT.sd"), by = "Subject") %>%
  mutate(FlankerEffect = incongruent_RT.mean - congruent_RT.mean)
```

Notice that I added an `ungroup()` function after summarise. You should be somewhat careful about using the `group_by()` function. In this case, we only needed the `group_by()` for the `summarise()` function. However, the `group_by()` function is preserved until otherwise specified. This means you can accidentally pass `group_by()` onto a function you don't intend to. In this case it actually does not make a difference, but as a good practice I like to `ungroup()` if I no longer need to preserve the groups. 

If you want to get rid of the other columns and keep only the `Subject` and `FlankerEffect` columns then you can add the `select()` function you learned about in the previous Chapter at the end, after `mutate()`. 

### Binned Scores

At the beginning I mentioned that we also want to calculate the binned Flanker scores. 

The process of calculating binned scores requires a lot of steps. Fortunate for you, I have created a function to calculate bin scores on any task. The function is `bin.score()` from the `englelab` package. 

```{r collapse=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
library(englelab)
data_flanker.bin <- bin.score(import)
```

Wow that was easy! There are actually several arguments that we did not have to specify because we just need the default values. If you want to use the `bin.score` function for other tasks use `help("bin.score")` to see all the arguments you can specify.

## Data Cleaning

Sorry, we are not done with data scoring yet! We need to cover one more important step. This is not as necessary if you plan on looking at relationships at the latent level, however it is a good rule of thumb to remove as much unsystematic "noise" from your data as possible. 

The reality of doing psychological research is that our research subjects may not always understand the nature of the task or are just not trying at all. If this is the case we probably are not measuring their "true" score for that task. Therefore, it can be a good idea to remove scores on tasks for those subjects that show indications of this type of performance. 

What indicators you actually decide on using may vary from one task or study to the next. As I am writing this, we in the EngleLab currently do not have a standard wasy of data cleaning for a lot of our tasks including the Flanker. 

Most recently, we talked about a few criteria we would like to try. 

**Minimum RT Criteria**: Any trials that have less than 200ms reaction times are removed.

**SDs on RT Criteria**: Any trials that are 3.5 SDs above or below the mean, for that Subject, are removed (or replaced with 3.5 SDs above or below the mean).

**SDs on Accuracy Criteria**: Any subjects that are below 3.5 SDs of the mean on accuracy are removed.

I will cover how to do data cleaning using these criteria. This should provide a pretty general basis on how to write R code for other criteria not covered here.

I will also only cover data cleaning for calculating the FlankerEffect and not the Bin Scores.

### Minimum RT Criteria

The first two criteria are cleaning data at the trial-level. Therefore, we will start with the original imported dataframe.

For this approach you will want to remove trials with less than 200ms before you aggregate a **Mean RT** using the `summarise()` function.

Recall that we also wanted to remove inaccurate trials before aggregating. The way did this was by using the `mutate()` function to set inaccurate RT's to `NA`. The reason we don't use `filter()` to completely remove the trial is that it will make it harder to calculate `Mean Accuracy`. So the better way to remove the trial is to set the RT to `NA`. 

This will be simple since we already did this for inaccurate trials. We just need to add an extra statement that we also want to set RT to `NA` for trials that have an RT of less than 200. 

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- import %>%
  mutate(RT = ifelse(ACC==0 | RT < 200, NA, RT))
```

All we did was add an OR statement `|` and `RT < 200`. Easy peasy, Japanesy! Hey I am Japanese, well half Japanese. :)

Because this is a criteria we might want to adjust if we change our minds about it, it is good practice to set this value of `200` to a variable at the top of the script. Now we can add the script to calculate the FlankerEffect.

```{r collapse=TRUE, message=FALSE, warning=FALSE}
min_RT.criteria <- 200

data_flanker <- import %>%
  mutate(RT = ifelse(ACC==0 | RT < min_RT.criteria, NA, RT)) %>%
  group_by(Subject, Condition) %>%
  summarise(RT.mean = mean(RT, na.rm = TRUE),
            RT.sd = sd(RT, na.rm = TRUE),
            ACC.mean = mean(ACC, na.rm = TRUE)) %>%
  ungroup() %>%
  reshape(key = "Condition", values = c("ACC.mean", "RT.mean", "RT.sd"), by = "Subject") %>%
  mutate(FlankerEffect = incongruent_RT.mean - congruent_RT.mean)
```

Now if you want to go back and change this criteria you do not have to search through the script to find where you had set the criteria. You know it will be at the top of the script. Also I won't talk about naming conventions, but it is a good idea to be clear when naming variables. The name `min_RT.criteria` is clear that this variable holds information for the minimum RT criteria. 

### SDs on RT Criteria

For this approach, what you will need to do is:

1. Calculate z-scores to determine which trials are 3.5 SDs above or below the mean.

2. Calculate the mean and SD of each Condition for each Subject seperately. 

3. Set RT values that are 3.5 SDs above or below the means to 3.5 SDs above or below the mean.

#### Calculate Z-scores

This actually takes several steps but I create a function to make it easy for you!. The function is `center()` from my `datawrangling` package. 

`center()` is a pretty simple function that takes 4 possible arguments:

**x**: the dataframe to be passed onto the function

**variables**: the columns to be centered

**standardize**: Do you want to standardize (calculate z-scores)? Logical argument.

Again, you can just type `help("center")` in the console to get this information.

You will want to calculate z-scores within each subject and for each Condition (congruent and incongruent). This means we need to specify the context to calculate z-scores as `context = c("Subject", "Condition")`.

Remember we are starting with the orginal imported data frame `import`

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- import %>%
  center(variables = "RT", context = c("Subject", "Condition"), standardize = TRUE)
```

If you look at the dataframe, you should see a new column labeled `RT_zwc`. The reason it has `_zwc` is to let you know that z-scores were calculated within-context. 

#### Calculate Mean and SDs

Now on to applying the data clearning procedure. What we want to do is replace trials above or below 3.5 SDs of the mean with 3.5 SDs above or below the mean. (We could simply remove the trials like we did before setting RTs to `NA`, but we will try something different here).

In order to change the RT to 3.5 SDs above or below the mean we need to know what the RT values at 3.5 SDs above and below the mean are. To do this we need the mean and SD values. You might be thinking that we can use `summarise()` to aggregate and calculate the mean and SD values. The problem with this is that `summaris()` reduces the dataframe and we will no longer have trial-level data. Shoot. 

What we want is to preserve the trial-level data while creating new columns with the mean and SD values. Feel free to take some time to think why this is a little challenging and why `summarise()` will not work. Maybe you can think of a way to make `summarise()` work but it requires several steps. What we want is to do this one single step.

As it turns out, there is a way to do this in a single step! We will need to use a combination of the `group_by()`, `mutate()`, `mean()`, and `sd()` functions.

We can use `group_by()` to group within Subject and within Condition. Then use `mutate()` to create a new columns to calculate mean and SDs on RTs for each Subject within each Condition. 

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- import %>%
  center(variables = "RT", context = c("Subject", "Condition"), standardize = TRUE) %>%
  group_by(Subject, Condition) %>%
  mutate(RT.mean = mean(RT, na.rm = TRUE),
         RT.sd = sd(RT, na.rm = TRUE))
```

Look at the dataframe to make sure everything seems to be about right. All congruent trials, for a given Subject, should have the same RT.mean and RT.sd values. Same for incongruent. 

#### Replace Outlier RT Values

Alright, we have all the information we need now to replace RTs 3.5 SDs above or below the mean.

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- import %>%
  center(variables = "RT", context = c("Subject", "Condition"), standardize = TRUE) %>%
  group_by(Subject, Condition) %>%
  mutate(RT.mean = mean(RT, na.rm = TRUE),
         RT.sd = sd(RT, na.rm = TRUE),
         RT = ifelse(RT_zwc > 3.5, RT.mean+(3.5*RT.sd), ifelse(RT_zwc < -3.5, RT.mean-(3.5*RT.sd), RT)))
```

If we put it all together with the **Minimum RT Criteria** approach, you will get something like this

```{r collapse=TRUE, message=FALSE, warning=FALSE}
sd_RT.criteria <- 3.5
min_RT.criteria <- 200

data_flanker <- import %>%
  mutate(RT = ifelse(ACC==0 | RT < min_RT.criteria, NA, RT)) %>%
  center(variables = "RT", context = c("Subject", "Condition"), standardize = TRUE) %>%
  group_by(Subject, Condition) %>%
  mutate(RT.mean = mean(RT, na.rm = TRUE),
         RT.sd = sd(RT, na.rm = TRUE),
         RT = ifelse(RT_zwc > sd_RT.criteria, RT.mean+(sd_RT.criteria*RT.sd), ifelse(RT_zwc < (-1*sd_RT.criteria), RT.mean-(sd_RT.criteria*RT.sd), RT))) %>%
  group_by(Subject, Condition) %>%
  summarise(RT.mean = mean(RT, na.rm = TRUE),
            RT.sd = sd(RT, na.rm = TRUE),
            ACC.mean = mean(ACC, na.rm = TRUE)) %>%
  ungroup() %>%
  reshape(key = "Condition", values = c("ACC.mean", "RT.mean", "RT.sd"), by = "Subject") %>%
  mutate(FlankerEffect = incongruent_RT.mean - congruent_RT.mean)
```

Wow, that got pretty heavy! I hope your head is not spinning. Go get some water and take a short break if you need to.

But you learned a useful technique to calculate aggregate variables without losing the trial-level structure. You do so using a combination of `group_by()` and `mutate()`, instead of `group_by()` and `summarise()`.

### SDs on Accuracy

This criteria is applied at the task-level. What we want to do is remove subjects who are 3.5 SDs below the mean on task-level accuracy. 

This will be easy because we already calculated mean accuracy for congruent and incongruent trials. We just need to z-score those columns and remove subjects that are above 3.5 or below -3.5. It wouldn't be too difficult to do this just for two variables, but imagine if you had to do this for a bunch of variables! That would get quite tedious to create a `filter()` for all those variables. 

Well, I come to the rescue again! I created a function to trim subjects who are 3.5 SDs above or below the mean on a list of variables. Now insated of calculating z-scores for each variable and filtering on each variables, we can specify all this in one line of code using the function.

The function is `trim()` from my `datawrangling` package. It is a simple function, you just need to specify:

**x**: the dataframe to be passed onto the function

**variables**: list of columns to be trimmed

**cutoff**: zscore cutoff to use for trimming (default: 3.5)

**replace**: 

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- trim(data_flanker, variables = c("congruent_ACC.mean", "incongruent_ACC.mean"), cutoff = 3.5, replace = "NA")
```

Now this isn't quite finished yet. If you look at the dataframe you will see that those subjects who got trimmed have either `congruent_ACC.mean` or `incongruent_ACC.mean` set as `NA`. The next step is to completely remove these subjects from the dataframe useing `filter()`.

```{r collapse=TRUE, message=FALSE, warning=FALSE}
data_flanker <- filter(data_flanker, !is.na(congruent_ACC.mean), !is.na(incongruent_ACC.mean))
```

We want to keep only those subjects that do not have missing values on congruent AND incongruent. `is.na()` is a way of evaluating missing values. `!` means "not". So `!is.na()` means "is NOT missing". 

## Merging Data Files

You should now have two dataframes with scores on the Flanker tasks, `data_flanker` and `data_flanker.bin`. Now I will go over how to merge these two files into a single dataframe.

We want to make sure we match the subjects in the two dataframes together; we will merge the two dataframes `by` using the `Subject` column as a reference. The code for this is pretty simple. We will use the `merge()` function from the `basic` R functions. 

```{r collapse=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
data_flanker <- merge(data_flanker, data_flanker.bin, by = "Subject")
```

By default `merge()` will retain all subjects from both dataframes. If you only want to keep subjects from one of the dataframes then you need to add some arguments. The first dataframe listed is the `x` dataframe, and the second is the `y` dataframe Let's say you only want to keep Subjects from the `x` dataframe. Then you need to specify the two arguments, `all.x = TRUE, all.y = FALSE`. And vice versa if you only want to keep subjects from the `y` dataframe.

## Save dataframe to File

```{r eval=FALSE}
write_delim(data_flanker, path = "data/Flanker_scores.txt", delim = "\t", na = "")
```

## Finalize Your Script

Now that you have completed all the components for data cleaning and calculating the FlankerEffect an BinScores, put it all together in an organized script file and save it. You might have something that looks like this

```{r collapse=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
sd_RT.criteria <- 3.5
min_RT.criteria <- 200

library(readr)
library(dplyr)
library(datawrangling)
library(englelab)

import <- read_delim("data/Flanker_raw.txt", "\t", escape_double = FALSE, trim_ws = TRUE) %>%
  filter(TrialProc!="practice")

data_flanker <- import %>%
  mutate(RT = ifelse(ACC==0 | RT < min_RT.criteria, NA, RT)) %>%
  center(variables = "RT", context = c("Subject", "Condition"), standardize = TRUE) %>%
  group_by(Subject, Condition) %>%
  mutate(RT.mean = mean(RT, na.rm = TRUE),
         RT.sd = sd(RT, na.rm = TRUE),
         RT = ifelse(RT_zwc > sd_RT.criteria, RT.mean+(sd_RT.criteria*RT.sd), ifelse(RT_zwc < (-1*sd_RT.criteria), RT.mean-(sd_RT.criteria*RT.sd), RT))) %>%
  group_by(Subject, Condition) %>%
  summarise(RT.mean = mean(RT, na.rm = TRUE),
            RT.sd = sd(RT, na.rm = TRUE),
            ACC.mean = mean(ACC, na.rm = TRUE)) %>%
  ungroup() %>%
  reshape(key = "Condition", values = c("ACC.mean", "RT.mean", "RT.sd"), by = "Subject") %>%
  mutate(FlankerEffect = incongruent_RT.mean - congruent_RT.mean) %>%
  trim(variables = c("congruent_ACC.mean", "incongruent_ACC.mean")) %>%
  filter(!is.na(congruent_ACC.mean), !is.na(incongruent_ACC.mean))

data_flanker.bin <- bin.score(import)

data_flanker <- merge(data_flanker, data_flanker.bin, by = "Subject")

write_delim(data_flanker, path = "data/Flanker_scores.txt", delim = "\t", na = "")

rm(list=ls())
```

## Conclusions

There you have it. That was pretty dense but you made it through to the end. The Flanker and Stroop tasks is one of the more challenging tasks to score, so good job!

********

**Now before we get onto the really fun stuff, we need to discuss organization**

