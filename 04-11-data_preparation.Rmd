# Data Preparation: Messy to Tidy

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

## Overview

```{r eval = TRUE, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_messy-tidy.png"))
```

**Data Preparation** is all about creating *tidy* raw data files from *messy* raw data files. This part is not always fun and it can be very tempting to skip and go straight to creating a *scored* data file that is ready for data analysis.  However, I strongly advise against that. There are at least a few good reasons why:

1) Sometimes you actually NEED the raw trial level data. For instance, to do reliability or internal consistency analyses. 

2) Visualizing and analyzing trial-level data can help you better understand your data.

3) You or some other researcher might want to go back and run analyses starting from the trial-level data. Maybe you/they want to score the data slightly differently than you did before. 

4) Data storage. Storing and sharing your data in a *tidy* raw data format makes SO MUCH more sense than storing your *messy* raw data. If working with your *messy* raw data gives you a headache now imagine how much worse that is when you have not thought about that data for a long time.

And many other reasons

## Steps of Data Preparation

Data preparation occurs both during and immediately after data collection. As such, the R scripts and resultant *tidy* raw data files are stored in the Data Collection directory. 

There are **4 data preparation steps:**

1. **Organize the raw data files**. This involves moving the raw *.edat* files
    
    from - *Tasks/Session #/#. Task/* 
    
    to - *Data Files/Subject Files/Task/*.

2. **Merge** the individual *.edat* files into a single **task.emrg** file using *E-Merge*

3. **Export** the **task.emrg** file to a **task.txt** file so we can process the data in **R**

4. **Source** the **0_task_raw.R** *Scripts* 

    **0_task_raw.R** imports a *task.txt* file and creates a *tidy* raw data file, **task_raw.csv**

Step 4 is the only step that requires using R (R scripts can also be used to automate Step 1 but that will not be covered here). See [Data Preparation Instructions](http://englelab.gatech.edu/dataprep/){target="_blank"} for a detailed guide on how to perform the other steps. 

## Raw script template

The R script template to convert *messy* raw data files to *tidy* raw data files was explained in Chapter 9. In this Chapter we will walk through a script to create a *tidy* raw data file from a *messy* raw data file from a visual arrays task. Feel free to follow along creating your own R script.

This is the R script template downloaded by: `workflow::template(type = "raw")`

```{r eval = FALSE}
#### Setup ####
## Load packages
library(here)
library(readr)
library(dplyr)

## Set Import/Output Directories
import_dir <- "Data Files/Merged"
output_dir <- "Data Files"

## Set Import/Output Filenames
task <- "taskname"
import_file <- paste(task, ".txt", sep = "")
output_file <- paste(task, "raw.csv", sep = "_")
################

#### Import ####
data_import <- read_delim(here(import_dir, import_file),
                     "\t", escape_double = FALSE, trim_ws = TRUE)
################

#### Tidy raw data ####
data_raw <- data_import %>%
  rename() %>%
  filter() %>%
  mutate() %>%
  select()
#######################

#### Output ####
write_csv(data_raw, here(output_dir, output_file))
################

rm(list=ls())
```

## Setup

- Load packages

    Any packages required for this script are loaded at the top. For this task all we will need are the `here`, `readr`, and `dplyr` packages so we do not need to change anything.
    
- Set Import/Output Directories

    To make this example easier you will not have to actually import/output any files.
    
- Set Import/Output Filenames

    The only line we need to change here is the `task <- "taskname"` to `task <- "VAorient_S"`.
    
## Import

This section can stay exactly the same. As long as you are using these templates this line of code can always remain untouched.

In fact, if you are using these templates, every line of code except `task <- "taskname"` can likely stay exactly the same.

## Tidy raw data

This is the meat of the script, where the action happens. It will also be different for every task - obviously. We will walk through each line of code for this section using the visual arrays task as an example. 

It will be easier if you have an example data set so you can follow along. Type the following lines of code in your console.

```{r eval = TRUE}
library(englelab)
data_import <- visualarrays_messy
```

Now you should see the object (data frame) `data_import` in your environment window. Click on the data frame to view it. This is a *messy* raw data set from the visual arrays task. You are starting as though you had already imported the data file and ready to create the R code for the "Tidy raw data" section.

----

When creating a *tidy* raw data file it is best to keep as much of the data as possible - just reorganize and rename things (though we will see there is a little extra work that is needed for the visual arrays task). Therefore, we might as well keep the information from the practice trials as well. 

### `filter()`

Often times there might be rows of data in the *messy* raw data file from the instruction or end procedures. There is not much need to keep these so we typically remove them and only keep the rows for practice and real trials. The column that contains the information as to which procedure the current row corresponds to is typically named `Procedure[Trial]` but it may be different depending on how the task was programmed.

Let's go ahead and evaluate the unique values in this column. This can help us figure out how to apply the filter and also check if this is the correct column.

In the console window, type:

```{r eval = TRUE}
unique(data_import$`Procedure[Trial]`)
```

Practice trials correspond to the values "pracproc" and real task trials correspond to values "showproc". You have to know how the task was programmed to know this.

`filter()` is usually the first step in creating a *tidy* raw data file because it helps to first remove a bunch of unwanted and irrelevant rows. 

Often times before `filter()` I will go ahead and `rename()` `TrialProc[Trial]` simply to `TrialProc` because we might need to keep on referring to this column as we move forward.

```{r eval = TRUE}
data_raw <- data_import %>%
  rename(TrialProc = `Procedure[Trial]`) %>%
  filter(TrialProc == "showproc" | TrialProc == "pracproc")
```

We are only keeping rows that either have the value "showproc" or "pracproc" in the `TrialProc` column.

### `mutate()`

In the console type in 

```{r eval = TRUE}
unique(data_import$VisResponse.RESP)
```

The subject's response was coded as `5` or `6`. What the hell does that mean? They pressed the `5` or `6` key but that tells us nothing about what that response meant - which was a same vs. different judgment. This is only something you would know if you knew how the task was programmed. 

*This is a property of a messy raw data file - it requires knowledge of how the task was programmed. The purpose of creating a tidy raw data file is to allow understanding of the columns and values without knowledge of how the task was programmed*. 

In this case `5` corresponds to a "same" judgment and `6` corresponds to a "different" judgment. We can make this more clear by changing value `5` and `6` to `same` and `different` respectively.

These sort of changes are typically the next step in creating *tidy* raw data.

Let's also change `showproc` and `pracproc` to `real` and `practice` respectively.

```{r eval = TRUE}
data_raw <- data_import %>%
  rename(TrialProc = `Procedure[Trial]`) %>%
  filter(TrialProc == "showproc" | TrialProc == "pracproc") %>%
  mutate(TrialProc = case_when(TrialProc == "showproc" ~ "real",
                               TrialProc == "pracproc" ~ "practice"),
         Response = case_when(VisResponse.RESP == 5 ~ "same",
                              VisResponse.RESP == 6 ~ "different",
                              TRUE ~ as.character(NA)),
         CorrectResponse = case_when(VisResponse.CRESP == 5 ~ "same",
                                     VisResponse.CRESP == 6 ~ "different"))
```

Great!

For some tasks, that is really all that will be required at this step. For others, it might take a lot more. For instance, in the visual arrays task it is important to know whether a trial response was a 

- Correct Rejection

    When the correct response was "same" and the actual response was "same"

- False Alarm

    When the correct response was "same" but the actual response was "different"

- Miss

    When the correct response was "different" but the actual response was "same"

or 

- Hit

    When the correct response was "different" and the actual response was "different"

Let's go ahead and add this information the the *tidy* raw data.

```{r eval = TRUE}
data_raw <- data_import %>%
  rename(TrialProc = `Procedure[Trial]`) %>%
  filter(TrialProc == "showproc" | TrialProc == "pracproc") %>%
  mutate(TrialProc = case_when(TrialProc == "showproc" ~ "real",
                               TrialProc == "pracproc" ~ "practice"),
         Response = case_when(VisResponse.RESP == 5 ~ "same",
                              VisResponse.RESP == 6 ~ "different",
                              TRUE ~ as.character(NA)),
         CorrectResponse = case_when(VisResponse.CRESP == 5 ~ "same",
                                     VisResponse.CRESP == 6 ~ "different"),
         CorrrectRejection = case_when(CorrectResponse == "same" & 
                                         Response == "same" ~ 1,
                                       TRUE ~ 0),
         FalseAlarm = case_when(CorrectResponse == "same" & 
                                  Response == "different" ~ 1,
                                TRUE ~ 0),
         Miss = case_when(CorrectResponse == "different" & 
                            Response == "same" ~ 1,
                          TRUE ~ 0),
         Hit = case_when(CorrectResponse == "different" & 
                           Response == "different" ~ 1,
                         TRUE ~ 0))
```

### `select()`

The final step is to subset only the columns we need to keep. Right now there are way more columns than what we need. In the console type:

```{r eval = TRUE}
colnames(data_import)
```

Wow we started with over 90 columns! This is not usable. Let's reduce it.

```{r eval = TRUE}
data_raw <- data_import %>%
  rename(TrialProc = `Procedure[Trial]`) %>%
  filter(TrialProc == "showproc" | TrialProc == "pracproc") %>%
  mutate(TrialProc = case_when(TrialProc == "showproc" ~ "real",
                               TrialProc == "pracproc" ~ "practice"),
         Response = case_when(VisResponse.RESP == 5 ~ "same",
                              VisResponse.RESP == 6 ~ "different",
                              TRUE ~ as.character(NA)),
         CorrectResponse = case_when(VisResponse.CRESP == 5 ~ "same",
                                     VisResponse.CRESP == 6 ~ "different"),
         CorrectRejection = case_when(CorrectResponse == "same" & 
                                        Response == "same" ~ 1,
                                      TRUE ~ 0),
         FalseAlarm = case_when(CorrectResponse == "same" & 
                                  Response == "different" ~ 1,
                                TRUE ~ 0),
         Miss = case_when(CorrectResponse == "different" & 
                            Response == "same" ~ 1,
                          TRUE ~ 0),
         Hit = case_when(CorrectResponse == "different" & 
                           Response == "different" ~ 1,
                         TRUE ~ 0)) %>%
  select(Subject, TrialProc, Trial, SetSize, Accuracy = VisResponse.ACC, 
         Response, CorrectResponse, CorrectRejection, FalseAlarm, Miss, Hit, 
         SessionDate, SessionTime)
```

Now in the console window type in:

```{r eval = TRUE}
colnames(data_raw)
```

We have gone from over 90 columns to only 13 columns of data! I would say that we have *tidied* this *messy* data!

## Output

The final section of the script is to save the *tidy* raw data file. You do not need to change this section at all.

## Final 

The final script will look like:

```{r eval = FALSE}
#### Setup ####
## Load packages
library(here)
library(readr)
library(dplyr)

## Set Import/Output Directories
import_dir <- "Data Files/Merged"
output_dir <- "Data Files"

## Set Import/Output Filenames
task <- "taskname"
import_file <- paste(task, ".txt", sep = "")
output_file <- paste(task, "raw.csv", sep = "_")
################

#### Import ####
data_import <- read_delim(here(import_dir, import_file),
                     "\t", escape_double = FALSE, trim_ws = TRUE)
################

#### Tidy raw data ####
data_raw <- data_import %>%
  rename(TrialProc = `Procedure[Trial]`) %>%
  filter(TrialProc == "showproc" | TrialProc == "pracproc") %>%
  mutate(TrialProc = case_when(TrialProc == "showproc" ~ "real",
                               TrialProc == "pracproc" ~ "practice"),
         Response = case_when(VisResponse.RESP == 5 ~ "same",
                              VisResponse.RESP == 6 ~ "different",
                              TRUE ~ as.character(NA)),
         CorrectResponse = case_when(VisResponse.CRESP == 5 ~ "same",
                                     VisResponse.CRESP == 6 ~ "different"),
         CorrectRejection = case_when(CorrectResponse == "same" & 
                                        Response == "same" ~ 1,
                                      TRUE ~ 0),
         FalseAlarm = case_when(CorrectResponse == "same" & 
                                  Response == "different" ~ 1,
                                TRUE ~ 0),
         Miss = case_when(CorrectResponse == "different" & 
                            Response == "same" ~ 1,
                          TRUE ~ 0),
         Hit = case_when(CorrectResponse == "different" & 
                           Response == "different" ~ 1,
                         TRUE ~ 0)) %>%
  select(Subject, TrialProc, Trial, SetSize, Accuracy = VisResponse.ACC, 
         Response, CorrectResponse, CorrectRejection, FalseAlarm, Miss, Hit, 
         SessionDate, SessionTime)
#######################

#### Output ####
write_csv(data_raw, here(output_dir, output_file))
################

rm(list=ls())
```

Wow congrats! You have created an R script to convert a *messy* raw data file to a *tidy* raw data fie.

## masterscript.R

The masterscript.R file in a Data Collection directory might look something like

```{r eval = FALSE}
## Data Preparation for StudyName

#############################################
#------ 0. "messy" to "tidy" raw data ------#
#############################################

source("R Scripts/0_rapm_raw.R", echo=TRUE)
source("R Scripts/0_lettersets_raw.R", echo=TRUE)
source("R Scripts/0_numberseries_raw.R", echo=TRUE)
source("R Scripts/0_ospan_raw.R", echo=TRUE)
source("R Scripts/0_symspan_raw.R", echo=TRUE)
source("R Scripts/0_rotspan_raw.R", echo=TRUE)
source("R Scripts/0_antisaccade_raw.R", echo=TRUE)
source("R Scripts/0_sact_raw.R", echo=TRUE)
source("R Scripts/0_visualarrays_raw.R", echo=TRUE)

rm(list=ls())
#############################################

```

This allows you to run all the scripts to create *tidy* raw data files all in one place.

----

The organization structure and workflow for data preparation is depicted below:

```{r echo=FALSE, eval = TRUE, out.width='40%', fig.align='left'}
knitr::include_graphics(rep("images/workflows/repository_datacollection.png"))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
rm(list=ls())
```
