# (PART) Analyzing Data {-}

# Project Organization

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

```{r echo=FALSE, eval = TRUE, out.width='75%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_all.png"))
```

In the EngleLab, we often conduct large-scale data collection studies in which there are many research projects (with their own research questions and analyses) going on at once. Although many of the tasks will be shared between these research projects, there will be a unique set of tasks for each project. Ideally, for the tasks that are shared between research projects, the scoring methods and data cleaning criteria are the same across projects. In practice, this might not actually be the case but we do try to standardize across projects when we can. 

In the past, the way we achieved this was sharing a data file that contained the scores for every task across research projects. However, there are some serious issues with this. 

1. You cannot calculate reliability with just the task scores, you need the raw data.

2. There is no transparency as to how the scores were calculated. Although whoever created the shared data file might know, this information will get lost over time. This is not good for reproducibility and open science practices.

3. You cannot go back and calculate task scores using different criteria and decisions, again you need the raw data to do that. 

R provides a convenient way to solve these problems. The solution is to share only the raw data files and R scripts used to score the tasks. 

- Sharing the raw data files allows you to calculate reliability and to go back and make different decisions on how to score the task. It also makes it much easier for someone to go back and use the data from a research project and do reanalyses. 

- Sharing the R scripts allows for transparency in how task scores were calculated.

In other words, **DO NOT SHARE** scored data files from one research project to another.

----

## Data Collection vs. Data Analysis

The organizational structure we use in the lab is to have separate directories for the data collection study and for the various data analysis projects. The **Data Collection** directory will serve as a central repository for the raw data files from the study. A new **Data Analysis** directory can be created by copying over the raw data files for the tasks relevant to that research project. This relationship between Data Collection and Data Analysis Repositories are depicted here:

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_relations.png"))
```

It is critical that you copy the raw data files only from the data collection directory to a data analysis directory. You do not copy data files from one data analysis directory to another.

You can copy R Scripts from one data analysis directory to another. For instance, if you or someone else already created an R Script (for a separate data analysis project) to score a particular task that you are also using, then there is no problem in copying the R Script.

This is because you can reproduce the data files from your R Scripts but not vice-versa. If you copy data files you lose reproducibility and the transparency of how the data file was created. 

## Data Preparation

**This step is performed in the data collection directory**.

At some point you will need to start analyzing the data. However, you first need to prepare the data so that it is ready to analyze. There are several steps in this process and it can be quite tedious. Nevertheless, undergraduate RAs are trained on how to do most of these steps, so recruit their help. There are also [step-by-step instructions for Data Preparation.](http://englelab.gatech.edu/dataprep/){target="_blank"}

Once you are ready for **Data Preparation** you will need to create a **Data Files** and **R Scripts** folders in the **Data Collection** directory. 

There are two scenarios in which you may need to start processing and analyzing data:

- Before data collection has finished

- After data collection has finished

For both of these scenarios, you will start with messy raw data files in some file format. Messy raw data files are hard to understand, have poor column and value labels, contain way too many columns and rows, and are just hard to work with. Data preparation is all about getting raw data files that are easy to work with.

```{r echo=FALSE, eval = TRUE, out.width='35%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_messy-tidy.png"))
```

The end product of the data preparation stage is *tidy* raw data files. Tidy raw data files are easy to understand, have sensible column and value labels, contain only relevant columns and rows, and are very easy to work with.

The **Data Preparation** stage is only required because the data files created from the E-Prime or other software program are usually not in a format that is easy to use or understand. I am referring to this format as a **messy raw data** file. Also, there are typically other preparation steps one needs to take before they can start looking at the data. These might include merging individual subject data files and exporting the data to a non-proprietary format so we can import the data into R. The purpose of the data preparation stage is simply to create **tidy raw data** files from the messy raw data files.

**Tidy raw data** files are easy to use and understand. There will be one row per trial, column labels that are easy to understand (e.g. Trial, Condition, RT, Accuracy, etc.), and values in columns that make sense. If values in a column are categorical, then the category names will be used rather than numerical values. Ideally, someone not involved in the research project should be able to look at a tidy raw data file and understand what each column represents, and what the values in the column refer to.

## Data Analysis

Okay, now say you are ready to analyze some data! It is tempting to do your analysis in the original *Data Collection* directory where the data are already stored. I highly suggest not doing this. You will be mixing up a *Data Collection* directory with a *Data Analysis* directory. This distinction is particularly important when we conduct large-scale studies with many data analysis projects for a single data collection study.

Instead, you should copy over the *tidy* raw data files from the data preparation stage to a separate *Data Analysis* directory.

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_copyover.png"))
```

You also might as well create an **Archival Backup** of the **Data Collection** directory on some other hard drive. That way you are at less risk of a hard drive crashing and losing all your precious data.

In the **Data Analysis** directory you have three main folders:

* Data Files

* R Scripts

* Results

**Data Files** is where you will store tidy raw data files, scored data files, and a single merged data file ready for statistical analysis.

It is advisable to store all your **R scripts** in one single place. I also like to prefix them with numbers corresponding to the order they need to be executed - that way they will be organized in an easy to find way.

Finally, you should create a separate folder to hold all your outputs from statistical analysis and data visualization in a **Results** folder.

There are three main stages to data analysis; **1**) scoring, cleaning, and merging data **2**) data visualization, and **3**) statistical analyses. Data analysis tends to be more cyclic and iterative therefore you may end up going back and forth between these stages.

```{r echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_dataanalysis.png"))
```

The first stage takes the tidy raw data files from the data preparation stage and converts them into a scored data file, usually by aggregating performance across trials. Data cleaning procedures (such as removing outliers) also occurs during this stage. The format of the scored data file will depend on the type of statistical analysis one plans on performing.

The data analysis stages usually occurs in tandem with one another. Visualizing our data, running statistical analyses, visualizing our statistical models, etc. If we have more exploratory data, then based on these visualizations and analyses we may decide that we want to use different scoring or cleaning procedures. Or we want to explore our data to further understand our findings. We may then go back to the scoring and cleaning stage, and on and on. 

You may have other folders in your **Data Analysis** directory:

* Figures

* Manuscript

* Presentations

**Figures** is where any image files, that are used in a manuscript or presentations, are stored. You may also have a **PowerPoint** file stored here.

**Manuscript** is where the manuscript and any drafts for this project are stored.

**Presentations** is where any **PowerPoint** presentation files related to this project can be stored.

These other folders are more optional.

----

## `workflow` package

I will show you how to automatically create **Data Collection** and **Data Analysis** directories using RStudio Projects and my [**workflow** package](https://dr-jt.github.io/workflow/){target="_blank"}

### Install

Install the `workflow` package

```{r eval = FALSE}
# if you do not have devtools installed then do this first
install.packages("devtools")
# install workflow
devtools::install_github("dr-JT/workflow")
```

### Create a New R Project

One of the features this package allows is for you to automatically setup the organization of a **Data Collection** or **Data Analysis** project.

Navigate to __File -> New Project... -> New Directory 

And browse until you see the option: __Research Study__

Click on that and you will see a dialogue box like this

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_proj_template.png"))
```

Here are what the different options mean:

* __Directory Name__: This will be the name of the folder for the study

* __Create project as subdirecotry of__: Select Browse and choose where the folder (Directory Name) should be located.

* __Repository Type__: **data collection** or **data analysis**. Depending on which one you choose it will create the corresponding directories and files:

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/repository_type.png"))
```

Notice that if you choose the **data collection** repository it will download a generic template for *converting "messy" raw data files to "tidy" raw data files*. And if you choose the **data analysis** repository it will download generic templates for *creating scored data files from "tidy" raw data files* and to *merge* the Scored data files into one final data file. 

* __# of Sessions__: How many sessions will the study have? This will create folders in the `Tasks` folder for each session. For instance, if there will be 4 sessions it will create the the folders "Session 1", "Session 2", "Session 3", and "Session 4". Obviously this is not needed for a **data analysis** repository.

* __Other Directories__: I talked earlier about some other folders you may want to include in a **Data Analysis** repository. Well you can automatically add them here. 

----

```{r eval = TRUE, echo=FALSE}
rm(list=ls())
```

