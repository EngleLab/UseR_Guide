[
["index.html", "EngleLab: UseR Guide", " EngleLab: UseR Guide Jason Tsukahara R is becoming a popular tool for doing statistical analyses across many scientific disciplines. It has many advantages but there can be a considerable learning curve. This guide is meant to help you become proficient in R and quickly learn standard data processing tasks that are common in the EngleLab. "],
["how-to-use-this-guide.html", "How to use this guide", " How to use this guide The best way to start learning R is if you have a data set to work on. It is true that, at first, it may take you longer to analyze data using R than your typical route of Excel, SPSS, or EQS. Because of this, it can be tempting to use those other programs to quickly analyze some data. You may even tell yourself that you will go back and do the analysis in R later when you have more time. But, be honest with yourself, will you really? Also, the daunting uncertainty of how to do anything in R may also make you reluctant to start using R with data you have now. All this is just prioritizing short-term gains over long-term gains. This is probably the biggest barrier that you will have to learning R. STOP IT! This attitude will only keep you behind the times not just on using statistial software but other areas of your research career. It is 100% okay to take longer to analyze some data if you are also aquiring skills that will greatly benefit you in the long-term. Randy is okay with this, you need to be too. I would not even argue that you should stop using other programs. They have their own advantages. In my experience it is not so much of R vs. SPSS but rather R offers a functionality that SPSS just sucks at. And that is working with the data, transforming variables, merging data, filtering, grouping, aggregating scores, etc. Everything you do with your data immediately prior to running the ANOVA, correlation, regression, or latent analysis. R is excellent at this. So if you still want to use SPSS or EQS for statistical analysis you can do so. But do everything prior to that in R. Use R to work with your data and use SPSS or EQS to conduct statistical analyses. R is also amazing at data visualization. I would take advantage of that as well. There are two general ways you might use this guide: Going through each chapter step-by-step as a tutorial Referencing sections of the guide as you work on your own projects No matter what, you should first read through Section III: Reproducible Workflows If you need to prepare your messy raw data files so that you can start analyzing data, then you can move on to Section IV: Data Preparation. This will get you working on your project in R as soon as possible. You can always reference Section I and Section II when you need to. If your data files are already prepared but you still need to clean and score your data, then you can move on to Section V: Cleaning and Scoring Data. If you need to work on statistical analyses and data visualization then you can move on to Section VI: Data Visualization and Section VII: Statistical Analysis. In these sections you only need to go through the Chapters that are relevant to your specific analysis. "],
["reseach-project-workflow.html", "Reseach Project Workflow", " Reseach Project Workflow The image below represents the general data processing stages required to go from raw data to data visualization and statistical analyses. The Data Preparation stage is only required because the data files created from the E-Prime or other software program are usually not in a format that is easy to use or understand. I am referring to this format as a “messy” data file. Also, there are typically other preparation steps one needs to take before they can start looking at the data. These might include merging individual subject data files and exporting the data to a non-proprietary format so we can import the data into R. The purpose of this stage is simply to create “tidy” raw data files. “Tidy” raw data files are easy to use and understand. There will be one row per trial, column labels that are easy to understand (e.g. Trial, Condition, RT, Accuracy, etc.), and values in columns that make sense. If values in a column are categorical, then the category names will be used rather than numerical values. Ideally, someone not involved in the research project should be able to look at a “Tidy” raw data file and understand what each column represents, and what the values in the column refer to. Traditionally we have not cared so much about the data preparation stage. But it is required and there are many advantages to saving and storing “tidy” raw data files. What we actually care about is the Data Analysis phase. There are three main stages to data analsysis; 1) scoring and cleaning raw data files, 2) data visualization, and 3) statistical analyses. Data analysis tends to be more cyclic and iterative therefore you may end up going back and forth between these stages. The first stage takes the “tidy” raw data files from the data preparation stage and converts them into a scored data file, usually by aggregating performance across trials. Data cleaning procedures (such as removing outliers) also occurs during this stage. The format of the scored data file will depend on the type of statistical analysis one plans on performing. The second and third stage usually occurs in tandem with one another. Visualizing our data, running statistical analyses, visualizing our statistical models, etc. Based on these visualizations and analyses we may decide that we want to use different scoring or cleaning procedures. Or we want to explore our data to further understand our findings. We may then go back to the scoring and cleaning stage, and on and on. The final phase in a research project is to Write up a Manuscript to share your study and findings with the scientific community. It is also a good idea to Share your Data, R scripts, and results. The Open Science Framework is a good place to openly share your projects with other researchers. "],
["installation.html", "Chapter 1 Installation 1.1 Installing R 1.2 Installing R Studio 1.3 The R Studio Environemnt 1.4 R Studio Settings", " Chapter 1 Installation 1.1 Installing R First you need to download the latest version of R from their website https://www.r-project.org Select CRAN on the left, just under Download Select the first option under 0-Cloud Select the download option depending on your computer Select the base installation (for Windows) or the Latest Release (for Mac) Open and Run the installation file 1.2 Installing R Studio The easiest way to interact with R is through the R Studio environment. To do this you need to install R Studio Select the Free version of R Studio Desktop Select the download option depending on your computer 1.3 The R Studio Environemnt Go ahead an open the RStudio application on your computer. When you open a fresh session of RStudio there are 3 window panes open. The Console window, the Environment window, and the Files window. Go ahead and navigate to File -&gt; New File -&gt; R Script. You should now see something similar to the image below There are 4 window panes and each one has it’s own set of tabs associated with it: The Console window (the bottom left window pane) is where code is executed and output is displayed. The Source window (the top left window pane) is where you will write your code to create a script file. When you open a new script file you will see a blank sheet where you can start writing the script. When you execute lines of code from here you will see it being executed in the Console window. The Source window is also where you can view dataframes you have just imported or created. In the image above, notice the different tabs in the Source window. There are two “Untitled” script files open and one dataframe called ‘data’. The Environment window (top right window pane) is where you can see any dataframes, variables, or functions you have created. Go ahead and type the following in your Console window and hit enter. hello &lt;- &quot;hello&quot; You should now see the object hello in the Environment window pane The Files window (the bottom right window pane) is where you can see your computer’s directories, plots you create, manage packages, and see help documentation. 1.4 R Studio Settings There are a few changes to R Studio settings I suggest you make. I will not go into why these are a good idea - so just do what I say! If you want to know you can talk to me about it. Navigate to Tools -&gt; Global Options Change the settings to look like this: Be sure to set ‘Save workspace to .RData on exit’ to Never You can also change the “Editor Theme” if you navigate to the “Appearance” tab in Settings. Dark themes are easier on the eyes. I use Material dark theme. Now you are ready to start writing some R code! "],
["basic-r.html", "Chapter 2 Basic R 2.1 Creating R objects 2.2 If…then Statements 2.3 R Packages 2.4 More R Basic Resources", " Chapter 2 Basic R This chapter will cover the basics of how to assign values to objects, create and extract information from vectors, lists, and dataframes. If you have not done so already, open a new R script file. To create a new R script go to File -&gt; New File -&gt; R Script This should have opened a blank Script window called Untitled. The Script window is your workspace. This is where you will write, edit, delete, re-write, your code. To follow along with the tutorial, you should type the lines of code I display in the tutorial into your script. Go ahead and save your empty script as 2_basics.R 2.1 Creating R objects In R, everything that exists is an object and everything you do to objects are functions. You can define an object using the assignment operator &lt;-. Everything on the left hand side of the &lt;- assignment operator is an object. Everything on the right hand side of &lt;- are functions or values. Go ahead and type the following two lines of code in your script string &lt;- &quot;hello&quot; string ## [1] &quot;hello&quot; You can execute/run a line of code by placing the cursor anywhere on the line and press Ctrl + Enter. Go ahead an run the two lines of code. In this example, the first line creates a new object called string with a value of “hello”. The second line simply prints the output of string to the Console window. In the second line there is no assignment operator. When there is no &lt;- this means you are essentially just printing to the console. You can’t do anything with stuff that is just printed to the console, it is just for viewing purposes. For instance, if I wanted to calculate 1 + 2 I could do this by printing it to the console 1 + 2 ## [1] 3 However, if I wanted to do something else with the result of that calculation then I would not be able to unless I assigned the result to an object using &lt;- result &lt;- 1 + 2 result &lt;- result * 5 result ## [1] 15 The point is, you are almost always going to assign the result of some function or value to an object. Printing to the console is not very useful. Almost every line of code, then, will have an object name on the left hand side of &lt;- and a function or value on the right hand side of &lt;- In the first example above, notice how I included &quot; &quot; around hello. This tells R that hello is a string, not an object. If I were to not include &quot; &quot;, then R would think I am calling an object. And since there is no object with the name hello it will print an error string &lt;- hello ## Error in eval(expr, envir, enclos): object &#39;hello&#39; not found Do not use &quot; &quot; for Numerical values a &lt;- &quot;5&quot; + &quot;1&quot; ## Error in &quot;5&quot; + &quot;1&quot;: non-numeric argument to binary operator You can execute lines of code by: Typing them directly into the Console window Typing them into the Script window and then on that line of code pressing Ctrl+Enter. With Ctrl+Enter you can execute one line of your code at a time. Clicking on Source at the top right of the Script window. This will run ALL the lines of code contained in the script file. It is important to know that EVERYTHING in R is case sensitive. a &lt;- 5 a + 5 ## [1] 10 A + 5 ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found 2.1.1 Classes Classes are types of values that exist in R: character &quot;hello&quot;, &quot;19&quot; numeric (or double) 2, 32.55 integer 5, 99 logical TRUE, FALSE To evaluate the class of an object you can use the typeof() typeof(a) ## [1] &quot;double&quot; To change the class of values in an object you can use the function as.character() , as.numeric() , as.double() , as.integer() , as.logical() functions. as.integer(a) ## [1] 5 as.character(a) ## [1] &quot;5&quot; as.numeric(&quot;hello&quot;) ## Warning: NAs introduced by coercion ## [1] NA 2.1.2 Vectors Okay so now I want to talk about creating more interesting objects than just a &lt;- 5. If you are going to do anything in R it is important that you understand the different data types and data structures you can use in R. I will not cover all of them in this tutorial. For more information on data types and structures see this nice Introduction to R Vectors contain elements of data. The length of a vector is the number of elements in the vector. For instance, the variable a we created earlier is actually a vector of length 1. It contains one element with a value of 5. Now let’s create a vector with more than one element. b &lt;- c(1,3,5) c() is a function. Functions contain arguments that are inputs for the function. Arguments are separated by commas. In this example the c() fucntion concatenates the arguments (1, 3, 5) into a vector. We are passing the result of this function to the object b. What do you think the output of b will look like? b ## [1] 1 3 5 You can see that we now have a vector that contains 3 elements; 1, 3, 5. If you want to reference the value of specific elements of a vector you use brackets [ ]. For instance, b[2] ## [1] 3 The value of the second element in vector b is 3. Let’s say we want to grab only the 2nd and 3rd elements. We can do this at least two differnt ways. b[2:3] ## [1] 3 5 b[-1] ## [1] 3 5 Now, it is important to note that we have not been changing vector b. If we display the output of b, we can see that it still contains the 3 elements. b ## [1] 1 3 5 To change vector b we need to define b as vector b with the first element removed b &lt;- b[-1] b ## [1] 3 5 Vector b no longer contains 3 elements. Now, let’s say we want to add an element to vector b. c(5,b) ## [1] 5 3 5 Here the c() fucntion created a vector with the value 5 as the first element followed by the values in vector b Or we can use the variable a that has a value of 5. Let’s add this to vector b b &lt;- c(a,b) b ## [1] 5 3 5 What if you want to create a long vector with many elements? If there is a pattern to the sequence of elements in the vector then you can create the vector using seq() seq(0, 1000, by = 4) ## [1] 0 4 8 12 16 20 24 28 32 36 40 44 48 ## [14] 52 56 60 64 68 72 76 80 84 88 92 96 100 ## [27] 104 108 112 116 120 124 128 132 136 140 144 148 152 ## [40] 156 160 164 168 172 176 180 184 188 192 196 200 204 ## [53] 208 212 216 220 224 228 232 236 240 244 248 252 256 ## [66] 260 264 268 272 276 280 284 288 292 296 300 304 308 ## [79] 312 316 320 324 328 332 336 340 344 348 352 356 360 ## [92] 364 368 372 376 380 384 388 392 396 400 404 408 412 ## [105] 416 420 424 428 432 436 440 444 448 452 456 460 464 ## [118] 468 472 476 480 484 488 492 496 500 504 508 512 516 ## [131] 520 524 528 532 536 540 544 548 552 556 560 564 568 ## [144] 572 576 580 584 588 592 596 600 604 608 612 616 620 ## [157] 624 628 632 636 640 644 648 652 656 660 664 668 672 ## [170] 676 680 684 688 692 696 700 704 708 712 716 720 724 ## [183] 728 732 736 740 744 748 752 756 760 764 768 772 776 ## [196] 780 784 788 792 796 800 804 808 812 816 820 824 828 ## [209] 832 836 840 844 848 852 856 860 864 868 872 876 880 ## [222] 884 888 892 896 900 904 908 912 916 920 924 928 932 ## [235] 936 940 944 948 952 956 960 964 968 972 976 980 984 ## [248] 988 992 996 1000 Vectors can only contain elements of the same “class”. d &lt;- c(1, &quot;2&quot;, 5, 9) d ## [1] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;9&quot; as.numeric(d) ## [1] 1 2 5 9 2.1.3 Factors Factors are special types of vectors that can represent categorical data. You can change a vector into a factor object using factor() factor(c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;)) ## [1] male female male male female female male ## Levels: female male factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;)) ## [1] high low medium high high low medium medium ## Levels: high low medium f &lt;- factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;), levels = c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;)) f ## [1] high low medium high high low medium medium ## Levels: high medium low 2.1.4 Lists Lists are containers of objects. Unlike Vectors, Lists can hold different classes of objects. list(1, &quot;2&quot;, 2, 4, 9, &quot;hello&quot;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] &quot;hello&quot; You might have noticed that there are not only single brackets, but double brackets [[ ]] This is because Lists can hold not only single elements but can hold vectors, factors, lists, dataframes, and pretty much any kind of object. l &lt;- list(c(1,2,3,4), &quot;2&quot;, &quot;hello&quot;, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) l ## [[1]] ## [1] 1 2 3 4 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] &quot;hello&quot; ## ## [[4]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; You can see that the length of each element in a list does not have to be the same. To reference the elements in a list you need to use the double brackets [[ ]]. l[[1]] ## [1] 1 2 3 4 To reference elements within list elements you use double brackets followed by a single bracket l[[4]][2] ## [1] &quot;b&quot; You can even give names to the list elements person &lt;- list(name = &quot;Jason&quot;, phone = &quot;123-456-7890&quot;, age = 23, favorite_colors = c(&quot;blue&quot;, &quot;red&quot;, &quot;brown&quot;)) person ## $name ## [1] &quot;Jason&quot; ## ## $phone ## [1] &quot;123-456-7890&quot; ## ## $age ## [1] 23 ## ## $favorite_colors ## [1] &quot;blue&quot; &quot;red&quot; &quot;brown&quot; And you can use the names to reference elements in a list person[[&quot;name&quot;]] ## [1] &quot;Jason&quot; person[[&quot;favorite_colors&quot;]][3] ## [1] &quot;brown&quot; 2.1.5 Data Frames You are probably already familiar with this type of data structure. SPSS and Excel uses this type of structure. It is just rows and columns of data. A data table! This is the format that is used to perform statiscital analyses on. So let’s create a data frame so you can see what one looks like in RStudio data &lt;- data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) data ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 You can view the Data Frame by clicking on the object in the Environment window or by executing the command View(data) Notice that it created three columns labeled id, x, and y. Also notice that since we only specified a vector of length 2 for x this column is coerced into 10 rows of repeateding “a” and “b”. All columns in a dataframe must have the same number of rows. You can use the $ notation to reference just one of the columns in the dataframe data$y ## [1] 10 20 30 40 50 60 70 80 90 100 Alternatively you can use data[&quot;y&quot;] ## y ## 1 10 ## 2 20 ## 3 30 ## 4 40 ## 5 50 ## 6 60 ## 7 70 ## 8 80 ## 9 90 ## 10 100 To reference only certain rows within a column data$y[1:5] ## [1] 10 20 30 40 50 data[1:5,&quot;y&quot;] ## [1] 10 20 30 40 50 2.2 If…then Statements If…then statements are useful for when you need to execute code only if a certain statement is TRUE. For instance,… First we need to know how to perform logical operations in R Okay, we have this variable a a &lt;- 5 Now let’s say we want to determine if the value of a is greater than 3 a &gt; 3 ## [1] TRUE You can see that the output of this statement a &gt; 3 is TRUE Here is a list of logical operations in R Now let’s write an if…then statement. If a is greater than 3, then multiply a by 2. if (a&gt;3){ a &lt;- a*2 } a ## [1] 10 The expression that is being tested is contained in parentheses, right after the if statement. If this expression is evaluated as TRUE then it will perform the next line(s) of code. The { is just a way of encasing multiple lines of code within one if statement. The lines of code then need to be closed of with }. In this case, since we only had one line of code b &lt;- a*2 we could have just written it as. a &lt;- 5 if (a&gt;3) a &lt;- a*2 a ## [1] 10 What if we want to do something to a if a is NOT greater than 3? In other words… if a is greater than 3, then multiple a by 2 else set a to missing a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 You can keep on chaining if…then… else… if… then statements together. a &lt;- 5 if (is.na(a)){ print(&quot;Missing Value&quot;) } else if (a&lt;0){ print(&quot;A is less than 0&quot;) } else if (a&gt;3){ print(&quot;A is greater than 3&quot;) } ## [1] &quot;A is greater than 3&quot; 2.3 R Packages R comes with a basic set of functions. All the functions we have used so far are part of the R basic functions. But when you want to start doing more complex operations it would be nice to have more complex functions. This is where R Packages come in… An R Package is simply a collection of functions - that usually have some common theme to them. Now the most wonderful thing about R is that other R users have developed tons of packages with functions they created themselves. For instance, a group of users have developed an R package called lavaan that makes it extremely easy to conduct SEM in R. 2.3.1 Installing and Loading R Packages R packages are easy to install and load. You just need to know the name of the package. install.packages(&quot;name_of_package&quot;) or for multiple packages at once install.packages(c(&quot;package1&quot;, &quot;package2&quot;, &quot;package3&quot;)) Installing the package does not mean you can start using the functions. To be able to use the function you need to then load the package library of functions as such library(name_of_package) When loading packages you do not have to incase the package name in &quot; &quot; 2.4 More R Basic Resources For additional tips in the basics of coding R see: https://ramnathv.github.io/pycon2014-r/visualize/README.html https://www.datacamp.com/courses/free-introduction-to-r/?tap_a=5644-dce66f&amp;tap_s=10907-287229 http://compcogscisydney.org/psyr/ http://r4ds.had.co.nz/workflow-basics.html The next chapter is optional and will cover more intermediate R programming "],
["intermediate-r.html", "Chapter 3 Intermediate R 3.1 For Loops 3.2 Functions", " Chapter 3 Intermediate R This chapter will cover more intermediate R programming, such as for loops, and functions. Save a new R script as 3_intermediate.R 3.1 For Loops For loops allow you iterate the same line of code over multiple instances. Let’s say we have a vector of numerical values c &lt;- c(1,6,3,8,2,9) c ## [1] 1 6 3 8 2 9 and want perform an if…then operation on each of the elements. Let’s use the same if…then statement we used above. If the element is greater than 3, then multiply it by 2 - else set it to missing. Let’s put the results of this if…then statement into a new vector d What we need to do is loop this if…then statement for each element in c We can start out by writing the for loop statement for (i in seq_along(c)){ } This is how it works. The statement inisde of parathenses after for contains two statements separated by in. The first statement is the variable that is going to change it’s value over each iteration of the loop. You can name this whatever you want. In this case I chose the label i. The second statement defines all the values that will be used at each iteration. The second statement will always be a vector. In this case the vector is seq_along(c). seq_along() is a function that creates a vector that contains a sequence of numbers from 1 to the length of the object. In this case the object is the vector c, which has a length of 6 elements. Therefore seq_along(c), creates a vector containing 1, 2, 3, 4, 5, 6. The for loop will start with i defined as 1, then on the next iteration the value of i will be 2 … and so until the last element of seq_along(c), which is 6. We can see how this is working by printing ‘i’ on each iteration. for (i in seq_along(c)){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 You can see how on each iteration it prints the values of seq_along(c) from the first element to the last element. What we will want to do is, on each iteration of the for loop, access the ith element of the vector c. Recall, you can access the element in a vector with [ ], for instance c[1]. Let’s print each ith element of c. for (i in seq_along(c)){ print(c[i]) } ## [1] 1 ## [1] 6 ## [1] 3 ## [1] 8 ## [1] 2 ## [1] 9 Now instead of printing i the for loop is printing each element of vector c. Let’s use the same if…then statement as above a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 But instead we need to replace a with c[i] For now let’s just print() the output of the if… then statement. for (i in seq_along(c)){ if (c[i] &gt; 3){ print(c[i]*2) } else { print(NA) } } ## [1] NA ## [1] 12 ## [1] NA ## [1] 16 ## [1] NA ## [1] 18 Now for each element in c, if it is is greater than 3, then multiply it by 2 - else set as missing value. You can see that on each iteration the output is either the ith element of c multiplied by 2 or NA. But just printing things to the console is useless! Let’s overwright the old values in c with the new values. for (i in seq_along(c)){ if (c[i] &gt; 3){ c[i] &lt;- c[i]*2 } else { c[i] &lt;- NA } } But what if we want to preserve the original vector c? Well we need to put it into a new vector, let’s call it vector d. This get’s a little more complicated but is something you might find yourself doing fairly often so it is good to understand how this works. But if you are goind to do this to a “new” vector that is not yet created you will run into an error. c &lt;- c(1,6,3,8,2,9) for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } You first need to create vector d - in this case we can create an empty vector. d &lt;- c() So the logic of our for loop, if…then statement is such that; on the ith iteration - if c[i] is greater than 3, then set d[i] to c[i]*2 - else set d[i] to NA. c &lt;- c(1,6,3,8,2,9) d &lt;- c() for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } c ## [1] 1 6 3 8 2 9 d ## [1] NA 12 NA 16 NA 18 Yay! Good job. 3.2 Functions Basically anything you do in R is by using functions. In fact, learning R is just learning what functions are available and how to use them. Not much more to it than that. You have only seen a couple of functions at this point. In this chapter, a common function used was c(). This function simply concatenates a series of numerical or string values into a vector. c(1,6,3,7). Functions start with the name of the function followed by parentheses function_name(). Inside the () is where you specify certain arguments separted by commas , . Some argruments are optional and some are required for the function to work. For example, another function you saw last chapter was data.frame(). This function creates a dataframe with the columns specified by arguments. data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 The arguments id, x, and y form the columns in the dataframe. These arguments themselves used functions. For instance y used the function seq(). This function creates a sequence of numbers in a certain range at a given interval. Sometimes arguments are not defined by an =. The first two arguments in in seq() specify the range of 10 to 100. The third argument by specified the interval to be 10. So seq(10, 100, by = 10) creates a sequence of numbers ranging from 10 to 100 in intervals of 10. seq(10, 100, by = 10) ## [1] 10 20 30 40 50 60 70 80 90 100 In the seq() function the by argument is not required. This is because there is a default by value of 1. seq(10, 100) ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [17] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## [33] 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 ## [49] 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 ## [65] 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 ## [81] 90 91 92 93 94 95 96 97 98 99 100 Obviously if you want to specify a different interval, then you will need to specify by =. 3.2.1 Creating Your Own Functions This section is optional. It will go over how to create your own functions. Even if you do not want to get too proficient in R, it can be a good idea to know how to create your own function. It also helps you better understand how functions actaully work. We are going to create a function that calculates an average of values. To define a function you use the function() and assign the output of function() to an object, which becomes the name of the function. For instance, function_name &lt;- function(){ } This is a blank function so it is useless. Before we put stuff inside of a function let’s work out the steps to calculate an average. Let’s say we have an array a that has 10 elements a &lt;- c(1,7,4,3,8,8,7,9,2,4) a ## [1] 1 7 4 3 8 8 7 9 2 4 To calculate an average we want to take the sum of all the values in a and divide it by the number of elements in a. To do this we can use the sum() and length() functions. sum(a) ## [1] 53 length(a) ## [1] 10 sum(a)/length(a) ## [1] 5.3 Easy! So now we can just put this into a function. average &lt;- function(x){ avg &lt;- sum(x)/length(x) return(avg) } When creating a function, you need to specify what input arguments the function is able to take. Here were are specifying the argument x. You can use whatever letter or string of letters you want, but a common notation is to use x for the object that is going to be evaluated by the function. Then, inside the function we use the same letter x to calculate the sum() and length() of x. What this means is that Arguments specified in a function become objects (or variables) passed inside the function You can create new objects inside a function. For instance we are creating an object, avg. However, these objects are created only inside the environment of the function. You cannot use those objects outside the function and they will not appear in your Environment window. To pass the value of an object outside of the function, you need to specify what you want to return() or what is the outpute of the function. In this case it is the object avg that we created inside the function. Let’s see the function in action average(a) ## [1] 5.3 Cool! You created your first function. Becuase the function only takes one argument x it knows that whatever object we specify in average() is the object we want to evaluate. But what if our vector contains missing values? b &lt;- c(1,NA,4,2,7,NA,8,4,9,3) average(b) ## [1] NA Uh oh. Here the vector b contains two missing values and the function average(b) returns NA. This is becuase in our function we use the function sum() without specifiying to ignore missing values. If you type in the console ?sum you will see that there is an argument to specify whether missing values should be removed or not. The default value of this argument is FALSE so if we want to remove the missing values we need to specify na.rm = TRUE. It is a good idea to make your functions as flexible as possible. Allow the user to decide what they want to happen. For instance, it might be the case that the user wants a value of NA returned when a vector contains missing values. So we can add an argument to our average() function that allows the user to decide what they want to happen; ignore missing values or return NA if missing values are present. Let’s label this argument na.ignore. We could label it na.rm like the sum() function but for the sake of this Tutorial I want you to learn that you can label these arguments however you want, it is arbitrary. The label should make sense however. Before we write the function let’s think about what we need to change inside the function. Basically we want our new argument na.ignore to change the value of na.rm in the sum() function. If na.ignore is TRUE then we want na.rm = TRUE. Remember that arguments become objects inside of a function. So we will want to change: avg &lt;- sum(x)/length(x) to avg &lt;- sum(x, na.rm = na.ignore)/length(x) Let’s try this out on our vector b na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b) ## [1] 3.8 We can test if our average function is calculating this correctly by using the actual base R function mean(). mean(b, na.rm = TRUE) ## [1] 4.75 Uh oh. We are getting different values. This is because length() is also not ignoring missing values. The length of b, is 10. The length of b ignoring missing values is 8. Unfortunately, length() does not have an argument to specify we want to ignore missing values. How we can tell length() to ignore missing values is by length(b[!is.na(b)]) ## [1] 8 This is saying, evaluate the length of elements in b that are not missing. Now we can modify our function with na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b[!is.na(b)]) ## [1] 4.75 to get average &lt;- function(x, na.ignore = FALSE){ avg &lt;- sum(x, na.rm = na.ignore)/length(x[!is.na(x)]) return(avg) } average(b, na.ignore = TRUE) ## [1] 4.75 mean(b, na.rm = TRUE) ## [1] 4.75 Walla! You did it. You created a function. Notice that we set the default value of na.ignore to FALSE. If we had set it as TRUE then we would not need to specify average(na.ignore = TRUE) since TRUE would have been the default. When using functions it is important to know what the default values are Both for loops and functions allow you to write more concise and readable code. If you are copying and pasting the same lines of code with only small modification, you can probably write those lines of code in a for loop or a function. Now let’s learn about the basics of working with data in R "],
["working-with-data-overview.html", "Working with Data: Overview", " Working with Data: Overview In this section you will learn how to work with data in R by using a collection of packages known as the tidyverse The tidyverse is a collection of R packages that share an underlying design philosophy, grammar, and data structures. Hadley Wickham has been the main contributor to developing the tidyverse. Although you will be learning R in this tutorial, it might be more appropriate to say that you are learning the tidyverse. The tidyverse consists of packages that are simple and intuitive to use and will take you from importing data (with readr), to transforming and manipulating data structures (with dplyr and tidyr), and to data vizualisation (with ggplot2). Learn how to import and output data in the next chapter "],
["importing-and-outputing-data.html", "Chapter 4 Importing and Outputing Data 4.1 CSV 4.2 Tab-Delimited 4.3 SPSS 4.4 RStudio Import GUI 4.5 Merging Data Files", " Chapter 4 Importing and Outputing Data In the Data Preparation stage, every R script that you write will require you to import a data file and output a new data file. In this Chapter you will learn how to import and output comma-separate value (csv), tab-delimited, and SPSS data files. For most of these data types we can use the readr package The readr package contains useful functions for importing and outputing data files. Go ahead and install the readr package. In the console type: install.packages(&quot;readr&quot;) We will also use the foreign and haven packages for SPSS data files install.packages(&quot;foreign&quot;) install.packages(&quot;haven&quot;) You do not really need to save an R script file for this Chapter. We will use some example data files for this chapter. Go ahead and download these files. You will have to unzip the file. For now just unzip it in your downloads folder. Inside the unzipped folder you will see a number of data files in different file formats. Download Example Import Data Files 4.1 CSV csv files are by far the easiest files to import into R and most software programs. For this reason, I suggest any time you want to save/output a data file to your computer, do it in csv format. 4.1.1 Import .csv We can import csv files using read_csv() from the readr package. library(readr) read_csv(&quot;filepath/datafile.csv&quot;) You can see this is very simple. We just need to specify a file path to the data. We will talk more about file paths later but for now we will use absolute file paths. First, figure out the absolute file path to your downloads folder (or wherever the unzipped data folder is located). On Windows the absolute file path will usually start from the C:/ drive. On Macs, it starts from ~/ Import the Flanker_Scores.csv file. You might have something that looks like read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) However, this just printed the output of read_csv() to the console. To actually import this file into R, we need to assign it to an object in our Environment. import_csv &lt;- read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) You can name the object whatever you like. I named it import_csv. To view the dataframe View(import_csv) 4.1.2 Output .csv We can output a csv file using write_csv() from the readr package. write_csv(object, &quot;filepath/filename.csv&quot;) Let’s output the object import_csv to a csv file named: new_Flanker_Scores.csv to the downloads folder write_csv(import_csv, &quot;~/Downloads/new_Flanker_Scores.csv&quot;) Note that whenever writeing (outputing) a file to our computer there is no need to assign the output to an object. 4.2 Tab-Delimited tab-delimited files are a little more tedious to import just because they require specifying more arguments. Which means you have to memorize more to import tab-delimited files. 4.2.1 Import .txt To import a tab-delimited file we can use read_delim() from the readr package. read_delim(&quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) There are three additional arguments we have to specify: delim, escape_double, and trim_ws. The notation for tab-delimted files is &quot;\\t&quot;. Let’s import the Flanker_raw.txt file import_tab &lt;- read_delim(&quot;~/Downloads/Flanker_raw.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) View the import_tab object 4.2.2 Output .txt We can output a tab-delimited file using write_delim() from the readr package. write_delim(object, path = &quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;) Output the import_tab object to a file named: new_Flanker_raw.txt write_delim(import_tab, path = &quot;~/Downloads/Flanker_raw.txt&quot;, delim = &quot;\\t&quot;) 4.3 SPSS As horrible as it might sound, there might be occassions where we need to import an SPSS data file. And worse, we might need to output an SPSS data file! I will suggest to use different packages for importing and outputing spss files. 4.3.1 Import .sav To import an SPSS data file we can use read.spss() from the foreign package. library(foreign) read.spss(&quot;filepath/filename.sav&quot;, to.data.frame = TRUE, use.value.labels = TRUE) The use.value.labels argument allows us to import the value labels from an SPSS file. Import and View the sav file CH9 Salary Ex04.sav import_sav &lt;- read.spss(&quot;~/Downloads/CH9 Salary Ex04.sav&quot;) 4.3.2 Output .sav To output an SPSS data file we can use write_sav() from the haven packge. library(haven) write_sav(object, &quot;filepath/filename.sav&quot;) Go ahead and output the import_sav object to a file: new_CH9 Salary Ex04.sav write_sav(import_sav, &quot;~/Downloads/new_CH9 Salary Ex04.sav&quot;) 4.4 RStudio Import GUI The nice thing about R Studio is that there is also a GUI for importing data files. When you are having difficulty importing a file correctly or unsure of the file format you can use the RStudio Import GUI. In the Environment window click on “Import Dataset”. You will see several options available, these options all rely on different packages. Select whatever data type you want to import You will see a data import window open up that looks like this Select “Browse” on the top right and select the data file you want to import. The “Data Preview” window will let you see if it is importing it in the right format. You can change the import options below this. You might want to change the “Name” but you can always do this later in the R Script. Make sure all the settings are correct by assessing the “Data Preview” window. Does the dataframe look as you would expect it to? Finally, copy and paste the code you need in the “Code Preview” box at the bottom right. You might not always need the library(readr) or View(data) lines. Rather than selecting “Import” I suggest just closing out of the window and pasting the code into your R script. csv files have a nice feauture in that RStudio knows that these are file types we might want to import. So instead of navigating through the Import Dataset GUI we can just click on the file in the Files window pane. 4.5 Merging Data Files You might find yourself in a situation where you need to merge multiple files together. There are two types of merge operations that can be performed. In R, a “join” is merging dataframes together that have at least some rows in common (e.g. Same Subject IDs) and have at least one column that is different. The rows that are common serve as the reference for how to “join” the dataframes together. In R, a “bind” is combining datarames together by staking either the rows or columns. It is unlikely that we you will need to do a column bind so we can skip that. A row “bind” takes dataframes that have the same columns but different rows. This will happen if you have separate data files for each subject from the same task. Each subject data file will have their unique rows (subject by trial level data) but they will all have the same columns. The E-Merge software program is performing a row “bind” of each subject .edat file. For E-Prime data we have to go through the E-Merge software program to bind individual subject files. However, you might have individual subject data files not from E-Prime that you need to merge. Or you may want to merge data files from multiple tasks into one big merged file. My datawrangling package contains two functions to merge data files together: files_join() files_bind() They both work in a similar way. The files you want to merge need to be in the same folder on your computer. You specify the location of this folder using the path = argument. You need to specify a pattern that uniquely identifies the files you want to merge (e.g. “.txt”, or “Flanker”) using the pattern = argument. Then specify the directory and filename you want to save the merge file to using the output.file = argument. Here are the arguments that can be specified: path: Folder location of files to be merged pattern: Pattern to identify files to be merged delim: Delimiter used in files. output.delim: Delimiter to be used in output file. Default is , (csv) na: How are missing values defined in files to be merged. Default is NA output.file: File name and path to be saved to. id: Subject ID column name. ONLY for files_join() For example: library(datawrangling) files_bind(&quot;filepath/data/subj&quot;, pattern = &quot;Flanker&quot;, delim = &quot;\\t&quot;, output.file = &quot;filepath/data/filename_merged.csv&quot;) This will bind any files in the directory filepath/data/subj that contain the string &quot;Flanker&quot; and output the mergeed data to a file called filename_merged.csv to the directory filepath/data. Now that you know how to import data, learn how to manipulate and transform that data in R "],
["data-manipulation-using-dplyr.html", "Chapter 5 Data Manipulation using dplyr 5.1 Setup 5.2 Import 5.3 rename() 5.4 filter() 5.5 select() 5.6 mutate() 5.7 group_by() 5.8 summarise() 5.9 spread() 5.10 Pipe Operator %&gt;%", " Chapter 5 Data Manipulation using dplyr In this Chapter you will learn the fundamentals of data manipulation in R. In the Getting Started in R section you learned about the various types of objects in R. The most important object you will be using is the dataframe. Last Chapter you learned how to import data files into R as dataframes. Now you will learn how to do stuff to that dataframe using the dplyr package (which is of course part of the tidyverse) dplyr is one of the most amazing packages in R. It uses a Grammar of Data Manipulation that is intuitive and easy to learn. The language of dplyr will be the underlying framework for how you will think about manipulating a dataframe. dplyr uses intuitive langauge that you are already familiar with. As with any R function, you can think of functions in the dplyr package as verbs - that refer to performing a particular action on a dataframe. The core dplyr functions are: rename() renames columns filter() filters rows based on their values in specified columns select() selects (or removes) columns mutate() creates new columns based on transformation from other columns, edits values within existing columns group_by() splits dataframe into separate groups based on specified columns summarise() aggregates across rows to create a summary statistic (means, standard deviations, etc.) For more information on these functions Visit the dplyr webpage If you have not done so already, install the dplyr package install.packages(&quot;dplyr&quot;) You will also need the tidyr package (a tidyverse package) for this Chapter install.packages(&quot;tidyr&quot;) Save a new R script file as 6_dplyr.R For this Chapter we will use an example data set from the Flanker task. This data set is a tidy raw data file for over 100 subjects on the Flanker task. There is one row per Trial per Subject and there is RT and Accuracy data on each Trial. Each Trial is either congruent or incongruent. What we will want to do is calculate a FlankerEffect for each Subject so that we end up with one score for each Subject. Go ahead and download the example data set and save it wherever you wish. We will talk about how to organize your data and R scripts in section III. Workflow. Download Example Tidyverse Data 5.1 Setup At the top of your script load the three packages you will need for this Chapter ## Setup library(readr) library(dplyr) library(tidyr) Notice how I added a commented line at the top. Adding comments to your scripts is highly advisable, as it will help you understand your scripts when you come back to them after not working on them for a while. You only need to add a single # to create a commented line. You will also notice that it printed out some warning messages. Sometimes different packages have the same function names. So when you load a package it may override or mask functions from other packages that are already loaded. 5.2 Import Import the data file you downloaded. Refer to Chapter 5 for importing data into R. import &lt;- read_csv(&quot;Data Files/tidyverse_example.csv&quot;) It is always a good idea to get to know your dataframe before you start messing with it. What are the column names? What kind of values are stored in each column? How many observations are there? How many Subjects? How many Trials? etc. What are the column names? use colnames() for a quick glance at the column names colnames(import) ## [1] &quot;Subject&quot; &quot;TrialProc&quot; ## [3] &quot;Trial&quot; &quot;Condition&quot; ## [5] &quot;RT&quot; &quot;ACC&quot; ## [7] &quot;Response&quot; &quot;TargetArrowDirection&quot; ## [9] &quot;SessionDate&quot; &quot;SessionTime&quot; To take a quick look at the first few rows of a dataframe use head(). head(import) ## # A tibble: 6 x 10 ## Subject TrialProc Trial Condition RT ACC Response ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 14000 practice 1 incongru… 1086 1 left ## 2 14000 practice 2 incongru… 863 1 left ## 3 14000 practice 3 congruent 488 1 right ## 4 14000 practice 4 incongru… 588 1 right ## 5 14000 practice 5 congruent 581 1 right ## 6 14000 practice 6 incongru… 544 1 right ## # … with 3 more variables: TargetArrowDirection &lt;chr&gt;, ## # SessionDate &lt;chr&gt;, SessionTime &lt;time&gt; This gives you a good idea of what column names you will be working with and what kind of values they contain. To evaluate what are all the unique values in a column you can use unique(). You can also use this in combination with length() to evaluate how many unique values are in a column. unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; unique(import$Trial) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## [17] 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ## [33] 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ## [49] 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 ## [65] 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ## [81] 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 ## [97] 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 ## [113] 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 ## [129] 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 ## [161] 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 ## [177] 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 max(import$Trial) ## [1] 192 length(unique(import$Subject)) ## [1] 410 unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$ACC) ## [1] 1 0 All these functions we just used from colnames() to unique() were to temporarily evaluate our data. They are not required to perform the actual data analysis. Therefore, I usually just type these in the console. A general rule of thumb is that if it is not required to be saved in your Script file then just type it in the console. Okay let’s take a look at how to use the dplyr functions to score this data. 5.3 rename() We do not really need to, but let’s go ahead and rename() a column. How about instead of ACC let’s label it as Accuracy. Pretty simple data &lt;- rename(import, Accuracy = ACC) rename() is really only useful if you are not also using select() or mutate(). In select() you can also rename columns as you select them to keep. This will be illustrated this later Notice that I passed the output of this function to a new object data. I like to keep the object import as the original imported file and any changes will be passed onto a new dataframe, such as data. This makes it easy to go back and see what the original data is. Because if we were to overwrite import then we would have to execute the read_csv() import function again to be able to see the original data file, just a little more tedious. 5.4 filter() filter() is an inclusive filter and requires the use of logical statements. In Chapter 2: Basic R I talked a little bit about logical statements. Here is a list of logical operators in R: In addition to these logical operators, these functions can be used infilter(): is.na() - include if missing !is.na() - include if not missing between() - values that are between a certain range of numbers near() - values that are near a certain value We do not want to include practice trials when calculating the mean on RTs. We will use filter() to remove these rows. First let’s evaluate the values in these columns unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; We can specify our filter() in a couple of different ways data &lt;- filter(data, TrialProc != &quot;practice&quot;, Condition != &quot;neutral&quot;) or data &lt;- filter(import, TrialProc == &quot;real&quot;, Condition == &quot;congruent&quot; | Condition == &quot;incongruent&quot;) Specifying multiple arguments separated by a comma , in filter() is equivalent to an &amp; (and) statement. In the second option, since there are two types of rows on Condition that we want to keep we need to specify Condition == twice, separated by | (or). We want to keep rows where Condition == &quot;congruent&quot; or Condition == &quot;incongruent&quot; Notice that the arguments have been separated on different lines. This is okay to do and makes it easier to read the code. Just make sure the end of the line still has a comma. Go ahead and view data. Did it properly remove practice trials? How about neutral trials? unique(data$TrialProc) ## [1] &quot;real&quot; unique(data$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; Again you should type these in the console NOT in the R Script! There is a lot of consistency of how you specify arguments in the dplyr package. You always first specify the dataframe that the function is being performed on, followed by the arguments for that function. Column names can be called just like regular R objects, that is without putting the column name in &quot; &quot; like you do with strings. If all you know is dplyr, then this might not seem like anything special but it is. Most non-tidyverse functions will require you to put &quot; &quot; around column names. 5.5 select() select() allows you to select which columns to keep and/or remove. Let’s keep Subject, Condition, RT, Trial, and Accuracy and remove TrialProc, TargetArrowDirection, SessionDate, and SessionTime. select() is actually quite versatile - you can remove columns by specifying certain patterns. I will only cover a couple here, but to learn more Visit the select() webpage We could just simply select all the columns we want to keep data &lt;- select(data, Subject, Condition, RT, Trial, Accuracy) alternatively we can specify which columns we want to remove by placing a - in front of the columns data &lt;- select(data, -TrialProc, -TargetArrowDirection, -SessionDate, -SessionTime) or we can remove (or keep) columns based on a pattern. For instance SessionDate and SessionTime both start with Session data &lt;- select(data, -TrialProc, -TargetArrowDirection, -starts_with(&quot;Session&quot;)) You might start realizing that there is always more than one way to perform the same operation. It is good to be aware of all the ways you can use a function because there might be certain scenarios where it is better or even required to use one method over another. In this example, you only need to know the most straightfoward method of simply selecting which columns to keep. You can also rename variables as you select() them… let’s change Accuracy back to ACC… just beacuse we are crazy! data &lt;- select(data, Subject, Condition, RT, Trial, ACC = Accuracy) We are keeping Subject, Condition, RT, Trial, and renaming ACC to Accuracy. 5.6 mutate() mutate() is a very powerful function. It basically allows you to do any computation or transformation on the values in the dataframe. You can change the values in already existing columns create new columns based on transformation of other columns 5.6.1 Changing values in an existing column Reaction times that are less than 200 milliseconds most likely do not reflect actual processing of the task. Therefore, it would be a good idea to not include these when calculating means. What we are going to do is is set any RTs that are less than 200 milliseconds to missing, NA. First let’s make sure we even have trials that are less than 200 milliseconds. Two ways to do this. 1) View the dataframe and click on the RT column to sort by RT. You can see there are RTs that are as small as 1 millisecond! Oh my, that is definitely not a real reaction time. 2) you can just evaluate the minimum value in the RT column: min(data$RT) ## [1] 0 Now lets mutate() data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT)) Since we are replacing values in an already existing column we can just specify that column name, RT = followed by the transformation. Here we need to specify an if…then… else statment. To do so within the mutate() function we use the function called ifelse(). ifelse() evaluates a logical statement specified in the first argument, RT &lt; 200. mutate() works on a row-by-row basis. So for each row it will evaluate whether RT is less than 200. If this logical statement is TRUE then it will perform the next agrument, in this case sets RT = NA. If the logical statement is FALSE then it will perform the last argument, in this case sets RT = RT (leaves the value unchanged). 5.6.2 Creating a new column Let’s say for whatever reason we want to calculate the difference between the RT on a trial minus the overall grand mean RT (for now, accross all subjects and all trials). This is not necessary for what we want in the end but what the heck, let’s be a little crazy. (I just need a good example to illustrate what mutate() can do.) So first we will want to calculate a “grand” mean RT. We can use the mean() function to calculate a mean. mean(data$RT, na.rm = TRUE) ## [1] 529.1414 Since we replaced some of the RT values with NA we need to make sure we specify in the mean() function to remove NAs by setting na.rm = TRUE. We can use the mean() function inside of a mutate() function. Let’s put this “grand” mean in a column labeled grandRT. First take note of how many columns there are in data ncol(data) ## [1] 5 So after calculating the grandRT we should expect there to be one additional column for a total of 6 columns data &lt;- mutate(data, grandRT = mean(RT, na.rm=TRUE)) Cool! Now let’s calculate another column that is the difference between RT and grandRT. data &lt;- mutate(data, RTdiff = RT - grandRT) We can put all these mutate()s into one mutate() data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) Notice how I put each one on a seperate line. This is just for ease of reading and so the line doesn’t extend too far off the page. Just make sure the commas are still there at the end of each line. 5.7 group_by() This function is very handy if we want to perform functions seperately on different groups or splits of the dataframe. For instance, maybe instead of calculating an overall “grand” mean we want to calculate a “grand” mean for each Subject seperately. Instead of manually breaking the dataframe up by Subject, the group_by() function does this automatically in the background. Like this… data &lt;- group_by(data, Subject) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) You will now notice that each subject has a different grandRT, simply because we specified group_by(data, Subject). Let’s say we want to do it not just grouped by Subject, but also Condition. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) group_by() does not only work on mutate() - it will work on any other functions you specify after group_by(). I suggest exercising caution when using group_by() because the grouping will be maintained until you specify a different group_by() or until you ungroup it using ungroup(). So I always like to ungroup() immediately after I am done with it. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) data &lt;- ungroup(data) 5.8 summarise() The summarise() function will reduce a data frame by summarising values in one or multiple columns. The values will be summarised on some statistical value, such as a mean, median, or standard deviation. Remember that in order to calculate the FlankerEffect for each subject, we first need to calculate each subject’s mean RT on incongruent trials and their mean RT on congruent trials We’ve done our filtering, selecting, mutating, now let’s aggergate RTs accross Condition to calculate mean RT. We will use a combo of group_by() and summarise(). summarise() is almost always used in conjunction with group_by(). data &lt;- group_by(data, Subject, Condition) data &lt;- summarise(data, RT.mean = mean(RT, na.rm = TRUE)) data &lt;- ungroup(data) To summarise() you need to create new column names that will contain the aggregate values. RT.mean seems to make sense to me. What does the resulting data frame look like? There should be three rows per subject, one for incongruent trials, one for congruent trials, and one for neutral trials. You can see that we now have mean RTs on all conditions for each subject. Also, notice how non-group_by columns got removed: Trial, and ACC. 5.9 spread() Our data frame now looks like head(data) ## # A tibble: 6 x 3 ## Subject Condition RT.mean ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 14000 congruent 394. ## 2 14000 incongruent 487. ## 3 14001 congruent 389. ## 4 14001 incongruent 406. ## 5 14002 congruent 452. ## 6 14002 incongruent 508. Ultimately, we want to have one row per subject and to calculate the difference in mean RT between incongruent and congruent conditions. It is easier to calculate the difference between two values when they are in the same row. Currently, the mean RT for each condition is on a different row. What we need to do is reshape the dataframe. To do so we will use the spread() function from the tidyr package. The tidyr package, like readr and dplyr, is from the tidyverse set of packages. The spread() function will convert a long data frame to a wide dataframe. In other words, it will spread values on different rows across different columns. In our example, what we want to do is spread() the mean RT values for the two conditions across different columns. So we will end up with is one row per subject and one column for each condition. Rather than incongruent, and congruent trials being represented down rows we are spreading them across columns (widening the data frame). The two main arguments to specify in spread() are key: The column name that contains the variables to create new columns by (e.g. “Condition”) value: The colunn name that contains the values (e.g. “RT”) data &lt;- spread(data, key = Condition, value = RT.mean) Now our dataframe looks like head(data) ## # A tibble: 6 x 3 ## Subject congruent incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 394. 487. ## 2 14001 389. 406. ## 3 14002 452. 508. ## 4 14003 563. 659. ## 5 14004 520. 643. ## 6 14005 469. 540. From here it is pretty easy, we just need to create a new column that is the difference between incongruent and congruent columns. We can use the mutate() function to do this data &lt;- mutate(data, FlankerEffect = incongruent - congruent) head(data) ## # A tibble: 6 x 4 ## Subject congruent incongruent FlankerEffect ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 394. 487. 93.6 ## 2 14001 389. 406. 17.5 ## 3 14002 452. 508. 56.0 ## 4 14003 563. 659. 95.9 ## 5 14004 520. 643. 123. ## 6 14005 469. 540. 71.4 Perfect! Using the readr, dplyr, and tidyr packages we have gone from a “tidy” raw data file to a dataframe with one row per subject and a column of FlankerEffect scores. 5.10 Pipe Operator %&gt;% One last thing about the dplyr package. dplyr allows for passing the output from one function to another using what is called a pipe operatior. The pipe operator is: %&gt;% This makes code more concise, easier to read, and easier to edit. When you pass the output of one function to another with %&gt;% you do not need to specify the dataframe (input) on the next function. %&gt;% implies that the input is the output from the previous funciton, so this is made implicit. We can pipe all the functions in the chapter together as such ## Setup library(readr) library(dplyr) library(tidyr) ## Import import &lt;- read_csv(&quot;Data Files/tidyverse_example.csv&quot;) ## Score data &lt;- import %&gt;% rename(Accuracy = ACC) %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% select(Subject, Condition, RT, Trial, ACC = Accuracy) %&gt;% group_by(Subject, Condition) %&gt;% mutate(RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE)) %&gt;% ungroup() %&gt;% spread(key = Condition, value = RT.mean) %&gt;% mutate(FlankerEffect = incongruent - congruent) Virtually all the R scripts you write will require the dplyr package. The more you know what it can do, the easier it will be for you to write R Scripts. I highly suggest checking out these introductions to dplyr. https://dplyr.tidyverse.org https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html Learn more advanced but common data manipulations in the next chapter "],
["common-data-manipulations.html", "Chapter 6 Common Data Manipulations 6.1 Descriptive Statistics 6.2 Centering and Standardizing Variables 6.3 Trimming 6.4 Composites 6.5 Scale Transformations 6.6 Custom Transformations", " Chapter 6 Common Data Manipulations In R, the term data wrangling is often times used to refer to performing data manipulation and transformations. The functions you will learn about in this Chapter come from the datawrangling package I developed. There are certain data transformations we use on a regular basis that would require several steps and lines of code to do. datawrangling allows you to perform these transformation in a single line of code. Hopefully, datawrangling will get you to start using R more easily for data analysi. I am hosting the datawrangling package on GitHub. To download packages on GitHub you first need to download the devtools package. install.packages(&quot;devtools&quot;) Now install the datawrangling package: devtools::install_github(&quot;dr-JT/datawrangling&quot;) Save a new R script file as 7_transform.R For this Chapter, let’s create a dataframe to use as an example for common data manipulations using datawrangling. Don’t worry about what this code means for now, just copy it into your script and run it. import &lt;- data.frame(ID = c(1:100), Score1 = rnorm(100, mean = 2, sd = .8), Score2 = rnorm(100, mean = 7, sd = 1.1), Score3 = rnorm(100, mean = 10, sd = 1.8), Score4 = rnorm(100, mean = 20, sd = 2.3)) head(import) ## ID Score1 Score2 Score3 Score4 ## 1 1 3.161073 6.857677 11.105983 21.57860 ## 2 2 2.396887 7.074663 8.529924 21.99176 ## 3 3 3.176875 6.626913 12.061554 23.80460 ## 4 4 1.661930 7.735108 10.054632 19.24501 ## 5 5 2.285693 5.843045 11.974262 20.97016 ## 6 6 1.322965 6.669155 8.927854 21.83858 6.1 Descriptive Statistics First you should know how to compute some basic descriptive statistics. Basic descriptive statistics include mean, median, standard deviation, max, min, skew, kurtosis, etc… The functions to calculate these are pretty straightforward: Base R maximum: max() minimum: min() count:n() mean: mean() median: median() standard deviation: sd() variance: var() quantiles (percentiles): quantile() specify the percentiles with the argument probs = (default is c(0, .25, .5, .75, 1)) e1071 package skewness: skewness(variable, na.rm = TRUE, type = 2) kurtosis: kurtosis(variable, na.rm = TRUE, type = 2) For all of these you need to specify na.rm = TRUE if the variable column has missing data. It is best to just always set na.rm = TRUE. For example, mean(variable, na.rm = TRUE) To calculate the overall mean on Score1 would look like library(dplyr) data &lt;- import %&gt;% mutate(Score1.mean = mean(Score1, na.rm = TRUE)) 6.2 Centering and Standardizing Variables The function center() will create either unstandardized or standardized (z-scored) centered variables. The list of arguments that can be passed onto the function are: x: dataframe variables: c() of columns to center standardize: Logical. Do you want to calculate zscores? (Default = FALSE) Example: library(datawrangling) data &lt;- center(import, variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), standardize = TRUE) View the dataframe data. You will notice that there are now 4 additional columns: Score1_z, Score2_z, Score3_z, and Score4_z. If you choose to to calculate centered (unstandardized) scores, then standardize = FALSE. And it will create variables with the suffix _c. 6.3 Trimming The function trim() will replace outlier scores that exceed a certain z-score cutoff. There are several options for how to replace the outlier scores. Replace with “NA” (missing value) “cutoff” (the z-score cutoff value, e.g. 3.5 SDs) “mean” “median” The arguments that can be specified are: x: dataframe variables: c() of variables to be trimmed. option to set variables = &quot;all&quot; to trim all variables in a dataframe. But then must specify id = cutoff: z-score cutoff to use for trimming (default: 3.5) replace: What value should the outlier values be replaced with. (default: replace = “NA”) id: Column name that contains subject IDs. **ONLY needs to be used if variables = &quot;all&quot; Example: data &lt;- import %&gt;% trim(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), cutoff = 3.5, replace = &quot;NA&quot;, id = &quot;ID&quot;) ## Warning in if (variables == &quot;all&quot;) {: the condition has length &gt; 1 and ## only the first element will be used Notice how you don’t even need to center() the variables first. The centering is being done inside of trim(). You can evaluate outliers and replace with different values (replace =) all in one function and one line of code. 6.4 Composites The composite() function allows you to easily create a composite score from multiple variables and also specifiy a certain criteria for how many missing values are allowed. data &lt;- import %&gt;% composite(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;), name = &quot;Score_comp&quot;, type = &quot;mean&quot;, standardize = TRUE, missing.allowed = 1) The function composite() will create composite scores out of specified columns. Right now you can only create “mean” composite scores. In the future I plan on adding “sum” and “factor score” composite types. Here is a list of the arguments you can specifiy: x: dataframe variables: c() of columns to create the composite from name: Name of the new composite variable to be created type: What type of composite should be calculated?, i.e. mean or sum. (Default = “mean”). standardize: Logical. Do you want to calculate the composite based on standardized (z-score) values? (Default = TRUE) missing.allowed: Criteria for the number of variables that can having missing values and still calculate a composite for that subject The remaining functions do not come from the datawrangling package but you may find them useful nonetheless. 6.5 Scale Transformations 6.5.1 log [insert base off of Field] 6.5.2 polynomial You can create orthogonal polynomials of variables using the poly() function and specify the degree of polynomial to go up to with degree = poly(import$Score1, degree = 3) You can see it creates up to three degrees of polynomials on the Score1 variable. The first degree is a linear, second is a quadratic, and third is cubic. Let’s say we want to create three new columns with each of these three polynomials. To do so we need to individually access each vector such as poly(import$Score1, degree = 3)[,1] library(dplyr) data &lt;- import %&gt;% mutate(Score1.linear = poly(Score1, degree = 3)[ , 1], Score1.quadratic = poly(Score1, degree = 3)[ , 2], Score1.cubic = poly(Score1, degree = 3)[ , 3]) Here is plot to show you visually what happened 6.6 Custom Transformations In general, with mutate() you can specify any custom transformation you want to do on a variable. For instance, if you want to subtract each score by 5, and divide by 10 then you can do it! I don’t know why you would ever want to do that, but you can. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 - 5) / 10) Or take the sum of Score1 and Score2 and divide by the difference between Score3 and Score4. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 + Score2) / (Score3 - Score4)) You have learned some fundamentals to working with data in R. Now on to understanding organizational and reproducibility principles "],
["reproducible-workflows-overview.html", "Reproducible Workflows: Overview", " Reproducible Workflows: Overview As you start becoming more proficient in R you will be able manage, process, and analyze your data at all stages of analysis through R scripts alone. Goodbye Excel, goodbye SPSS, and good ridance to EQS! Although we deal with data all the time as scientists, we have never really been educated on what good data science practices look like. In psychology, I believe this is partly due to our reliance on programs like SPSS or Excel for data analysis and visualization. While these programs offer a nice user interface, they do not offer any tools to help us manage and process our data in a reproducible way. Open Science and Reproducibility practices are very quickly becoming the norm in most fields of science. Programming languages like R are perfectly suited to help us implement these practices. Reliance on programs like SPSS and Excel will hinder your ability to join the Open Science and Reproducibility movement. However, simply learning the R syntax alone is not enough. You will need to start thinking about what are good data science practices that allow me to manage, process, and analyze my data in a way that is consistent with Open Science and Reproducibility? I think it is useful to start thinking and implementing these practices from the get go. You could go through a lot of trial and error (which is what I have done) but why not just build on what others have learned. This section is divided into 3 Chapters: Project Organization I will start at a higher level and consider how we organize our research projects from the data collection phase to the data analysis phase Reproducible Practices I will then talk about how we need to think about interacting with the R Environment, R Scripts, and the data files on our computer in order to not undermine reproducibility. R Script Templates The script workflow is more or less the same from one task and research project to another. I have developed generic script templates that you should use. This will help you more quickly start using R for your research projects. The purpose of this section is to get you thinking about these topics now and introduce you to ways of automating them so that you do not have to think about them later! Let’s talk about Project Organization "],
["project-organization.html", "Chapter 7 Project Organization 7.1 Data Collection 7.2 Data Preparation 7.3 Data Analysis 7.4 workflow package", " Chapter 7 Project Organization In the EngleLab, we often conduct large-scale data collection studies in which there are many different research projects going on at once. Many of the tasks will be shared between these research projects. This means each separate project is using a lot of the same data. This can make it difficult to figure out how and at what stage to separate data processing and analysis between these studies. I will suggest a particular organizational method to deal with this issue. This method will also make it easier in the future to go back and work with unanalyzed or archival data. The basic idea is that you have a central repository for a study. That central repository will store: Raw data files R Scripts To convert messy raw data files to tidy raw data files The programmed tasks administered to subjects Any documents related to the study Methods document, Consent form, demographic forms, etc. Essentially the initial repository used for data collection will become the central repository. From this central repository you can then create separate repositories for different data analysis projects. To do so, and this is the key part, you only need to copy and paste the tidy raw data files. The tidy raw data files are created during the data preparation stage. An advantage to this is that for each data analysis project, you will be able to FULLY reproduce your data processing and analyses independently of other data analysis projects. A key part of this is that you copy ONLY the tidy raw data files from the central repository to a data analysis project. You do not copy data files from one data analysis project to another! You CAN copy R Scripts from one data analysis project to another. For instance, if you or someone else already created an R Script (for another data analysis project) to score a particular task that you are also using, then there is no problem in copying the R Script. In general, you will probably copy R scripts from one project to another. The important point is that you are not copying data files from one project to another! This is because you can reproduce the data files from your R Scripts and not vice versa. If you copy data files you lose reproducibility and the transparency of how the data file was created. 7.1 Data Collection Unfortunately, you have to actually collect data before you can start analyzing anything. Therefore, you start out with a single repository: Data Collection. It may be organized something like this The Tasks folder is where the E-Prime task files are that will be used to administer each task to the subjects. This folder is synced (through Dropbox) to the running room computers. In our lab we typically have multiple Sessions and multiple Tasks in each session. As you begin data collection, .edat data files will start to accumulate in each Task folder. Documents is where you may store various documents related to the study, such as a Methods.docx document describing each task in detail. This is an important document for archival purposes. Some of your other documents in this directory may not be as important for archival purposes, such as an informed consent form. 7.2 Data Preparation At some point you will need to start analyzing the data. However, you first need to prepare the data so that it is ready to analyze. There are several steps in this processs and it can be quite tedious. Nevertheless, undergraduate RAs are trained on how to do most of these steps, so recruit their help. There are also step-by-step instructions for Data Preparation. Once you are ready for Data Collection you will need to create a Data Files and R Scripts folders. There are two scenarios in which you may need to start processing and analyzing data: Before data collection has finished After data collection has finished For both of these scenarios, there are 4 data preparation steps: Organize the raw data files. This involves moving the raw .edat files from Tasks/Session #/#. Task/data to Data Files/subj/Task/. Merge the individual .edat files into a single task.emrg file using E-Merge Export the task.emrg file to a task.txt file so we can process the data in R Source the 0_task_raw.R Scripts 0_task_raw.R imports a task.txt file and creates a tidy raw data file, task_raw.csv See Data Preparation Instructions for a detailed guide on how to perform each of these steps. I have automated step 1 using a simple line of R code (so you will definitely want to check that out). After performing the 4 data preparation steps you will have tidy raw data files for each task saved as task_raw.csv in the Data Files folder. It is these files that you can then copy over to a Data Analysis repository. The Data Collection repository will serve as the Central Repository that you use to create separate Data Analysis repositories from. 7.3 Data Analysis Okay, now say you are ready to analyze some data! It is tempting to do your analysis in the original Data Collection repository where the data are already stored. I highly suggest not doing this. You will be mixing up a Central Repository with a Data Analysis repository. This is particularly important when we conduct large-scale studies with many data analysis projects for a single data collection. Instead, you should copy over the task_raw.csv data files to a separate Data Analysis repository. You also might as well create an Archival Backup of the Central Repository on some other hard drive. That way you are at less risk of a hard drive crashing and losing all your precious data. In the Data Analysis repository you have three main directories: Data Files R Scripts Results Your final repository will look something like: You start with only the task_raw.csv files located in Data Files/Raw Data, copied over from the Central Repository. The 1_task_score.R scripts imports a task_raw.csv file and performs data cleaning and scoring to create a task_Scores.csv file located in Data Files/Scored Data. The 2_merge.R script merges all the task_Scores.csv files together into one Final_Data.csv located in Data Files. This file is ready for statistical analysis, it will have all the variables you are interested in and univariate outliers removed. The 3_Analysis.Rmd is an R Markdown script document for conducting statistical analyses and data visualization on Final_Data.csv. The output of this script document is Main_Analysis.html located in Results Notice how this organization corresponds to the procedure in the workflow diagram below You may have other directories in your Data Analysis repository: Figures Manuscript Presentations Figures is where any image files, that are used in a manuscripta or presentations, are stored. You may also have a PowerPoint file stored here. Manuscript is where the manuscript and any drafts for this project are stored. Presentations is where any PowerPoint presentation files related to this project can be stored. These other directories are more optional. 7.4 workflow package I will show you how to automatically create Data Collection and Data Analysis repositories using RStudio Projects and my workflow package The most important thing to remember is that the Data Collection repository will serve as the Central Repository and that you need to copy and paste the tidy task_raw.csv data files to a Data Analysis repository. But never copy and paste data files from one Data Analysis repository to another. When should you create a separate Data Analysis repository? Basically, if the set of analyses is going to be it’s own Manuscript then create a new repository. If the set of analyses (whether exploratory or supplemental) is part of a larger set of analyses already in the works for a manuscript then no need to create a separate repository. 7.4.1 Install Install the workflow package devtools::install_github(&quot;dr-JT/workflow&quot;) 7.4.2 Create a New R Project One of the features this package allows is for you to automatically setup the organization of a Data Collection or Data Analysis project. Navigate to __File -&gt; New Project… -&gt; New Directory And browse until you see the option: Research Study Click on that and you will see a dialogue box like this Here are what the different options mean: Directory Name: This will be the name of the folder for the study Create project as subdirecotry of: Select Browse and choose where the folder (Directory Name) should be located. Repository Type: data collection or data analysis. Depending on which one you choose it will create the corresponding directories and files: Notice that if you choose the data collection repository it will download a generic template for converting “messy” raw data files to “tidy” raw data files. And if you choose the data analysis repository it will download generic templates for creating scored data files from “tidy” raw data files and to merge the Scored data files into one final data file. # of Sessions: How many sessions will the study have? This will create folders in the Tasks directory for each session. For instance, if there will be 4 sessions it will create the the folders “Session 1”, “Session 2”, “Session 3”, and “Session 4”. Obviously this is not needed for a data analysis repository. Other Directories: I talked earlier about some other directories you may want to include in a Data Analysis repository. Well you can automatically add them here. Go ahead and play around with creating different types of repositories. Now on to reproducible practices "],
["reproducible-practices.html", "Chapter 8 Reproducible Practices 8.1 What is real? 8.2 Where does your analysis live? 8.3 Environment", " Chapter 8 Reproducible Practices An advantage of using R is learning how to manage and handle your data processing workflow in a way that empowers your ability to analyze and explore your data. If you treat R as just an alternative to SPSS, then it is all too easy to create poorly written scripts and disorganized projects that completely undermine reproducibility. Honestly, I am not sure if it is worth taking the time and effort to learn R as simply an alternative to SPSS. In this chapter I will talk about how we need to think about interacting with the R Environment, R Scripts, and the data files on our computer in order to not undermine reproducibility. Parts of this next section are taken directly from the excellent book on Data Science in R R for Data Science One day you will need to quit R, go do something else and return to your analysis the next day. One day you will be working on multiple analyses and/or projects simultaneously that all use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 8.1 What is real? As a beginner R user, it is tempting to consider whatever is in your Environment as “real” (i.e. data we have imported and objects we have created with &lt;-). However, we should consider our R Scripts and Data Files saved on our computer as real. With your R scripts (and your data files), you can recreate the Environment. It’s much harder to recreate R scripts from your Environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your R history. Think of the Environment as more of a temporary workspace. This workspace will get cleared out every time you Restart or Quit out of R and RStudio. If you treat your Environment as real this can have disasterous consequences and you can lose a lot of productivity and undermine your reproducibility. Your R Scripts and Data Files are real Even if you need to Quit R, come back to an analysis the next day, or want to run your analysis on a different computer than you started with, you should be able to reproduce your analyses. You should be able to Reproduce all your analyses from your saved R scripts and original data files. 8.2 Where does your analysis live? “One day you will be working on multiple analyses and/or projects simultaneously that all use R and you want to keep them separate” There are two levels at which your analysis will live Project level Individual R Script level 8.2.1 Project Level The Project Level refers to where you are storing all your files associated with the project (i.e. R scripts, data files, figures, results) as well as to the organization of your folders and files. I talked about Project Organization in the previous chapter but now I want to talk about why using RStudio Projects is a good idea. If you are working on multiple projects at one time, then it is vital to: Keep your analysis from different projects separated You do not want objects created in your Environment from one project to get mixed up with objects (perhaps with the same object names) in a different project. We have not talked about the concept of Working Diectories yet, but you also need to ensure your working directory is correctly set in order to import and output files. If you are working on multiple projects at one time, not keeping them separated will create issues Reproducing your analysis because the working directory might not be set correctly. The three most important elements to an environment are: The working directory Loaded packages Objects The best way to keep project Environments separated is to work on them in separate R Sessions. You can have multiple Sessions of R open at one time. The three elements of an Environment in one R Session will be different and independent from those in another R Session. RStudio has an excellent way of managing separate projects with a feature called RStudio Projects. Use RStudio Projects to create separate Environments for your projects See below for more details on RStudio Projects. 8.2.2 Script Level Within a Project, you will have multiple R Scripts that are performing a different analysis. It is also important to keep the Environment of an R Script independent of the Environment from other R Scripts, even within the same project. Obviously your R Scripts will be dependent on one another in the sense that one R Script might be creating data files that other R Scripts will later use. For example, you might have an R Script (or multiple scripts) that prepare the data for statistical analysis by creating a scored merged data file. Then another R Script will actually run and output the statistical analyses. The statistical analysis R Script is dependent on a data file created by the first R Script. However, the Environment of the R Scripts are independent from one another. R Scripts are linked in a data processing workflow through the data files they create not by the objects in the Environment Therefore, it is important that the Environment of each R Script within a Project are independent from one another. This means that an R Script should not depend on objects created or data imported in other R Scripts. Any packages required for an R Script should be loaded in THAT script and not depend on them being loaded in other R Scripts. As long as the data files required for an R Script are already created, you should be able to open a completely new and fresh session of R and succesfully execute all the lines of code in that one Script (without having ran any other scripts). At the end of each R Script I had you include the line of code rm(list = ls()) This removes all Objects in your Environment. That way when you run the next R Script it is starting from a blank Environment Some people in the R community say this is not a good practice, but in reality it is not harmful to add this line of code. 8.3 Environment Because we should treat our Environment as a temporary workspace we need to make sure that out Environments are as independent as possible from one another, at both the Project and Scripts Levels. I mentioned above that the three most important elements to an environment are: The working directory Loaded packages Objects 8.3.1 Working Directory You could use what are refered to as absolute file paths to import and export files but this is not good practice. The reason is that the absolute file path is specific to a particular computer. No one computer is going to have the same absolute file path. An absolute file path starts from the root directory on your computer and may look something like: Mac: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial Windows: C:\\Users\\jasontsukahara\\Dropbox (GaTech)\\My Work\\Coding Projects\\R\\R-Tutorial You do not want to write scripts that can only work on a specific computer! One of the great advantages to programming for data processing is reproducibility. You and your future self (and other researchers) can reproduce your exact same data processing steps. If you use absolute file paths you are undermining the reproducibility of your scripts. It is good practice to use relative file paths instead. Relative file paths start from a working directory. Let’s say you have a working directory set to the following location: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study And you want to import files from a Raw Data directory within Cool Study. The absolute path to raw data files in Cool Study might look like: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study/Data Files/Raw Data Whereas a elative file path from the working directory would be: Data Files/Raw Data You can see that with relative file paths, only the internal organization of the project directory matters. This allows your script to be ran on different computeres, systems, and environments! You working directory can be specified in various ways. By default, if you open up RStudio directly it will set the working directory to some default location such as your User root directory. This is not good. If you open up RStudio by directly opening an R script file then it will set the working directory to the location of that file. This is better but still not ideal. Because the working directory changes depending on how you open RStudio a lot of R Users will set a working directory at the top of their script using setwd() However, this is not good because it requires the use of an absolute filepath. Ideally, we should not even have to think about the working directory since it can change depending on how R is opened. Thankfully there is a solution to make our dreams come true! 8.3.1.1 RStudio Projects and here::here() Visit this page for more details on R Projects. Using RStudio Projects, helps keep your scripts and Environments from one project to the next separate from each other. RStudio Projects allow you to open a fresh session of R that automatically sets the working directory to the location where the R Project is saved. R Projects have the file extension .Rproj. Save .Rproj to your project’s root directory There are a couple of ways you can open an RStudio Project. One way is to just simply open the .Rproj file. This will open a new R Session (and RStudio window). If you already have an RStudio window open you can navigate to the very top-right of the application window and browse different projects you have recently worked on. This is where you can also see which Project you currently have open. The here package in combination with RStudio Projects allows you to not even have to think about working directories. For a passionate ode to the here package see: https://github.com/jennybc/here_here First go ahead and install the here package, install.packages(&quot;here&quot;). Basically, when the here package is loaded, library(here), it will search for an .Rproj file and it will set a starting file path at that location when you use here(). For instance, I have an .Rproj file saved in my “UseR_Guide” folder. When I use here() it will output a file path to that location. library(here) here() ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding Projects/R/Books/UseR_Guide&quot; You can then use here() to set a relative file path here(&quot;Data Files/Raw Data/flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding Projects/R/Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; This is equivalent to here(&quot;Data Files&quot;, &quot;Raw Data&quot;, &quot;flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding Projects/R/Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; I typically like to set the first argument as the relative file path and the second argument as the filename here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding Projects/R/Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; This visually separates the file path and the filename, making your script easier to read. You can use here() directly in import and output functions: import &lt;- read_csv(here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;)) write_csv(data, here(&quot;Data Files/Scored Data&quot;, &quot;flanker_scored.csv&quot;)) And you know that everytime you use here() that the file path will start at where you have your .Rproj file saved. Avoid using setwd() by using RStudio Projects and here::here() 8.3.2 Loaded Packages To make sure that the packages used in one R Script do not depend on the packages loaded in other R Scripts: Load all required packages for that one R Script at the top of the script This allows you to easily evaluate which packages are required for that script. 8.3.3 Objects To make sure that the objects and functions created in one R Script do not depend on the objects and functions created in other R Scripts: Include the following line of code at the bottome of the script rm(list=ls()) This will remove any objects and functions in the current environment. That way when the next R Script is ran, there will be no leftover objects or functions from previous R Scripts. Let’s take a look at some of the R Templates you will be using "],
["r-script-templates.html", "Chapter 9 R Script Templates 9.1 Download Templates 9.2 Data Preparation 9.3 Data Analysis", " Chapter 9 R Script Templates One of the nice features of using R scripts to analyze your data is that you can use a lot of the same code from one step or one project to another. For each of these steps, the script organization from one task, analyis, or project to the next will be more or less the same. This means we can take advantage of using R script templates. Using R Script templates will not just save time but help you stay organized and use reproducibility practices talked about in the previous Section 9.1 Download Templates I have created some templates you can easily download using a single R function workflow::template() 9.1.1 Install workflow package If you have not done so already, install the workflow package devtools::install_github(&quot;dr-JT/workflow&quot;) There are different types of templates you can download using workflow::template(). You can always use ?workflow::template to see help documentation. 9.2 Data Preparation Data Preparation will occur in a Central Repository (Data Collection), separately from Data Analysis. It will have its own .RProj and mastercript.R files. The main step during data preparation is converting messy raw data files to tidy raw data files. In the diagram below, this step corresponds to the 0_task_raw.R script file. The rawscript filenames are prefixed with 0_ because this is a preliminary stage and are suffixed with **_raw.R** because they create a tidy raw data file called task_raw.csv. 9.2.1 messy-to-tidy script If you want to download a template script for the step of converting messy raw data files to tidy raw data files during data preparation you can simply specify: workflow::template(rawscript = TRUE) Let us take a look at what the rawscript template looks like #### Set up #### ## Load packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import.dir &lt;- &quot;Data Files/Merged&quot; output.dir &lt;- &quot;Data Files&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import.file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ################ #### Import #### import &lt;- read_delim(here(import.dir, import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ################ #### Tidy raw data #### data_raw &lt;- import %&gt;% filter() %&gt;% rename() %&gt;% mutate() %&gt;% select() ####################### #### Output #### write_csv(data_raw, here(output.dir, output.file)) ################ rm(list=ls()) There are 4 main blocks of R code: Set up the script Load required packages using library() Set the import/output directories and filenames Doing these steps at the top of the script makes it obvious, without having to read the rest of the script, what packages the script will require and what data file it is importing and outputing. In the diagram above you can see the “messy” raw data files (“task.txt”) are in “Data Files/Merged”. This script will output a “tidy” raw data file to “Data Files” called “task_raw.csv” Import a data file using read_delim() from the readr package Tidy the imported dataframe using dplyr functions, such as filter(), rename(), mutate(), case_when(), and select() Output a “tidy” data file using write_csv() from readr Last, remove all objects from the enivornment with rm(list=ls()). To me this template is beautiful. The only thing you need to change is task &lt;- &quot;taskname&quot; to the name of the task used in the filename Fill in what happens in the Tidy raw data block. The rest can LITERALLY stay the same. How easy! 9.2.2 masterscript: Data Preparation In the EngleLab, we often have up to 40 tasks for a single Data Collection study. When you have a lot of R Scripts it is quite tedious to open, Source, and exit, each R script one at a time. The masterscript allows you to Source each of your scripts with a line of code using source(). The source() function is a way to execute all the lines of code in a script file. Rather than having to manually open each script file and sourcing it from there you can control your entire data processing workflow from the masterscript using source(). The argument echo = TRUE will print the results of the script to the console that way you can still see what the script is doing. This allows you to run each script from the masterscript and control the order in which you run them. For Data Preparation the order does not really matter, but once you get to Data Analysis the order is crucial. The masterscript template for Data Preparation can be downloaded with: workflow::template(masterscript = &quot;data preparation&quot;) This masterscript is simple: ## Data Preparation for StudyName ############################################# #------ 0. &quot;messy&quot; to &quot;tidy&quot; raw data ------# ############################################# library(here) source(here(&quot;R Scripts&quot;, &quot;0_taskname_raw.R&quot;), echo=TRUE) rm(list=ls()) ############################################# 9.3 Data Analysis Data Analysis will occur in it’s own repository separate from the Central Repository (Data Collection) and separate from Data Preparation. The task_raw.csv files created during Data Preparation will be copy and pasted over to Data Files/Raw Data in the Data Analysis repository. Typically there are at least two steps that need to be taken in order to create a final data file that is ready for statistical analysis. Clean and Score the data from each task by aggregating performance over trials removing any outlier trials or problematic/poor performing subjects. Merge the scored data file from each task into one final data file (this step is more specific to correlational studies but may also apply to experimental studies). There may be some additional steps that are required but these are the basic ones. In the diagram above these two steps correspond to the 1_task_score.R and 2_merge.R script files. They are numbered in the order the scripts have to be ran. This also helps organize the files on your computer. The suffix of the scripts **_score.R** and **_merge.R** describes what the script does. 9.3.1 clean and score script If you want to download a template script for the step of cleaning and scoring a raw data file you can type in the console: workflow::template(scorescript = TRUE) Let us take a look at what the scorescript template looks like #### Setup #### ## Load Packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import.dir &lt;- &quot;Data Files/Raw Data&quot; output.dir &lt;- &quot;Data Files/Scored Data&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output.file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ## Set Data Cleaning Params ############### #### Import #### import &lt;- read_csv(here(import.dir, import.file)) ################ #### Data Cleaning and Scoring #### data &lt;- import %&gt;% filter() %&gt;% group_by() %&gt;% summarise() ################################### #### Output #### write_csv(data, here(output.dir, output.file)) ################ rm(list=ls()) Like the rawscript, there are 4 main blocks of R code: Set up the script Load required packages using library() Set the import/output directories and filenames Set Data Cleaning Params In the diagram above you can see the “tidy” raw data files (“task_raw.csv”) are in “Data Files/Raw Data”. This script will output a data file with “Scores” on the task to “Data Files/Scored Data” called “task_Scores.csv” You can also set some optional data cleaning parameters at the top of the script. This will be explained in more detail later. Import a data file using read_csv() from the readr package Clean and Score the imported dataframe using dplyr functions, such as filter(), mutate(), group_by(), and summarise() Output a data file with task Scores using write_csv() from readr Last, remove all objects from the enivornment with rm(list=ls()). The parts you may need to change are: What packages are loaded, library() task &lt;- “taskname” Optional data cleaning paramaters in the “Set Data Cleaning Params” section The “Data Cleaning and Scoring” block 9.3.2 merge script To download a template script to merge several **_Scores.csv** files together: workflow::template(mergescript = TRUE) The template looks like this: #### Set up #### ## Load packages library(here) library(datawrangling) # to use files_join() and trim() library(dplyr) ## Set import/output directories import.dir &lt;- &quot;Data Files/Scored Data&quot; output.dir &lt;- &quot;Data Files&quot; ################ #### Import Files #### import &lt;- files_join(here(import.dir), pattern = &quot;Scores&quot;, id = &quot;Subject&quot;) ###################### #### Select only important variables and trim outlier scores #### data_merge &lt;- import %&gt;% select() %&gt;% trim(variables = &quot;all&quot;, cutoff = 3.5, id = &quot;Subject&quot;) ## Create list of final subjects subj.list &lt;- select(data_merge, Subject) ################################################################# #### Output #### write_csv(data_merge, here(output.dir, &quot;name_of_datafile.csv&quot;)) write_csv(subj.list, here(output.dir, &quot;subjlist_final.csv&quot;)) ################ rm(list=ls()) Again, this template script has 4 main blocks: Set up the script Load required packages using library() Set the import/output directories Notice that in the setup section, the import and output filenames are not specified. The import filenames are not specified because 1) we may need to import quite a lot of **_Scores.csv** files and 2) I created a function to do this without having to specify each individual filename. You could add a section to include an output filename if you want. Import the data files with task scores. You can merge multiple files with the same rows (Subjects) and different columns (variables or task scores) using datawrangling::files_join(). This is be explained in more detail later on. Select and trim only relevant variables. It is likely that the individual task_Scores.csv files will have way more columns of scores than you are interested in. You may also want to trim outlier scores with datawrangling::trim(), which will be explained in more detail later. Also, more optional is to create a dataframe with a list of subjects that have made it through all the processing stages. Output a final data file that is ready for statistical analysis with write_csv() from readr Optional to also create a file with a list of all subjects that have made it through all these stages of processing. Last, remove all objects from the enivornment with rm(list=ls()). 9.3.3 masterscript: Data Analysis The masterscript template for Data Analysis can be downloaded with: workflow::template(masterscript = &quot;data analysis&quot;) ## Data Analysis for StudyName ################################################# #------ 1. &quot;tidy&quot; raw data to Scored data ------# ################################################# library(here) source(here(&quot;R Scripts&quot;, &quot;1_taskname_score.R&quot;), echo=TRUE) rm(list=ls()) ############################################################# #------ 2. Create Final Merged Data File for Analysis ------# ############################################################# library(here) source(here(&quot;R Scripts&quot;, &quot;2_merge.R&quot;), echo=TRUE) rm(list=ls()) ############################### #------ 3. Data Analysis ------# ############################### library(here) library(markdown) render(here(&quot;R Scripts&quot;, &quot;3_MainAnalyses.Rmd&quot;), output_dir = here(&quot;Results&quot;), output_file = &quot;MainAnalyses.html&quot;, params = list(data = here(&quot;Data Files&quot;, &quot;Name_of_datafile.csv&quot;))) rm(list=ls()) You can see that it is organized in the order in which the scripts need to be ran and how the scripts are named. The render(), function seen in the “3. Data Analysis” section, is how to knit an RMarkdown document. We will see later that this creates a flexible way to knit RMarkdown documents because you can specify the output filename and location, as well as certain parameters for what data set to import or analysis parameters to set. render() comes from the package rmarkdown which is why it is loaded at the top of the data analysis section. Finally, let’s get start working with an example data set "],
["data-preparation-overview.html", "Data Preparation: Overview 9.4 Data Collection Repository", " Data Preparation: Overview In this section we will go over creating R scripts for the steps of Data Preparation using an example data set. This part is not always fun and it can be very tempting to skip and go straight to creating a scored data file that is ready for data analysis. However, I strongly advise against that. There are at least a few good reasons why: Sometimes you actually NEED the raw trial level data. For instance, to do reliability or internal consistency analyses. Visualizing and analyzing trial-level data can help you better understand your data. You or some other researcher might want to go back and run analyses starting from the trial-level data. Maybe you/they want to score the data slightly differently than you did before. Data storage. Storing and sharing your data in a tidy raw data format makes SO MUCH more sense than storing your messy raw data. If working with your messy raw data gives you a headache now imagine how much worse that is when you have not thought about that data for a long time. And so many other reasons There are 4 data preparation steps: Organize the raw data files. This involves moving the raw .edat files from Tasks/Session #/#. Task/ to Data Files/edat/Task/. Merge the individual .edat files into a single task.emrg file using E-Merge Export the task.emrg file to a task.txt file so we can process the data in R Source the 0_task_raw.R Scripts 0_task_raw.R imports a task.txt file and creates a tidy raw data file, task_raw.csv We will cover step 1 and step 4. See Data Preparation Instructions for a detailed guide on how to perform the other steps. 9.4 Data Collection Repository Create a new Data Collection Repository for this section of the tutorial You will first have to install the workflow package. If you have not done so you can install it: devtools::install_github(&quot;dr-JT/workflow&quot;). Then: File –&gt; New Project… –&gt; New Directory –&gt; Research Study Directory Name: UseRGuide_DataCollection Create project as subdirectory of: Your UseR_Guide directory Repository Type: data collection # of Sessions: 2 In the folder UseRGuide_DataCollection you should see an organization that looks like this: You will notice that there are already some template scripts so you can quickly start using R for Data Preparation. Whenever working on this Data Preparation section you should open RStudio by opening the UseRGuide_DataCollection.Rproj file. Let’s see the example data set we are working with "],
["example-data-flanker-task.html", "Example Data: Flanker Task", " Example Data: Flanker Task Why the Flanker task? Because we love the Flanker task! Not really, but it is a good task to illustrate most of the functions you will want to use for other tasks. Basically, in the Flanker task the subject responds to the direction of a centrally presented arrow. There are two types of trials, congruent and incongruent. On congruent trials, the arrows surrounding (flanking) the central arrow are in the same direction as the center arrow. In the incongruent condition, the arrows are facing the opposite direction. Reaction time is slower on incongruent trials compared to congruent trials. The difference in Reaction Time is the dependent measure for this task, and we can call this score a FlankerEffect. Go ahead and download the example Flanker data set Download Example Flanker Data Create the following directory in the Data Collection repository: Tasks/Session 1/1. Flanker From the example data set you downloaded, Save the files in the data folder to: Tasks/Session 1/1. Flanker/data Save the files in the Merged folder to: Data Files/Merged Save the file subjlist.csv to: Data Files/subj Now on to some of those more preliminary data preparation steps "],
["organize-raw-files.html", "Chapter 10 Organize Raw Files", " Chapter 10 Organize Raw Files Before starting, make sure the data is saved and organized as stated in the Overview and open RStudio by opening the UseRGuide_DataCollection.Rproj R Project file. Scenario: Data collection has finished for a two session study. We need to prepare and analyze data for a number of tasks, one of which is the Flanker task. We only want to analyze the data for subjects that completed ALL sessions. This first requires some organization of the raw data files. What we want to do is: Move the raw data files from Tasks/Session 1/1. Flanker/data to Data Files/subj/Flanker. Separate the raw files for subjects that did not complete ALL sessions from those that did. The subjlist.csv file contains a list of subjects that completed ALL sessions. These two steps can easily be done with the following line of code. Just type this into the console workflow::copy_raw(from = &quot;Tasks/Session 1&quot;, to = &quot;Data Files/subj&quot;, filetype = &quot;.edat&quot;, subj.file = &quot;Data Files/subjlist_final.csv&quot;, remove = TRUE) Notice how at first there are no files in Data Files/subj/Flanker. Then after executing this line of code, it is now populated with .edat files. This function will perform the two steps for every task in the Session 1 folder. To perform them for any other sessions just change it from Session 1 to any Session #. You will also see a folder called Did not finish all sessions. The .edat files for subjects that did not finish all sessions will be in here. If you are doing this step before Data Collection has finished, and you do not want to delete the files from the Tasks directory or separate out subjects then leave out the remove = TRUE argument (or set it to FALSE) and leave out the subj.list = argument. See 1. Organize Raw Files in the Data Preparation Guide for more details on this step. The next chapter is more optional but you will probably find it useful at some point "],
["merge-individual-subject-files.html", "Chapter 11 Merge Individual Subject Files 11.1 files_bind()", " Chapter 11 Merge Individual Subject Files This Chapter is not Required If you have not done so already, open RStudio by opening the UseRGuide_DataCollection.Rproj R Project file. For the most part, you will merge individual subject files through the E-Merge program because the individual files are in the proprietary .edat2 file type that cannot be opened outside of E-Prime programs. See Data Preparation Instructions for a detailed guide on how to use E-Merge. However, there may be cases where you have individual subject files not in .edat2 but in .txt, .csv, or .xlsx file types. These can easily be merged into one data file using R. I have created a function to easily merge individual subject files together. The files_bind() function comes from my datawrangling To install datawrangling: devtools::install_github(&quot;dr-JT/datawrangling&quot;) 11.1 files_bind() See Chapter 5: Importing and Outputing Data for more detailed information on file_bind(). The arguments you need to specify in files_bind() are: path: The file path where the data are located delim: The delimiter or filetype of the data files. (i.e. `&quot; for tab-delimited) output.file: The filepath and filename to save the merged data file as By default the merged file will be saved as a .csv file. You can change this by setting output.delim. Using this function may look something like library(here) library(datawrangling) files_bind(here(&quot;Data Files/subj/Flanker&quot;), delim = &quot;\\t&quot;, output.file = here(&quot;Data Files/Merged&quot;, &quot;Flanker.csv&quot;)) The first argument here(&quot;Data Files/subj/Flanker&quot;) is just specifiying the file directory path. Go ahead and type this in your console window to see how this is the case. here(&quot;Data Files/subj/Flanker&quot;) If you have more tasks in which you need to merge individual subject data files you would then just include a similar line of code specific for that task (different file path and different filename). You may want to use this function in one of two ways: Execute it in the console window (do not save it to a script file) every time you need to merge files. Include it where you would normally import the already merged messy raw data file in the 0_task_raw.R script. #### Import #### import &lt;- files_bind(here(&quot;Data Files/subj/Flanker&quot;), delim = &quot;\\t&quot;, output.file = here(&quot;Data Files/Merged&quot;, &quot;Flanker.csv&quot;)) ################ The next chapter is the whole point of this section on Data Preparation "],
["messy-to-tidy-0-task-raw-r.html", "Chapter 12 Messy to Tidy: 0_task_raw.R 12.1 Setup 12.2 Import 12.3 Tidy raw data 12.4 Output 12.5 Masterscript", " Chapter 12 Messy to Tidy: 0_task_raw.R In this Chapter you will go over an example of how to write an R Script for converting messy raw data files to tidy raw data files. 12.1 Setup In the previous Chapters you should have downloaded the example Flanker data set, setup your directory organization, and created an RStudio Project. If you have not done so already, open RStudio by opening the UseRGuide_DataCollection.Rproj R Project file. In the folder R Scripts/templates open the file 0_taskname_raw.R and save it as 0_flanker_raw.R in the R Scripts folder. If you do not see the template file 0_taskname_raw.R, you can download it. Just type in the following line of code in the console workflow::template(rawscript = TRUE) In the script file take note of the import and output directories. Change task &lt;- &quot;taskname&quot; to task &lt;- &quot;Flanker&quot; Just by looking at the Setup section of the script you should be able to tell that this script will import the file Data Files/Merged/Flanker.txt and output the file Data Files/Flanker_raw.csv. Note: Typically you will have to create the Flanker.txt file by using E-Merge and exporting the .emrg file to a .txt file. I have performed this step for you already. 12.2 Import Now it happens on occasion that the wrong subject number is entered in when an RA is starting up a task. This can result in duplicate Subject numbers in the E-Merge file. Luckily I have created a function to remove the duplicate subjects, and put their information (with session date and time) into a specific file. This file will be created in a new folder called “duplicates”. The function is duplicates_remove() from the datawrangling package on my GitHub. It can be difficult to remember what arguments you need to include in a function. To see helpful documentation about a function you can type in the console ?duplicates_remove First, to the Setup section add the line of code to load the datawrangling package. library(datawrangling) To the import section add the following line of code with the pipe operator %&gt;% #### Import #### import &lt;- read_delim(here(import.dir, import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) %&gt;% duplicates_remove(taskname = task, output.folder = here(output.dir, &quot;duplicates&quot;)) ################ Execture the lines of code up to and including import. You will notice that there was a folder created called duplicates in the Data Files folder. Open the file contained in this folder, Flanker_duplicates.csv. These are duplicate subject numbers and must be taken care of. For the sake of this guide, however, we will not do anything with them now. Though, they have been removed from the data. View the dataframe by typing in the console View(import) It is a mess, right? Here are some things you need to know about the messy raw data file. These are the columns and what type of values they contain: Subject: Subject number Procedure[Trial]: Procedure type (keep: TrialProc and PracTrialProc) PracTrialList.Sample: Trial number for practice trials TrialList.Sample: Trial number for real trials FlankerType: condition for real and practice trials (Values are: congruent, incongruent, and neutral) PracSlideTarget.RT: Reaction time for practice trials PracSlideTarget.ACC: Accuracy for practice trials PracSlideTarget.RESP: Response for practice trials ({LEFTARROW} = left and {RIGHTARROW} = right) SlideTarget.RT: Reaction time for real trials SlideTarget.ACC: Accuracy for real trials SlideTarget.RESP: Response for real trials ({LEFTARROW} = left and {RIGHTARROW} = right) TargerDirection: direction of the target arrow for practice trials TargetDirection: direction of the target arrow for real trials SessionDate: Date of session SessionTime: Time of session 12.3 Tidy raw data After the import block of R code, the next block is where we actually tidy the messy raw data. This basically involves filtering rows, renaming columns, changing values in columns, and selecting columns. And accordingly you will use these dplyr functions to do so: filter() rename() mutate() select() For a more detailed explanation on how these functions work, see Chapter 5. 12.3.1 Filter Filter only relevant rows. We want to keep only the rows that contain trials from the practice and real trials (rows from the Instruction procedure and End Screen are in the imported data). #### Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) ####################### The column names are contained in single ticks because the names contain the special characters [ ]. There are certain characters R does not like to use as variable names and one of them is square brackets. This line of code is saying only include (filter) rows where Procedure[Trial] is equal to TrialProc or (|) Procedure[Trial] is equal to PracTrialProc. 12.3.2 Rename Columns Now the column specifying real vs practice trials is a little tedious to keep typing out since it requires the single quotes and brackets. To rename columns we can use rename() function in dplyr. #### Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) ####################### This line of code renames the column Procedure[Trial] to TrialProc. 12.3.3 Change Values We should change the values in TrialProc. unique(data_raw$TrialProc) ## [1] &quot;PracTrialProc&quot; &quot;TrialProc&quot; Right now real trials have the value of TrialProc. The same name as the column, not good! And the “practice” trials have the value of PractTrialProc. Let’s change these values simply to real and practice, respectively. We can use a combination of mutate() and case_when() to change these values. #### Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;)) ####################### case_when() is an alternative to an ifelse(). The second line of the case_when() might read something like: when the value in the column TrialProc is equal to “PracTrialProc” set (~) TrialProc = “practice”. Evaluate that this worked unique(data_raw$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; Okay now let’s move on to figuring out what other columns we want to keep and if we need to do any more computations on them. We want to keep the columns that specify the following information Subject number TrialProc (real vs practice) Trial number Condition (congruent vs incongruent) Reaction time Accuracy Response Target arrow direction (left or right) Session Date Session Time This gets a little more tricky here because the information for some of these variables are in one column for practice trials and a different column for real trials. That means we need to merge the information from these two columns into one. For instance the RT data for practice trials is contained in the column PracSlideTarget.RT and for real trials RT data is in SlideTarget.RT. We can again use case_when() here. We can create a new column labeled RT, that when the values in the column TrialProc is equal to “real” then set (~) RT = SlideTarget.RT. And when TrialProc is equal to “practice” then set (~) RT = PracSlideTarget.RT. ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT, TRUE ~ as.numeric(NA))) ##################### It is not required in this scenario, however just as an illustration I added a third argument to case_when(), TRUE ~ as.numeric(NA) Scenarios in which you cannot or do not need to specify all possible rows in a dataframe, you need to add an option to tell case_when() what to do for those rows. In this scenario, “real” and “practice” in the column TrialProc covers all possible rows in the dataframe. However, if this is not true, you need to add another argument TRUE ~ followed by the value or function that those rows should have. To add an extra layer of complexity, if you want those rows to have missing values NA, then you need to specify a typeof missing value. If the other values in the column are numerical then you should specify as.numeric(NA). If the other values in the column are character strings then you should specify as.character(NA). We can do the same thing for trial, accuracy, response, and target arrow direction. Combining them all into one mutate() function ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT, TRUE ~ as.numeric(NA)), Trial = case_when(TrialProc == &quot;real&quot; ~ TrialList.Sample, TrialProc == &quot;practice&quot; ~ PracTrialList.Sample), Accuracy = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.ACC, TrialProc == &quot;practice&quot; ~ PracSlideTarget.ACC), Response = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RESP, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RESP), TargetArrowDirection = case_when(TrialProc == &quot;real&quot; ~ TargetDirection, TrialProc == &quot;practice&quot; ~ TargerDirection)) ##################### You might want to change the values in the Response and CorrectResponse columns to be more clear (left and right). ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT), Trial = case_when(TrialProc == &quot;real&quot; ~ TrialList.Sample, TrialProc == &quot;practice&quot; ~ PracTrialList.Sample), Accuracy = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.ACC, TrialProc == &quot;practice&quot; ~ PracSlideTarget.ACC), TargetArrowDirection = case_when(TrialProc == &quot;real&quot; ~ TargetDirection, TrialProc == &quot;practice&quot; ~ TargerDirection), Response = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RESP, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RESP), Response = case_when(Response == &quot;z&quot; ~ &quot;left&quot;, Response == &quot;{/}&quot; ~ &quot;right&quot;)) ##################### 12.3.4 Select Columns The last thing to do is select only the columns we want to keep. Remember we want to only select columns with the following information Subject number Trial number Condition Reaction time Accuracy Response Correct Response Target arrow direction Session Date Session Time ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT), Trial = case_when(TrialProc == &quot;real&quot; ~ TrialList.Sample, TrialProc == &quot;practice&quot; ~ PracTrialList.Sample), Accuracy = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.ACC, TrialProc == &quot;practice&quot; ~ PracSlideTarget.ACC), TargetArrowDirection = case_when(TrialProc == &quot;real&quot; ~ TargetDirection, TrialProc == &quot;practice&quot; ~ TargerDirection), Response = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RESP, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RESP), Response = case_when(Response == &quot;z&quot; ~ &quot;left&quot;, Response == &quot;{/}&quot; ~ &quot;right&quot;)) %&gt;% select(Subject, TrialProc, Trial, Condition = FlankerType, RT, Accuracy, Response, TargetArrowDirection, SessionDate, SessionTime) ##################### And that is it for the Tidy raw data block! Compare the import and data_raw dataframes. How many columns does each one have? Rows? View each one, which one is easier to understand? 12.4 Output You do not need to change anything in the output block #### Output #### write_csv(data_raw, here(output.dir, output.file)) ################ That’s it! Great! You have written an R script for Data Preparation, converting a messy raw data file to a tidy raw data file. 12.5 Masterscript Finally, you should add a line of code to the masterscript that sources the file 0_flanker_raw.R. source(here(&quot;R Scripts&quot;, &quot;0_flanker_raw.R&quot;), echo = TRUE) The next Chapter goes over how to create tidy raw data files for the complex span tasks "],
["data-preparation-summary.html", "Data Preparation: Summary", " Data Preparation: Summary This Section was all about getting the raw data into a format that is easy to use and understand. This initial data preparation stage tends to get overlooked but it is important and can really make your life easier once you get to the data analysis stage. This step can actually be done before data collection even starts. Once you have tested your task, and it is ready to be used in a study, you should have some data files you can start working with to create the R Script. In reality this may not actually happen. More realistic is that you have collected enough data to start looking at some of the findings and not until this point do you start creating R Scripts for the data preparation stage. Randy will likely request that you do this many times before data collection has actually finished. Before we started using R, this meant we had to manually go through this data preparation stage every time Randy wanted a report on some of the findings from the study. This is obviously tedious and take up a lot of time. The nice thing about R is that once you create an R Script to do something, you have now automated the process for when you need to do it in the future. See the Overview of this Section for more reasons you should not skip this data preparation stage. Generally, there are four steps during the data preparation stage: Organize the raw data files. This involves moving the raw .edat files from Tasks/Session #/#. Task/ to Data Files/edat/Task/. Merge the individual .edat files into a single task.emrg file using E-Merge Export the task.emrg file to a task.txt file so we can process the data in R Source the 0_task_raw.R Scripts The last few Chapters focused on how to perform steps 1 and 4 using R. Steps 2 and 3 will typically have to be performed using E-Merge. However, in cases where you may not be using E-Prime for a task, Chapter 11 covers how to merge individual subject files (steps 2 and 3). Try to utilize RAs to perform these data preparation steps. They will be trained on how to do so. The most important outcome of this Chapter was creating the R Script to tidy the messy raw data file. Later on in the UseR Guide we will need some additional tasks. So why not start working with these tasks right now. Don’t worry I am not going to have you create any more R Scripts for these tasks. Instead you will download the messy raw data and R Scripts to create tidy raw data files. Go ahead and download the data files: Download Messy Raw Data Files These files were E-Merged files exported as .txt files. Move them to the current repository in Data Files/Merged. All of these tasks are standard tasks we have used in the Engle Lab for some time. Therefore, we can download the R Scripts using the englelab::get_script() function. englelab::get_script(type = &quot;raw&quot;, gf = TRUE, wmc = TRUE, antisaccade = TRUE, stroop = TRUE) type = &quot;raw&quot; tells the function to download scripts to create tidy raw data files. When you want to download scripts to create scored data files use type = &quot;score&quot;. You should now see more R Scripts corresponding to the tasks you downloaded. Note: You will actually have to make some changes to a few of the scripts to get them to work. These scripts were created based on newer tasks versions we use while the data is from older task versions. In 0_antisaccade_raw.R, remomve the StartTime =, FinishTime =, and AdminTime = lines from the mutate() function. Remove the AdminTime from the select() function. Change Mask.ACC and Mask.RT to masks.ACC and masks.RT in the select() function. In 0_symspan_raw.R, 0_ospan_raw.R, and 0_rotspan_raw.R add the argument taskVersion = &quot;old&quot; to the raw_symspan(), raw_ospan(), and raw_rotspan() functions. In 0_stroop_raw.R, just replace the entire data_raw &lt;- data_import %&gt;% block with data_raw &lt;- data_import %&gt;% filter(`Procedure[Trial]` == &quot;StoopTrial&quot; | `Procedure[Trial]` == &quot;stroopPRAC2&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = ifelse(TrialProc == &quot;StoopTrial&quot;, &quot;real&quot;, &quot;practice&quot;), Trial = ifelse(TrialProc == &quot;real&quot;, List2.Sample, List5.Sample), Condition = ifelse(TrialProc == &quot;real&quot;, trialType, pracTYPE), Condition = ifelse(Condition == &quot;Cong&quot; | Condition == &quot;Filler&quot;, &quot;congruent&quot;, &quot;incongruent&quot;), RT = ifelse(TrialProc == &quot;real&quot;, stim.RT, PracStim2.RT), Accuracy = ifelse(TrialProc == &quot;real&quot;, stim.ACC, PracStim2.ACC), Response = ifelse(TrialProc == &quot;real&quot;, stim.RESP, PracStim2.RESP), Response = ifelse(Response == 1, &quot;GREEN&quot;, ifelse(Response == 2, &quot;BLUE&quot;, ifelse(Response == 3, &quot;RED&quot;, NA))), Word = ifelse(TrialProc == &quot;real&quot;, word, pracWORD), Hue = ifelse(TrialProc == &quot;real&quot;, hue, pracHUE)) %&gt;% select(Subject, TrialProc, Trial, Condition, RT, Accuracy, Response, Word, Hue, SessionDate, SessionTime) That should be all the changes you need. Add a line in the masterscript to source() each one of these scripts. Source each script. You should now see more data files in Data Files/ corresponding to each of the tasks. Just for demonstration sake, delete (yes really) the _raw.csv files in Data Files/. Now, run each line of code in the masterscript. You should see that the _raw.csv files have been created again. On to Data Analysis! "],
["cleaning-and-scoring-data-overview.html", "Cleaning and Scoring Data: Overview 12.6 Data Analysis Repository 12.7 Copy tidy raw data", " Cleaning and Scoring Data: Overview This is the first step of actual data analysis. When performing statistical analyses we are asking questions about the relationship between two or more variables. In psychology, our variables are either experimental manipulations or measured observations. Variables that are measured observations are typically aggregated over multiple responses (e.g. multiple trials in a task) within an individual. To continue with the Flanker example from previous chapters, we manipulate the congruency of the central target arrow with the flanking distractor arrows. A participant will perform many trials, some of which will be congruent and some of which will be incongruent. If we wanted to assess the effect of congruency on reaction time (RT) we would first need to calculate a mean RT on congruent trials and a mean RT on incongruent trials for each participant separately. This is because any one single trial will contain a lot of error (or noise) and may not accurately reflect that individuals RT. As an attempt to reduce some of that error in a single trial, we aggregate performance across many trials. In this case we aggregate by calculating a mean RT for each congruency condition and for each participant. This results in two aggregated variables per particpant, a mean RT on congruent and a mean RT on incongruent trials. Generally, the more trials the less error in the aggreagated variable. Before you can conduct statistical analyses you need to calculate aggregated scores from the raw trial level data. If appropriate you may also need to do some data cleaning to remove trials and/or particpants that are clearly problematic. Sometimes a participant will not understand the instructions, or does not care enough to correctly perform the task, and this will usually be reflected in extremely poor performance across the task. Or a particpant, for a single trial, may give an anticipatory response that results in an RT value that is shorter than possible (e.g. 100 ms). These participants and trials should be removed from further analysis. This is what is meant by data cleaning. In the previous Section, I covered how to create a tidy raw data file that had one row per trial (including practice trials). In this Section you will use that tidy raw data file to calculate the FlankerEffect for each Subject. The FlankerEffect is the difference of mean RTs on incongruent trials minus congruent trials. You will also do some data cleaning to remove problematic trials and participants. 12.6 Data Analysis Repository In the previous Section you created a Data Collection repository. The Data Collection repository is the directory where data collection occurs and where the tidy raw data files are stored. This repository eventually becomes the Central Repository after data collection has finished. The Central Repository is where both the messy and tidy raw data files are stored. No data analysis steps occur in the Central Repository. Data analysis steps occur in the Data Analysis repositories. In the EngleLab, we often conduct large-scale data collection studies in which there are many different research projects going on at once. Many of the tasks will be shared between these research projects. This means each separate project is using a lot of the same data. This can make it difficult to figure out how and at what stage to separate data processing and analysis between these studies. If someone already created a data file, for their research project, with scored variables on a lot of the tasks that you need, should you copy and paste that data file into your analysis repository?. NO! This will undermine the reproducibility of your research project and analyses. You will also have no way of figuring out how they scored their variables, what data cleaning procedures they used, etc. There is no transparency of how they got from A (the raw data) to B (the scored data file). They simply gave you B. Whereas with the Project Organization method I outlined in Chapter 7, you can copy the tidy raw data files from the Central Repository into your Data Analysis repository. And if someone has already created scripts to score some of the tasks you need, you can also copy those R scripts over to your project. Now you will have A (the raw data) and the R script of how to get from A to B (the scored data file). This will give you full transparency and you can completely reproduce the analyses. To make your life even easier, I have automated the process for you to create these various repositories. This will make sure you (and any of your collegues) have the same directory organization across your different projects. You will first have to install the workflow package. If you have not done so you can install it: devtools::install_github(&quot;dr-JT/workflow&quot;). Then: File –&gt; New Project… –&gt; New Directory –&gt; Research Study Directory Name: UseRGuide_DataAnalysis Create project as subdirectory of: Your R_Tutorial directory Repository Type: data analysis In the folder UseRGuide_DataAnalysis, you should see an organization that looks like this: You will notice that there are already some template scripts so you can quickly start using R for Cleaning and Scoring your data. When working on this Data Preparation section you should open RStudio by opening the UseRGuide_DataAnalysis.Rproj file. 12.7 Copy tidy raw data In the previous Section you created a tidy raw data file in the UseRGuide_DataCollection repository. Copy the tidy raw data file, Flanker_raw.csv, to the UseRGuide_DataAnalysis repository - Data Files/Raw Data/Flanker_raw.csv. Normally you would be copying several tidy raw files, however, for the sake of this tutorial we are only working with a single task right now. If you did not complete the previous section or do not have the Flanker_raw.csv file, then you can download it below. Move the downloaded file to Data Files/Raw Data/Flanker_raw.csv. Download Example Flanker Data Let’s get to know the Example Data Set you will be working with "],
["example-data-flanker.html", "Example Data: Flanker", " Example Data: Flanker If you have not done so already, open RStudio by opening the UseRGuide_DataAnalysis.Rproj file. In the Files Pane navigate to Data Files/Raw Data and click on the Flanker_raw.csv file -&gt; Import Data Set… -&gt; Update (top right) -&gt; Import. Note: This is just a quick way to view a csv file. It is not suggested you import files this way. Let’s get to know this data set. Looks like there is one row per trial per subject. Both practice and real trials are included. There are neutral, congruent, and incongruent trials. Both RT and Accuracy are recorded. All this can easily be seen just by viewing the data file. Sometimes you cannot see all the information you want very easily just by viewing the file. This is where some basic R functions come in handy. To evaluate the different TrialProcs: unique(Flanker_raw$TrialProc) To evaluate how many different conditions there are: unique(Flanker_raw$Condition) How many trials are there? length(unique(Flanker_raw$Trial)) How many subjects? length(unique(Flanker_raw$Subject)) What are all the column names? colnames(Flanker_raw) I often times print colnames() in the console as I type code so that I can easily see the column names I need to refer to. It is a good idea to get familiar with a data set before working on it. By doing this you may even catch errors, typos, or other mistakes in the data. Now let’s create an R Script to score this data "],
["tidy-to-scored-1-task-score-r.html", "Chapter 13 Tidy to Scored: 1_task_score.R 13.1 Setup 13.2 Data Cleaning and Scoring 13.3 Trimming 13.4 Calculate FlankerEffect 13.5 Remove Subjects 13.6 Calculate Binned Scores 13.7 Merge 13.8 Output 13.9 Masterscript", " Chapter 13 Tidy to Scored: 1_task_score.R In this Chapter you will go over an example of how to write an R Script for converting tidy raw data files to scored and cleaned data files. 13.1 Setup You should have created your Data Analysis repository and copied over the example Flanker data set. If not go back to Data Cleaning and Scoring: Overview and Example Data: Flanker. If you have not done so already, open RStudio by opening the UseRGuide_DataAnalysis.Rproj R Project file. In the folder R Scripts/templates open the file 1_taskname_score.R and save it as 1_flanker_score.R in the R Scripts folder. If you do not see the template file 1_taskname_score.R, you can download it. Just type in the following line of code in the console workflow::template(scorescript = TRUE) In the R script file take note of the import and output directories. Change task &lt;- &quot;taskname&quot; to task &lt;- &quot;Flanker&quot; Just by looking at the Setup section of the script you should be able to tell that this script will import the file Data Files/Raw Data/Flanker_raw.csv and output the file Data Files/Scored Data/Flanker_Scores.csv. 13.1.1 Set Data Cleaning Parameters In the example we are about to go through we will implement the following data cleaning procedures when calculating the FlankerEffect. They will be implemented in the following order rt.min: Set RTs less than 200ms to missing (NA) and Accuracy to incorrect (0). rt.trim: Trim RTs. Replace Outlier RTs that are above or below 3.5 SDs of the mean, with values exactly at 3.5 SDs above or below the mean. This is evaluated for each Subject by each condition seprately. acc.criteria: Finally remove subjects that performed less than 3.5 standard deviations below the mean accuracy on any Condition (congruent or incongruent). Let’s add these data cleaning parameters to the Setup block ## Set Data Cleaning Params rt.min &lt;- 200 rt.trim &lt;- 3.5 acc.criteria &lt;- -3.5 ############# You will now be able to use these objects rt.min, rt.trim, and acc.criteria when writing the code to actually do the data cleaning. This is useful because if you want to change these parameters you can do it right here in the Setup block rather than searching through all your code and figure out where you need to change the values. Again, just from the Setup section you can get an idea of what sort of data cleaning criteria we will use. Notice how you do not need to change anything in the Import section. 13.2 Data Cleaning and Scoring 13.2.1 What is the dependent variable? The next block is where we do the actual data cleaning and task scoring. This step is more complicated and often times requires some forethought. But we don’t always have the best forethought so you will likely re-write previous lines of code. One thing you must think about before writing the script for this stage is the statistical analyses you eventually plan on conducting. The type of statistical analyses you plan on conducting will determine the final dataframe you want to end up at in this stage of data analysis. What are the final dependent variables (or task scores) you want to calaculate? In the Flanker task there are several task scores we might want to calculate (in a regression context). FlankerEffect on RT: Mean reaction time difference between incongruent and congruent trials FlankerEffect on Accuracy: Mean accuracy difference between incongruent and congruent trials Flanker Binned Scores: A scoring method to combine accuracy and reaction time (an alternative to difference scores) 13.3 Trimming First we need to get rid of practice trials / keep only real trials. ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) ################################# Then, set RTs less than 200ms to missing (NA) and Accuracy to 0 using mutate() and ifelse() ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) ################################# And, Trim RTs, grouped by Subject and Condition using the trim() function from my datawrangling package. trim() is an easy way to trim values on a variable using a certain z-score cutoff. The main arguments to pass onto trim() are: variables: The column name that contains the values you want to trim cutoff: What z-score cutoff value you want to use replace: How you want to replace the outlier values. Options are, &quot;mean&quot;, &quot;cutoff&quot;, or &quot;NA You can use group_by() with trim() to trim independently for each Condition (congruent, incongruent, neutral). Always ungroup() afterwards. ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = rt.trim, replace = &quot;cutoff&quot;) %&gt;% ungroup() ################################# This will trim any RTs that are 3.5 SDs above or below the mean grouped by Subject and Condition, and then replace the outlier score with 3.5 SDs above or below the mean. We will implement the third data cleaning procedure later. 13.4 Calculate FlankerEffect What we want to do is calculate both the FlankerEffect and FlankerBinned Scores. These are separate scoring procedures. The general approach we will take is to create two separate dataframes for each procedure, based off the data_trim dataframe. Then we will merge the two dataframes back into one. First calculate the FlankerEffect. We want to calcualte the FlankerEffect on RT using only Accurate trials. Since we also want to calculate the FlankerEffect on Accuracy we cannot just use a filter(). Instead we should mutate() the values in RT to be NA when Accuracy is 0. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) Notice how we are now creating a new dataframe called data_flanker. Next step is to calculate mean RT and mean Accuracy separately for congruent, incongruent, and neutral trials. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() Because we used group_by(Subject, Condition), summarise() will calculate the mean RT and mean Accuracy separately for each Subject and each Condition. Always be sure to ungroup() afterwards. View the data frame. In the console type View(data_flanker) Now rather than having one row per trial, group_by() and summarise() has aggregated the data down to Subject x Condition. What we want to do is calculate the difference between incongruent and congruent conditions on RT.mean and Accuracy.mean. However, congruent, incongruent, and neutral conditions are on separate rows. What we need to do is reshape the data so that there is a column for each Condition on RT.mean and Accuracy.mean. Our columns should be congruent_RT.mean incongruent_RT.mean neutral_RT.mean congruent_Accuracy.mean incongruent_Accuracy.mean neutral_Accuracy.mean Typically, to reshape a data frame we would use the gather() and spread() functions from the tidyr package. However, these do not allow reshaping on more than one value column. We have two value columns, RT.mean and Accuracy.mean. Luckily I have created a function that can allow us to do this, reshape_spread() from my datawrangling package. The main arguments you need to specify are: variables: The column name that contains the key variables to spread on values: The column name(s) that hold the values to be used id: Which columns should be preserved (i.e. Subject) So we can add something like this ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), ACC.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;ACC.mean&quot;)) Now View() the data frame. There should now be only ONE row per Subject. We can now just use mutate() to calculate the difference between these columns to get the FlankerEffect. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;)) %&gt;% mutate(FlankerEffect_RT = incongruent_RT.mean - congruent_RT.mean, FlankerEffect_ACC = incongruent_Accuracy.mean - congruent_Accuracy.mean) 13.5 Remove Subjects Next, we should implement the third data cleaning procedure listed above. acc.criteria: Remove subjects that performed less than 3.5 standard deviations below the mean accuracy on any Condition (congruent, incongruent, or neutral). It is convenient to do it now because we have a column with mean Accuracy for congruent and incongruent trials. We also want to do this before applying the binning procedure. The approach I like to take with entirely removing subjects is to keep a record of those subjects in a data file somewhere. To do this we will 1) create a new data frame of subjects that will be removed and then 2) use a function I created, remove_save() from the datawrangling package. This function is a short hand way of doing two things at once. Removing the subjects from the full data file Saving the removed subjects to a specified directory. The criteria we are removing subjects based on are those who performed 3.5 SDs below the mean. So we first need to calculate a column of z-scores (on SD units), then filter those who are below 3.5 z-scores. We can use datawrangling::center() to standardize the variables. ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) Then use remove_save(). The main arguments to specify are: x: the data frame that contains ALL subjects remove: the data frame that contains subjects to be removed output.dir: directory to output file with removed subjects to output.file: name of file with removed subjects I put the removed subjects in a folder called “removed” and named the file “Flanker_removed.csv”. ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) data_flanker &lt;- remove_save(data_flanker, data_remove, output.dir = here(output.dir, &quot;removed&quot;), output.file = paste(task, &quot;removed.csv&quot;, sep = &quot;_&quot;)) If any subjects were removed you should now see a folder called removed in the Scored Data folder with a file called “Flanker_removed.csv”. In the Files window of RStudio, navigate to Data Files/Scored Data/removed and click on Flanker_removed.csv -&gt; Import DataSet… -&gt; Update (top right) -&gt; Import. View the dataframe. Notice how these Subjects had poor performance on at least one of the conditions. These subjects have been removed from further analysis. This file is just a track record of who was removed and why. 13.6 Calculate Binned Scores Great! We have now calculated FlankerEffects scores and performed the data cleaning procedures. Now we need to calculate Binned scores. The data_flanker data frame is no longer in a format that we can calculate bin scores. We need to use the trimmed data frame that has trial level data. We should remove the poor performing subjects and Missing RTs. This step is actually really important for the binning procedure because bin scores are relative to other subjects in the data. ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), !(Subject %in% data_remove$Subject)) This is stating, keep only Trials without missing values on RT AND Subjects that are NOT (!) in (%in%) data_remove. We also need to remove neutral trials to calculate bin scores. Bin scores are based on comparing one condition to a baseline condition. In this case we want to compare the incongruent condition to the baseline congruent condition. So we need to get rid of neutral conditions. ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) And finally calculate bin scores using bin_score() from the englelab package. The main arguments to specify are: x: The data frame rt.col: Column name that contains the reaction time data. Default = “RT” accuracy.col: Column name that contains the accuracy data. Default = “Accuracy” condition.col: Column name that contains the trial condition type. Default = “Condition” baseline.condition: The values that specify the baseline condition type: How should Bin trials be aggregated, “sum” or “mean”. Default = “mean” id: Column name that contains subject identifiers. Default = “Subject” ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) %&gt;% bin_score(baseline.condition = &quot;congruent&quot;, type = &quot;mean&quot;,) %&gt;% rename(FlankerBin = &quot;BinScore&quot;) Awesome! Now we have two data frames, one, data_flanker, with FlankerEffect scores and another, data_binned with FlankerBin scores. They both have one row per subject. 13.7 Merge Now we can merge these two data frames together using the merge() function from base R. ## Merge data_flanker &lt;- merge(data_flanker, data_binned, by = &quot;Subject&quot;, all = TRUE) Now view the data_flanker. It should be one row per subject and have columns for FlanekrEffect_RT, FlankerEffect_ACC, and FlankerBin. 13.8 Output You do not need to change anything in the output block ## Output #### write_csv(data_flanker, here(output.dir, output.file)) ############## Great! You have written an R script for Data Cleaning and Scoring. 13.9 Masterscript Finally, you should add a line of code to the masterscript that sources the file 1_flanker_score.R. source(here(&quot;R Scripts&quot;, &quot;1_flanker_score.R&quot;), echo = TRUE) Woo! You should feel accomplished. Now move on! "],
["merge-create-a-final-data-file.html", "Chapter 14 Merge: Create A Final Data File 14.1 Setup 14.2 Merge 14.3 Select and Trim 14.4 Output 14.5 Masterscript", " Chapter 14 Merge: Create A Final Data File In our lab we run large-scale studies in which we might have more than 30 tasks to score. This means you will have lots of scripts for creating scored data files. In the end, you don’t just want a bunch of separate scored data files. You want a single data file with all the variables and subjects together. In this Chapter you will learn how to write a script for merging multiple scored data files and doing some more data cleaning by removing outlier scores. In the previous Chapter you wrote a score script for the Flanker task. For this Chapter to be useful you will need more tasks with scored data. In the previous Section, I had you create tidy raw data files in the Data Collection repository. If you did not complete the previous section you can download these files here: Copy these additional tidy raw data files to the current Data Analysis repository in Data Files/Raw Data. Since these are common tasks we use in the lab you can download the scripts to score each of these tasks using: englelab::get_script(type = &quot;score&quot;, wmc = TRUE, gf = TRUE, antisaccade = TRUE, stroop = TRUE) Notice how the complex span tasks are all scored in the same script, 0_wmc_score.R, and all the fluid intelligence tasks are scored in the same script, 0_gf_score.R. This procedure of copying tidy raw data files from a Central Repository and R Scripts from another student in the lab will be very common. If you have not done so already, open RStudio by opening the UseRGuide_DataAnalysis.Rproj R Project file. In the masterscript add lines of code to source() the R Scripts you downloaded. Execute those lines of code. You should notice that there are now more files in Data Files/Scored Data. In this Chapter you will learn how to merge these files together. In the folder R Scripts open the file 2_merge.R. If you do not see the file 2_merge.R, you can download it. Just type in the following line of code in the console workflow::template(mergescript = TRUE) 14.1 Setup Just by looking at the Setup section of the script you should be able to tell that this script will import files in Data Files/Scored Data and output the file Data Files/Data.csv. 14.2 Merge In this block we can import/merge all the Scored data files. We will import/merge all these files in one step using files_join() from my datawrangling package. The main argunents for this function are: path: Folder location of files to be merged pattern: String pattern to uniquely identify files to be merged id: Subject ID variable name. ## Merge #### import &lt;- files_join(here(import.dir), pattern = &quot;Scores&quot;, id = &quot;Subject&quot;) ############# View import. Notice that it contains A LOT of columns, most of which we may not be interested in for Data Analysis. 14.3 Select and Trim The next step is really straight forward. The Scored data files will each contain way more columns than we are actually interested in. Therefore, we should only select those columns that contain the variables we want to analyze in the Data Analysis stage. We can also remove Scores that are univariate outliers, using trim() from datawrangling. ## Select and Trim #### data &lt;- import %&gt;% select(Subject, OSpan = OSpan.Partial, SymSpan = SymSpan.Partial, RotSpan = RotSpan.Partial, FlankerEffect = FlankerEffect_RT, StroopEffect = StroopEffect_RT) %&gt;% trim(variables = &quot;all&quot;, cutoff = 3.5, replace = &quot;NA&quot;, id = &quot;Subject&quot;) ######################## Notice how I am renaming some of the variable names to be more concise. 14.4 Output Besides outputing the merged data file, I like to also output a file that simply has a list of all Subjects that made it through all the data cleaning procedures. This will be a list of the final subjects that go into Data Analysis. ## Output ## subj.list &lt;- select(data, Subject) write_csv(data, here(output.dir, output.file)) write_csv(subj.list, here(output.dir, &quot;subjlist_final.csv&quot;)) ############ 14.5 Masterscript Now put a source() line in the masterscript to source this merge script. Run all the lines of code in the masterscript. Cool! Now you have a file that is ready for statistical analysis "],
["cleaning-and-scoring-data-summary.html", "Cleaning and Scoring Data: Summary", " Cleaning and Scoring Data: Summary In this Section, you cleaned and scored data from the Flanker task. There were a lot of steps involved to do so. You calculated the Flanker Effect in addition to FlankerBin scores. You also performed data cleaning procedures to get rid of bad trials and subjects. The Flanker task is one of the more involved tasks to score. Wow! Now you have completed your Data Analysis scripts to go from tidy raw data to scored data and finally a single merged data file that is all ready for Statistical Analysis! And you can control all this by running the code in the masterscript, or just sourcing the entire masterscript. You could either move on to Data Visualization or Statistical Analysis next. "],
["data-visualization-overview.html", "Data Visualization: Overview 14.6 R Markdown", " Data Visualization: Overview 14.6 R Markdown Phew! You’ve made it this far, good job. Up until now you have been learning how to do data preparation steps in R. Now for the fun part, statistical analyses and data visualization! This is the third and final step in the data workflow process depicted above. Traditionally you have likely done these analyses in SPSS or EQS and have created figures in Excel or PowerPoint. The rest of the guide will cover how to do these steps in R. Writing scripts to do statistical analyses is an entirely different process than writing scripts for data preparation. Therefore, we should first go over the general process of conducting and outputing statistical analyses in R. In programs like SPSS when you run a statistical analysis, it will be outputed to a viewable .spv document. One dowfall of this is that .spv files are proprietry format so can only be opened if you have SPSS installed. However, there is the option to export a .spv file as a PDF. One downfall about R is that unlike SPSS, there is not a native way to create output documents from statistical analyses. Fortunately, RStudio has an output document format called R Markdown. 14.6.1 What is an R Markdown File? R Markdown is a powerful way to create reports of statistical analyses. Reports can be outputed in a lot of different formats; html, Microsoft Word, PDF, presentation slides, and more. In fact, this guide was created using R Markdown. The easiest format to output as is html. html documents are opened in a web browser and therefore can be opened on any computer and device (phones, tablets, Windows, Mac). Follow this link for a brief Intro to R Markdown First, you need to install the rmarkdown package install.packages(&quot;rmarkdown&quot;) To open an R Markdown document go to File -&gt; New File -&gt; R Markdown… Select HTML and click OK An example R Markdown document will open. Go ahead and read the contents of the document. There are three types of content in an R Markdown document: A YAML header R code chunks Formatted text 14.6.2 YAML header The YAML header contains metadata about how the document should be rendered and the output format. It is located at the very top of the document and is surrounded by lines of three dashe, --- title: &quot;Title of document&quot; output: html_document --- There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html 14.6.3 R code chunks Unlike a typical R script file (.R), an R Markdown document (.Rmd) is a mixture of formated text and R code chunks. Not everything in an R Markdown document is executed in the R console, only the R code chunks. To run chunks of R code you can click on the green “play” button on the top right of the R code chunk. Go ahead and do this for the three R code chunks in the R Markdown document you opened. (cars and pressure are just example dataframes that come pre-loaded with R). We have not gone over these functions yet, but you can see that the results of the R code are now displayed in the document. The first R code chunk is just setting some default options of how the output of R code chunks should be displayed. We will cover these options in more detail later. 14.6.4 Formatted text The formatted text sections are more than just adding comments to lines of code. You can write up descriptive reports, create bulleted or numbered lists, embed images or web links, create tables, and more. The text is formatted using a language known as Markdown, hence the name R Markdown. Markdown is a convenient and flexible way to format text. When a Markdown document is rendered into some output (such as html or PDF), the text will be formatted as specified by the Markdown syntax. In the R Markdown document you have open you can see some Markdown syntax. The pound signs ## at the beggining of a line are used to format headers. One # is a level one header, two ## is a level two header and so on. Also notice in the second paragraph, the word Knit is surrounded by two asterisks on each side. When this document is redered, thw word Knit will be bolded. Go ahead and render the R Markdown document by clicking on the Knit button at the top of the window. Once it is done rendering you will see a new window pop up. This is the outputed html file. You can see how the document has formated text based on the Markdown syntax. There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. Here is one I reference oftern: Markdown Cheatsheet It will help to first learn some fundamentals of data visualization "],
["fundamentals-of-data-visualization.html", "Chapter 15 Fundamentals of Data Visualization 15.1 Grammar of Graphics 15.2 Plotting Functions in R", " Chapter 15 Fundamentals of Data Visualization Data visualization is an essential skill for anyone working with data. It is a combination of statistical understanding and design principles. In this way, data visualization is about graphical data analysis and communication and perception. Data visualization is often times glossed over in our stats courses. This is unfortunate because it is so important for better understanding our data, for communicating our results to others, and frankly it is too easy to create poorly designed visualizations. As a scientist, there are two purposes for visualizing our data. Data exploration: it is difficult to fully understand our data just by looking at numbers on a screen arranged in rows and columns. Being skilled in data visualization will help you better understand your data. Explain and Communicate: You will also need to explain and communicate your results to collegues or in scientific publications. The same data visualization principles apply to both purposes, however for communicating your results you may want to place more emphasis on aesthetics and readability. For data exploration your visualizations do not have to be pretty. 15.1 Grammar of Graphics Leland Wilkinson (Grammar of Graphics, 1999) formulized two main principles in his plotting framework: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings The essential grammatical elements to create any visualization are: 15.2 Plotting Functions in R It is possible to create plots in R using the base R function plot(). The neat thing about plot() is that it is really good at knowing what kind of plot you want without you having to specify. However, these are not easy to customize and the output is a static image not an R object that can be modified. To allow for data visualization that is more in line with the principles for a grammar of graphics, Hadley Wickham (pictured below) created the ggplot2 package. This by far the most popular package for data visualization in R. Let’s learn you some ggplot2! "],
["introduction-to-ggplot2.html", "Chapter 16 Introduction to ggplot2 16.1 Grammar of Graphics 16.2 Data layer 16.3 Aesthetic Layer 16.4 Geometries Layer 16.5 Facets Layer 16.6 Statistics Layer 16.7 Coordinates Layer 16.8 Themes Layer", " Chapter 16 Introduction to ggplot2 16.1 Grammar of Graphics We saw from the last chapter that the two main components in a grammar of graphics are: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings We also saw that the three essential elements are the data layer, aesthetics layer, and geometrics layer. In ggplot2 there are a total of 7 layers we can add to a plot 16.2 Data layer The Data Layer specifies the data being plotted. Let’s see what this means more concretely with an example data set. A very popular data set used for teaching data science is the iris data set. In this data set various species of iris were measured on their sepal and petal length and width. This data set actually comes preloaded with R, so you can simply view it by typing in your console View(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa We can see that this data is in wide format. What type of graph we can visualize will depend on the format of the data set. On occassion, in order to visualize a certain pattern of the data will require you to change the formating of the data. Let’s go ahead and start building our graphical elements in ggplot2. Load the ggplot2 library. Then: library(ggplot2) ggplot(data = iris) You can see that we only have a blank square. This is becuase we have not added any other layers yet, we have only specified the data layer. 16.3 Aesthetic Layer The next grammatical element is the aesthetic layer, or aes for short. This layer specifies how we want to map our data onto the scales of the plot The aesthetic layer maps variables in our data onto scales in our graphical visualization, such as the x and y coordinates. In ggplot2 the aesthetic layer is specified using the aes() function. Let’s create a plot of the relationship between Sepal.Length and Sepal.Width, putting them on the x and y axis respectively. ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) You can see we went from a blank box to a graph with the variable and scales of Sepal.Length mapped onto the x-axis and Sepal.Width on the y-axis. However, there is no data yet :( What are we to do? 16.4 Geometries Layer The next essential element for data visualization is the geometries layer or geom layer for short. Just to demonstrate to you that ggplot2 is creating R graphic objects that you can modify and not just static images, let’s assign the previous graph with data and aesthetics layers only onto an R object called p, for plot. p &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) Now let’s say we want to add the individual raw data points to create a scatterplot. To do this we can use the function geom_point(). This is a geom layer and the type of geom we want to add are points. In ggplot2 there is a special notation that is similar to the pipe operator %&gt;% seen before. Except it is plus sign + p + geom_point() And walla! Now we have a scatterplot of the relationship between Sepal.Length and Sepal.Width. Cool. If we look at the scatterplot it appears that there are at least two groups or clusters of points. These clusters might represent the different species of flowers, represented in the Species column. There are different ways we can visualize or separate this grouping structure. First, we will consider how to plot these species in separate plots within the same visualization. 16.5 Facets Layer The facet layer allows you to create subplots within the same graphic object The previous three layers are the essential layers. The facet layer is not essential, however given your data you may find it helps you to explore or communicate your data. Let’s create facets of our scatterplot by Species ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) 16.6 Statistics Layer The statistics layer allows you plot statistical values calculated from the data So far we have only plotted the raw data values. However, we may be interested in plotting some statistics or calculated values, such as a regression line, means, standard error bars, etc. Let’s add a regression line to the scatterplot. First without the facet layer then with the facet layer ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) 16.7 Coordinates Layer The coordinate layer allows you to adjust the x and y coordinates You can adjust the min and max values, as well as the major ticks. This is more useful when you have separate graphs (non-faceted) and you want to plot them on the same scale for comparison. This is actually a very important deisgn principle in data visualization. If you want to compare two separate graphs, then they need to be on the same scale!!! library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = seq(4, 8, by = 1), ylim = seq(2, 5, 1)) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = seq(4, 8, by = 1), ylim = seq(2, 5, 1)) ggplot(filter(iris, Species == &quot;virginica&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = seq(4, 8, by = 1), ylim = seq(2, 5, 1)) Just a note. I highly suggest not using scale_x_continuous() or scale_y_continuous() functions. The coord_cartesian() function is like zooming in and out of the plot area. The scale_ functions actually change the shape of the data and statistics layers. If a data point falls outside of the scale limits then it will be removed from any statistical analyses (even if the individual data points are not plotted geom_point()) 16.8 Themes Layer The Themes Layer refers to all non-data ink. You can change the labels of x or y axis, add a plot title, modify a legend title, add text anywhere on the plot, change the background color, axis lines, plot lines, etc. There are three types of elements within the Themes Layer; text, line, and rectangle. Together these three elements can control all the non-data ink in the graph. Underneath these three elements are sub-elements and this can be represented in a hierarchy such as: For instance, you can see that you can control the design of the text for the plot title and legend title theme(title = element_text()) or individually with theme(plot.title = element_text(), legend.title = element_text()). Any text element can be modified with element_text() Any line element can be modified with element_line() Any rect element can be modified with element_rect() You can then control different features such as the color, linetype, size, font family, etc. As an example let’s change some theme elements to our facet plot. Let’s change the axis value labels to red font and increase the size ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text = element_text(color = &quot;red&quot;, size = 14)) Now let’s only change the x-axis text and not the y-axis text. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) It is a good idea to have a consistent theme across all your graphs. And so you might want to just create a theme object that you can add to all your graphs. a_theme &lt;- theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14), panel.grid = element_blank(), panel.background = element_rect(fill = &quot;pink&quot;)) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + a_theme 16.8.1 Built-in Themes For the most part you can probably avoid the theme() function by using built-in themes, unless there is a specific element you want to modify. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + theme_linedraw() You can also set a defeault theme for the rest of your ggplots at the top of your script. That way you do not have to keep on specifying the theme for evey ggplot. theme_set(theme_linedraw()) Now you can create a ggplot with theme_linedraw() without specifying theme_linedraw() every single time. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) You can do a google search to easily find different types of theme templates. I personally like theme_linedraw() Scatterplots are next! "],
["scatterplots.html", "Chapter 17 Scatterplots 17.1 Scatterplots", " Chapter 17 Scatterplots You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) The main type of plots we typically want to create in psychological science are: Scatterplots Bar graphs Line graphs Histograms 17.1 Scatterplots We have already spent a good amount of time creating scatterplots using stat_smooth() and/or geom_smooth(). These two functions are essntially identical. In fact, many of the geom_ functions are just wrappers around stat_ functions. The scatterplot we created from last chapter is essentially an interaction plot. The interaction of Species x Sepal.Length on Sepal.Width. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) For modelling an interaction effect in regression it is easier to interpret if the lines extend to all possible values - not just across the values within a group. We can do this by specifying the argument geom_smooth(fullrange = TRUE) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) Now what if the moderator was a continuous variable and not categorical like Species? We would want to set the color aesthetic to be on +/- 1 SD on the mean. How would we go about doing this? The answer is: It would be very difficult to do so. This is where the function plot_model() from the sjPlot package comes in handy. 17.1.1 Adding other geoms There might be other geoms we want to add to a scatterplot. Let’s add some summary statistics to the graph. Specifically, a horizontal dashed line representing the mean on Sepal.Width and a vertical dashed line representing the mean on Sepal.Length. To make it more simple let’s only do this for Species == &quot;setosa&quot;. library(dplyr) iris_means &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% mutate(Sepal.Width_mean = mean(Sepal.Width, na.rm = TRUE), Sepal.Length_mean = mean(Sepal.Length, na.rm = TRUE)) ggplot(iris_means, aes(Sepal.Length, Sepal.Width)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + geom_hline(aes(yintercept = Sepal.Width_mean), linetype = &quot;dashed&quot;, color = &quot;red4&quot;) + geom_vline(aes(xintercept = Sepal.Length_mean), linetype = &quot;dashed&quot;, color = &quot;green4&quot;) Next is plotting group means "],
["plotting-means.html", "Chapter 18 Plotting Means 18.1 Bar Graphs 18.2 Alternatives to Bar Graphs 18.3 Two-Way Interaction Plots 18.4 Three-Way Interaction Plots", " Chapter 18 Plotting Means You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) 18.1 Bar Graphs Bar graphs are the standard. They are ubiquitous across psychology. Basically everyone uses them. But in all honesty, Bar graphs SUCK!. The worst part about them is that they hide the distribution of the raw data points (even when error bars are included). Even worse, too often you will see bar graphs with NO ERROR BARS! Yikes! A bar graph with no error bars tells you almost NOTHING! To illustrate this let’s use a data set containing information on mammalian sleep patterns from the data set msleep. head(msleep) ## # A tibble: 6 x 11 ## name genus vore order conservation sleep_total sleep_rem ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chee… Acin… carni Carn… lc 12.1 NA ## 2 Owl … Aotus omni Prim… &lt;NA&gt; 17 1.8 ## 3 Moun… Aplo… herbi Rode… nt 14.4 2.4 ## 4 Grea… Blar… omni Sori… lc 14.9 2.3 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 ## 6 Thre… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 ## # … with 4 more variables: sleep_cycle &lt;dbl&gt;, awake &lt;dbl&gt;, ## # brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; Let’s plot the relationship between the different eating habits (vore) and total sleep time (sleep_total). msleep1 &lt;- filter(msleep, !is.na(vore)) ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) This only tells us what the means are. We have no idea about the distributions. Well for this reason people usually like to see error bars. Okay well let’s add error bars. ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2) Okay better. But we still cannot see the underlying distribution. 18.2 Alternatives to Bar Graphs Here is a crazy idea. What if we plotted the raw data points. Like we do with scatterplots! Whoa! What a concept ggplot(msleep1, aes(vore, sleep_total)) + geom_point() When plotting raw data points with categorical variables on the x-axis it makes more sense to jitter the points so they are not all just lying on top of each other. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) Wow! Does this give you a completely different picture than the bar graph with error bars? It does to me! Especially look at the insecti and omni eating habits. There is definitely a bi-modal distribution happening there. From the bar graph with error bars, we might be fooled into thinking that the distributions for carni and omnie are pretty similar. But are they? Not at all! THIS IS WHY YOU SHOULD ALWAYS PLOT THE RAW DATA POINTS But means and error bars are also useful information so let’s add those ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) Another aesthetic option that is useful when we are plotting means and error bars ontop of raw data is the alpha aesthetic. This can allow us to make the raw data points more transparent, fade into the background a little more. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) + stat_summary(fun.y = mean, na.rm = TRUE, aes(group = 1), geom = &quot;line&quot;, color = &quot;dodgerblue&quot;, size = .75, shape = &quot;diamond&quot;) 18.3 Two-Way Interaction Plots library(tidyr) iris.long &lt;- iris %&gt;% mutate(Flower = row_number()) %&gt;% gather(&quot;Part&quot;, &quot;Inches&quot;, -Flower, -Species) %&gt;% separate(Part, into = c(&quot;Part&quot;, &quot;Measurement&quot;)) %&gt;% arrange(Flower, Species) %&gt;% select(Flower, Species, Part, Measurement, Inches) head(iris.long) ## Flower Species Part Measurement Inches ## 1 1 setosa Sepal Length 5.1 ## 2 1 setosa Sepal Width 3.5 ## 3 1 setosa Petal Length 1.4 ## 4 1 setosa Petal Width 0.2 ## 5 2 setosa Sepal Length 4.9 ## 6 2 setosa Sepal Width 3.0 ggplot(iris.long, aes(Measurement, Inches, group = Species, color = Species)) + geom_point(position = position_jitterdodge(jitter.width = .2, dodge.width = .7), alpha = .1) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, shape = &quot;diamond&quot;, size = 4, color = &quot;black&quot;, position = position_dodge(width = .7)) + stat_summary(fun.data = mean_cl_normal, na.rm = TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;black&quot;, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set1&quot;) 18.4 Three-Way Interaction Plots Just add facet_wrap(~ Part) after the first ggplot() line. ggplot(iris.long, aes(Measurement, Inches, group = Species, color = Species)) + facet_wrap(~ Part) + geom_point(position = position_jitterdodge(jitter.width = .2, dodge.width = .7), alpha = .1) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, shape = &quot;diamond&quot;, size = 4, color = &quot;black&quot;, position = position_dodge(width = .7)) + stat_summary(fun.data = mean_cl_normal, na.rm = TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;black&quot;, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set1&quot;) Therefore, you can see how to plot interactions using group/color and facet_wrap(). Now let’s move on to univariate plots "],
["univariate-plots.html", "Chapter 19 Univariate Plots", " Chapter 19 Univariate Plots This will be a chapter on univariate plots The next Chapter introduces one of the most amazing packages "],
["sjplot.html", "Chapter 20 sjPlot", " Chapter 20 sjPlot This will be a chapter on sjPlot See the next Chapter if you want information on creating Custom tables "],
["custom-tables.html", "Chapter 21 Custom Tables", " Chapter 21 Custom Tables This will be a chapter on creating nice looking Tables in R Now onto Statistical Analysis! "],
["statistical-analysis-overview.html", "Statistical Analysis: Overview 21.1 R Markdown", " Statistical Analysis: Overview 21.1 R Markdown Phew! You’ve made it this far, good job. Up until now you have been learning how to do data preparation steps in R. Now for the fun part, statistical analyses and data visualization! This is the third and final step in the data workflow process depicted above. Traditionally you have likely done these analyses in SPSS or EQS and have created figures in Excel or PowerPoint. The rest of the guide will cover how to do these steps in R. Writing scripts to do statistical analyses is an entirely different process than writing scripts for data preparation. Therefore, we should first go over the general process of conducting and outputing statistical analyses in R. In programs like SPSS when you run a statistical analysis, it will be outputed to a viewable .spv document. One dowfall of this is that .spv files are proprietry format so can only be opened if you have SPSS installed. However, there is the option to export a .spv file as a PDF. One downfall about R is that unlike SPSS, there is not a native way to create output documents from statistical analyses. Fortunately, RStudio has an output document format called R Markdown. 21.1.1 What is an R Markdown File? R Markdown is a powerful way to create reports of statistical analyses. Reports can be outputed in a lot of different formats; html, Microsoft Word, PDF, presentation slides, and more. In fact, this guide was created using R Markdown. The easiest format to output as is html. html documents are opened in a web browser and therefore can be opened on any computer and device (phones, tablets, Windows, Mac). For a brief intro to R Markdown see https://rmarkdown.rstudio.com/lesson-1.html First, you need to install the rmarkdown package install.packages(&quot;rmarkdown&quot;) To open an R Markdown document go to File -&gt; New File -&gt; R Markdown… Select HTML and click OK An example R Markdown document will open. Go ahead and read the contents of the document. There are three types of content in an R Markdown document: A YAML header R code chunks Formatted text 21.1.2 YAML header The YAML header contains metadata about how the document should be rendered and the output format. It is located at the very top of the document and is surrounded by lines of three dashe, --- title: &quot;Title of document&quot; output: html_document --- There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html 21.1.3 R code chunks Unlike a typical R script file (.R), an R Markdown document (.Rmd) is a mixture of formated text and R code chunks. Not everything in an R Markdown document is executed in the R console, only the R code chunks. To run chunks of R code you can click on the green “play” button on the top right of the R code chunk. Go ahead and do this for the three R code chunks in the R Markdown document you opened. (cars and pressure are just example dataframes that come pre-loaded with R). We have not gone over these functions yet, but you can see that the results of the R code are now displayed in the document. The first R code chunk is just setting some default options of how the output of R code chunks should be displayed. We will cover these options in more detail later. 21.1.4 Formatted text The formatted text sections are more than just adding comments to lines of code. You can write up descriptive reports, create bulleted or numbered lists, embed images or web links, create tables, and more. The text is formatted using a language known as Markdown, hence the name R Markdown. Markdown is a convenient and flexible way to format text. When a Markdown document is rendered into some output (such as html or PDF), the text will be formatted as specified by the Markdown syntax. In the R Markdown document you have open you can see some Markdown syntax. The pound signs ## at the beggining of a line are used to format headers. One # is a level one header, two ## is a level two header and so on. Also notice in the second paragraph, the word Knit is surrounded by two asterisks on each side. When this document is redered, thw word Knit will be bolded. Go ahead and render the R Markdown document by clicking on the Knit button at the top of the window. Once it is done rendering you will see a new window pop up. This is the outputed html file. You can see how the document has formated text based on the Markdown syntax. There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. One guide that I frequently reference is https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Something "],
["statistics-in-r.html", "Chapter 22 Statistics in R", " Chapter 22 Statistics in R This will be an introduction to statistical analysis in R Something "],
["structural-equation-modelling.html", "Chapter 23 Structural Equation Modelling 23.1 lavaan 23.2 semoutput", " Chapter 23 Structural Equation Modelling By far the most common statistical analyses we do in this lab are confirmatory factor analysis (CFA) and structural equation modelling (SEM). This Chapter will cover how to conduct CFA’s and SEM’s in R using the lavaan package. 23.1 lavaan Install the lavaan package install.packages(&quot;lavaan&quot;) Visit the lavaan website and navigate to the Tutorial tab. This is an excellent resource for you to consult if you forget any syntax or want more details on using lavaan. You should go over the full tutorial yourself, but I will go ahead and cover the basics here. There are only two main steps to run a lavaan model. Build the model object Run the model with cfa() or sem() 23.1.1 Building the model object The model object is where you specify the model equation for the CFA or SEM. It is actually very easy and intuitive to do. Basically you specify the model equation within single quotes and pass it to an object called model. Let’s say we have want to run a model corresponding to this model diagram: We would simply specify: model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; This defines a CFA model with three latent factors; visual, textual, and speed with ther respective indicators. The indicators need to correspond to column names in the dataframe. There are certain defaults that lavaan uses so that we do not have to specify every single path in the model. For instance, by default it will add correlations between the latent factors in a CFA model. That is why in the model example above, the latent correlations are not explicit, yet they are implicitly part of the model. Model Syntax formula type operator mnemonic latent variable definition =~ is measured by regression ~ is regressed on variance/covariance ~~ is correlated with new parameter := is defined by 23.1.2 Run the model Then the model can be ran using cfa() or sem() functions fit &lt;- cfa(model, data) The first two arguments to pass onto the lavaan functions are model and data, respectively. There are other important arguments that we will cover later. The summary(fit, fit.measures = TRUE, standardized = TRUE) output to a lavaan model looks like ## lavaan 0.6-5 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 301 ## ## Model Test User Model: ## ## Test statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 918.852 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.931 ## Tucker-Lewis Index (TLI) 0.896 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3737.745 ## Loglikelihood unrestricted model (H1) -3695.092 ## ## Akaike (AIC) 7517.490 ## Bayesian (BIC) 7595.339 ## Sample-size adjusted Bayesian (BIC) 7528.739 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.092 ## 90 Percent confidence interval - lower 0.071 ## 90 Percent confidence interval - upper 0.114 ## P-value RMSEA &lt;= 0.05 0.001 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv ## visual =~ ## x1 1.000 0.900 ## x2 0.554 0.100 5.554 0.000 0.498 ## x3 0.729 0.109 6.685 0.000 0.656 ## textual =~ ## x4 1.000 0.990 ## x5 1.113 0.065 17.014 0.000 1.102 ## x6 0.926 0.055 16.703 0.000 0.917 ## speed =~ ## x7 1.000 0.619 ## x8 1.180 0.165 7.152 0.000 0.731 ## x9 1.082 0.151 7.155 0.000 0.670 ## Std.all ## ## 0.772 ## 0.424 ## 0.581 ## ## 0.852 ## 0.855 ## 0.838 ## ## 0.570 ## 0.723 ## 0.665 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv ## visual ~~ ## textual 0.408 0.074 5.552 0.000 0.459 ## speed 0.262 0.056 4.660 0.000 0.471 ## textual ~~ ## speed 0.173 0.049 3.518 0.000 0.283 ## Std.all ## ## 0.459 ## 0.471 ## ## 0.283 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv ## .x1 0.549 0.114 4.833 0.000 0.549 ## .x2 1.134 0.102 11.146 0.000 1.134 ## .x3 0.844 0.091 9.317 0.000 0.844 ## .x4 0.371 0.048 7.779 0.000 0.371 ## .x5 0.446 0.058 7.642 0.000 0.446 ## .x6 0.356 0.043 8.277 0.000 0.356 ## .x7 0.799 0.081 9.823 0.000 0.799 ## .x8 0.488 0.074 6.573 0.000 0.488 ## .x9 0.566 0.071 8.003 0.000 0.566 ## visual 0.809 0.145 5.564 0.000 1.000 ## textual 0.979 0.112 8.737 0.000 1.000 ## speed 0.384 0.086 4.451 0.000 1.000 ## Std.all ## 0.404 ## 0.821 ## 0.662 ## 0.275 ## 0.269 ## 0.298 ## 0.676 ## 0.477 ## 0.558 ## 1.000 ## 1.000 ## 1.000 Yikes!! You really should learn to understand this output, but nicer looking output would be nice right? This is where my semoutput package comes in handy. 23.2 semoutput If you have not done so already open the RStudio project file for this tutorial. Download the PoliticalDemocracy dataset used in the lavaan tutorial Save it to the folder Data Files of your project directory. Install semoutput devtools::install_github(&quot;dr-JT/semoutput&quot;) Additional packages you will need to have installed install.packages(&quot;sjPlot&quot;) install.packages(&quot;semPlot&quot;) Once you install semoutput you should Restart R by going to: Session -&gt; Restart R You can download an R Markdown template for doing CFA and SEM in lavaan. Go to: File -&gt; New File -&gt; R Markdown…From Template -&gt; CFA/SEM (lavaan) 23.2.1 YAML Header At the top of the document is what is called the YAML header. Here is where you can specify certain parameters that you may want to use as default in your analyses. You also need to specify the location and name of the data file you will be working with params: import.file: &quot;&quot; # Relative file path to data mimic: &quot;lavaan&quot; # Which software program to mimic for estimating models missing: &quot;ML&quot; # How to deal with missing values: &quot;ML&quot; or &quot;listwise&quot; std.lv: TRUE # For CFAs, default setting whether to set latent variances to 1 or not std.ov: FALSE # Standardize all observed varialbes? se: &quot;standard&quot; # How to calcualte standard errors: &quot;standard&quot; or &quot;bootstrap&quot; bootstrap: 1000 # If se = &quot;bootstrap&quot; how many boostrap samples? skipping the efa example for now The first R code chunk Required Packages is where you should load any packages used in the document. The next R code chunk is where the data file is imported. The next two R code chunk’s print out a descriptives and correlational tables. The next section is a template for conducting an exploratory factor analysis with psych::fa(). Let’s skip this for now. The next two sections are templates for CFA and SEM using lavaan. For each CFA and SEM section there are 4 subsections. The “Summary Output” subsection displays nice looking tables summarizing the model results The “Diagram Output” subsection will display a model diagram The “Residual Correlation Matrix” subsection will display the residual correlation matrix The “Full Output” subsection will display the results from summary() along with parameter estimates and modification indices. This way you can still get the full output from a lavaan model as it provides more information than the “Summary Output”. You can also add additional output to this section if you need more info about the model. 23.2.2 CFA Example Ultimately we will run the following SEM model. First let’s conduct a CFA of the model. The following error residuals are correlated: y1 and y5; y2 and y4; y2 and y6; y3 and y7; y4 and y8; y6 and y8 To correlate error residuals you would specify: y1 ~~ y5 Move down to the CFA section. First, you need to create a list of the latent factor labels (this is for the output and not running a lavaan model). factors &lt;- c(&quot;dem60&quot;, &quot;ind60&quot;, &quot;dem65&quot;) Then specify the model equation. The commented lines (e.g. # latent factors) are just optional and can be changed or removed. Remember, the factor correlations are implied. model &lt;- &#39; # latent factors # correlated errors # constraints &#39; model &lt;- &#39; # latent factors dem60 =~ y1 + y2 + y3 + y4 ind60 =~ x1 + x2 + x3 dem65 =~ y5 + y6 + y7 + y8 # correlated errors y1 ~~ y5 y2 ~~ y4 y2 ~~ y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 # constraints &#39; You do not need to change anything for cfa(). Unless you want to change some of the defaults you set in the YAML header. fit &lt;- cfa(model = model, data = data, mimic = params$mimic, missing = params$missing, std.lv = params$std.lv, std.ov = params$std.ov, se = params$se, bootstrap = params$bootstrap) Run this R code chunk by pressing the green arrow button. Then run each R code chunk in each subsection to print the output. sem_sig(fit) Table 23.1: Model Significance Sample.Size Chi.Square df p.value 75 38.125 35 0.329 sem_fitmeasures(fit) Table 23.1: Model Fit Measures CFI RMSEA RMSEA.Lower RMSEA.Upper AIC BIC 0.995 0.035 0 0.092 3179.582 3276.916 sem_factorloadings(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 23.1: Factor Loadings Standardized Latent Factor Indicator Loadings sig p Lower.CI Upper.CI SE z dem60 y1 0.850 *** 0 0.765 0.936 0.044 19.435 dem60 y2 0.717 *** 0 0.592 0.843 0.064 11.207 dem60 y3 0.722 *** 0 0.596 0.849 0.064 11.221 dem60 y4 0.846 *** 0 0.759 0.933 0.044 19.020 ind60 x1 0.920 *** 0 0.874 0.965 0.023 39.658 ind60 x2 0.973 *** 0 0.941 1.005 0.017 58.917 ind60 x3 0.872 *** 0 0.812 0.933 0.031 28.304 dem65 y5 0.808 *** 0 0.713 0.903 0.048 16.698 dem65 y6 0.746 *** 0 0.634 0.858 0.057 13.031 dem65 y7 0.824 *** 0 0.734 0.913 0.046 18.063 dem65 y8 0.828 *** 0 0.738 0.918 0.046 18.030 sem_factorcor(fit, factors = factors) Table 23.1: Latent Factor Correlations Factor 1 Factor 2 r sig p Lower.CI Upper.CI SE dem60 ind60 0.447 *** 0 0.242 0.652 0.105 dem60 dem65 0.967 *** 0 0.909 1.024 0.029 ind60 dem65 0.578 *** 0 0.403 0.753 0.089 sem_factorvar(fit, factors = factors) Table 23.1: Latent Factor Variance/Residual Variance Factor 1 Factor 2 var var.std sig p sem_rsquared(fit) Table 23.1: R-Squared Values Variable R-Squared y1 0.7232242 y2 0.5142640 y3 0.5217883 y4 0.7152245 x1 0.8461294 x2 0.9467924 x3 0.7606256 y5 0.6528920 y6 0.5565270 y7 0.6784378 y8 0.6853215 semPaths(fit, latents = factors, whatLabels = &quot;std&quot;, layout = &quot;tree2&quot;, rotation = 2, style = &quot;lisrel&quot;, optimizeLatRes = TRUE, intercepts = FALSE, residuals = TRUE, curve = 1, curvature = 2, sizeLat = 10, nCharNodes = 8, sizeMan = 11, sizeMan2 = 4, edge.label.cex = 1.2, edge.color = &quot;#000000&quot;) modificationIndices(fit, sort. = TRUE, minimum.value = 3) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 59 ind60 =~ y4 4.796 0.577 0.577 0.174 0.174 ## 60 ind60 =~ y5 4.456 0.559 0.559 0.215 0.215 ## 67 dem65 =~ y4 4.260 2.986 2.986 0.898 0.898 ## 72 y1 ~~ y3 3.771 0.849 0.849 0.274 0.274 ## 81 y2 ~~ x1 3.040 -0.155 -0.155 -0.200 -0.200 23.2.3 SEM Example Now let’s run the actual SEM model. Really the only difference is that we will add some regression paths factors &lt;- c(&quot;dem60&quot;, &quot;ind60&quot;, &quot;dem65&quot;) model &lt;- &#39; # latent factors dem60 =~ y1 + y2 + y3 + y4 ind60 =~ x1 + x2 + x3 dem65 =~ y5 + y6 + y7 + y8 # variances # covariances y1 ~~ y5 y2 ~~ y4 y2 ~~ y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 # regressions dem65 ~ dem60 + ind60 dem60 ~ ind60 &#39; fit &lt;- sem(model = model, data = data, mimic = params$mimic, missing = params$missing, std.lv = FALSE, std.ov = params$std.ov, se = params$se, bootstrap = params$bootstrap) sem_sig(fit) Table 23.2: Model Significance Sample.Size Chi.Square df p.value 75 38.125 35 0.329 sem_fitmeasures(fit) Table 23.2: Model Fit Measures CFI RMSEA RMSEA.Lower RMSEA.Upper AIC BIC 0.995 0.035 0 0.092 3179.582 3276.916 sem_factorloadings(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 23.2: Factor Loadings Standardized Latent Factor Indicator Loadings sig p Lower.CI Upper.CI SE z dem60 y1 0.850 *** 0 0.765 0.936 0.044 19.435 dem60 y2 0.717 *** 0 0.592 0.843 0.064 11.207 dem60 y3 0.722 *** 0 0.596 0.849 0.064 11.221 dem60 y4 0.846 *** 0 0.759 0.933 0.044 19.020 ind60 x1 0.920 *** 0 0.874 0.965 0.023 39.658 ind60 x2 0.973 *** 0 0.941 1.005 0.017 58.917 ind60 x3 0.872 *** 0 0.812 0.933 0.031 28.304 dem65 y5 0.808 *** 0 0.713 0.903 0.048 16.698 dem65 y6 0.746 *** 0 0.634 0.858 0.057 13.031 dem65 y7 0.824 *** 0 0.734 0.913 0.046 18.063 dem65 y8 0.828 *** 0 0.738 0.918 0.046 18.030 sem_paths(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 23.2: Regression Paths Standardized Predictor DV Path Values SE z sig p Lower.CI Upper.CI dem60 dem65 0.885 0.052 17.100 *** 0.000 0.784 0.987 ind60 dem65 0.182 0.073 2.498 0.013 0.039 0.325 ind60 dem60 0.447 0.105 4.267 *** 0.000 0.242 0.652 sem_factorcor(fit, factors = factors) Table 23.2: Latent Factor Correlations Factor 1 Factor 2 r sig p Lower.CI Upper.CI SE sem_factorvar(fit, factors = factors) Table 23.2: Latent Factor Variance/Residual Variance Factor 1 Factor 2 var var.std sig p dem60 dem60 3.956 0.800 *** 0.000 ind60 ind60 0.448 1.000 *** 0.000 dem65 dem65 0.172 0.039 0.434 sem_rsquared(fit) Table 23.2: R-Squared Values Variable R-Squared y1 0.7232243 y2 0.5142639 y3 0.5217879 y4 0.7152243 x1 0.8461294 x2 0.9467924 x3 0.7606255 y5 0.6528920 y6 0.5565263 y7 0.6784382 y8 0.6853214 dem60 0.1995522 dem65 0.9609949 semPaths(fit, latents = factors, whatLabels = &quot;std&quot;, layout = &quot;tree2&quot;, rotation = 2, style = &quot;lisrel&quot;, optimizeLatRes = TRUE, intercepts = FALSE, residuals = TRUE, curve = 1, curvature = 2, sizeLat = 10, nCharNodes = 8, sizeMan = 11, sizeMan2 = 4, edge.label.cex = 1.2, edge.color = &quot;#000000&quot;) Something "]
]
