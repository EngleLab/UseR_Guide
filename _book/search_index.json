[
["index.html", "R Tutorial for the EngleLab Preface", " R Tutorial for the EngleLab Jason Tsukahara 2018-11-13 Preface This is an R Tutorial designed specifically for the EngleLab. This tutorial is specific to what you would need to know to do most data processing and analyses that are common in our lab. Hopefully this will mean less time learning R and more time using R. And as you start using R, you can learn more about it as you go. There are more detailed tutorials and resources mentioned throughout this guide. If you want to become more proficient in R it is a good idea to go through those as well. I essentially learned R by using it and Googling how to do certain things in R. There are a lot of forums where people have asked the same questions as you and someone has provided a clear answer. I suggest taking advantage of Google in this way. R is becoming a popular tool for doing statistical analyses across many scientific disciplines. It has many advantages but it does also takes some time to learn. Chapters 1 and 2 go over the basics of what you need to know to start using R. Chapter 3 introduces the general data processing workflow in our lab. Chapter 4 covers the basics of working with dataframes, including how to do transformations, computations, creating new variables, filtering, renaming, merging and more. This will give you the skills you need to go from raw data files to a data file that is organized and ready for statistical analyses. Chapter 5 will cover principles of how you should structure and organize the workflow in your R scripts. This chapter is very important In Chapters 6 and 7 you will use a Flanker data set as an example of how to go from a “messy” raw data file to a “tidy” raw data file and finally to a scored task file that is ready for statistical analysis. Before getting into using R, let’s talk about how R scripts can automate our workflow. "],
["installation.html", "Chapter 1 Installation 1.1 Installing R 1.2 Installing R Studio 1.3 The R Studio Environemnt 1.4 R Studio Settings", " Chapter 1 Installation 1.1 Installing R First you need to download the latest version of R from their website https://www.r-project.org Select CRAN on the left, just under Download Select the first option under 0-Cloud Select the download option depending on your computer Select the base installation (for Windows) or the Latest Release (for Mac) Open and Run the installation file 1.2 Installing R Studio The easiest way to interact with R is through the R Studio environment. To do this you need to install R Studio from https://www.rstudio.com/products/rstudio/download/#download Select the Free version of R Studio Desktop Select the download option depending on your computer 1.3 The R Studio Environemnt Go ahead an open the RStudio application on your computer. When you open a fresh session of RStudio there are 3 window panes open. The Console window, the Environment window, and the Files window. Go ahead and navigate to File -&gt; New File -&gt; R Script. You should now see something similar to the image below There are 4 window panes and each one has it’s own set of tabs associated with it: The Console window (the bottom left window pane) is where code is executed and output is displayed. The Source window (the top left window pane) is where you will write your code to create a script file. When you open a new script file you will see a blank sheet where you can start writing the script. When you execute lines of code from here you will see it being executed in the Console window. The Source window is also where you can view dataframes you have just imported or created. In the image above, notice the different tabs in the Source window. There are two “Untitled” script files open and one dataframe called ‘data’. The Environment window (top right window pane) is where you can see any dataframes, variables, or functions you have created. Go ahead and type the following in your Console window and hit enter. hello &lt;- &quot;hello&quot; You should now see the object hello in the Environment window pane The Files window (the bottom right window pane) is where you can see your computer’s directories, plots you create, manage packages, and see help documentation. 1.4 R Studio Settings There are a few changes to R Studio settings I suggest you make. I will not go into why these are a good idea - so just do what I say! If you want to know you can talk to me about it. Navigate to Tools -&gt; Global Options Change the settings to look like this: Be sure to set ‘Save workspace to .RData on exit’ to Never You can also change the “Editor Theme” if you navigate to the “Appearance” tab in Settings. Dark themes are easier on the eyes. I use Material dark theme. Now you are ready to start writing some R code! "],
["basics.html", "Chapter 2 Basics 2.1 Creating R objects 2.2 If…then Statements 2.3 For Loops 2.4 Functions 2.5 R Packages 2.6 Installing and Loading R Packages 2.7 More R Basic Resources", " Chapter 2 Basics This chapter will cover the basics of how to assign values to objects, create and extract information from vectors, lists, and dataframes. 2.1 Creating R objects In R, everything that exists is an object and everything you do to objects are functions. You can define an object using the operator &lt;-. For instance, string &lt;- &quot;hello&quot; string ## [1] &quot;hello&quot; In this example, the first line defines the object string with a value of “hello”. The second line simply prints the output of string to the Console window. Notice how I included &quot; &quot; around hello. This tells R that hello is a string, not an object. If I were to not include &quot; &quot;, then R would think I am calling an object. And since there is no object with the name hello it will print an error string &lt;- hello ## Error in eval(expr, envir, enclos): object &#39;hello&#39; not found Do not use &quot; &quot; for Numerical values a &lt;- 5 a ## [1] 5 You can execute commands by: Typing them directly into the Console window Typing them into the Source window and then on that line of code pressing Ctrl+Enter. With Ctrl+Enter you can execute one line of your code at a time. Go ahead and define an object with a value and print it to the Console window. You should now see an object with the value you assigned to it in the Environment window. It is important to know that EVERYTHING in R is case sensitive. That means you can’t use A in place of a A + 5 ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found a + 5 ## [1] 10 2.1.1 Classes character &quot;hello&quot;, &quot;19&quot; numeric (or double) 2, 32.55 integer 5, 99 logical TRUE, FALSE To evaluate the class of an object you can use the typeof() typeof(a) ## [1] &quot;double&quot; To change the class of an object you can use the function as.character() , as.numeric() , as.double() , as.integer() , as.logical() functions. as.integer(a) ## [1] 5 as.character(a) ## [1] &quot;5&quot; as.numeric(&quot;hello&quot;) ## Warning: NAs introduced by coercion ## [1] NA 2.1.2 Vectors Okay so now I want to talk about creating more interesting objects than just a &lt;- 5. If you are going to do anything in R it is important that you understand the different data types and data structures you can use in R. I will not cover all of them in this tutorial. For more information on data types and structures you should go to https://ramnathv.github.io/pycon2014-r/learn/structures.html Vectors contain elements of data. The length of a vector is the number of elements in the vector. For instance, the variable a we created earlier is actually a vector of length 1. It contains one element with a value of 5. Now let’s create a vector with more than one element. b &lt;- c(1,3,5) c() is a function. Functions contain arguments that are inputs for the function. Arguments are separated by commas. In this example the c() fucntion concatenates the arguments (1, 3, 5) into a vector. We are passing the result of this function to the object b. What do you think the output of b will look like? b ## [1] 1 3 5 You can see that we now have a vector that contains 3 elements; 1, 3, 5. If you want to reference the value of specific elements of a vector you use brackets [ ]. For instance, b[2] ## [1] 3 The value of the second element in vector b is 3. Let’s say we want to grab only the 2nd and 3rd elements. We can do this at least two differnt ways. b[2:3] ## [1] 3 5 b[-1] ## [1] 3 5 Now, it is important to note that we have not been changing vector b. If we display the output of b, we can see that it still contains the 3 elements. b ## [1] 1 3 5 To change vector b we need to define b as vector b with the first element removed b &lt;- b[-1] b ## [1] 3 5 Vector b no longer contains 3 elements. Now, let’s say we want to add an element to vector b. c(5,b) ## [1] 5 3 5 Here the c() fucntion created a vector with the value 5 as the first element followed by the values in vector b Or we can use the variable a that has a value of 5. Let’s add this to vector b b &lt;- c(a,b) b ## [1] 5 3 5 What if you want to create a long vector with many elements? If there is a pattern to the sequence of elements in the vector then you can create the vector using seq() seq(0, 1000, by = 4) ## [1] 0 4 8 12 16 20 24 28 32 36 40 44 48 52 ## [15] 56 60 64 68 72 76 80 84 88 92 96 100 104 108 ## [29] 112 116 120 124 128 132 136 140 144 148 152 156 160 164 ## [43] 168 172 176 180 184 188 192 196 200 204 208 212 216 220 ## [57] 224 228 232 236 240 244 248 252 256 260 264 268 272 276 ## [71] 280 284 288 292 296 300 304 308 312 316 320 324 328 332 ## [85] 336 340 344 348 352 356 360 364 368 372 376 380 384 388 ## [99] 392 396 400 404 408 412 416 420 424 428 432 436 440 444 ## [113] 448 452 456 460 464 468 472 476 480 484 488 492 496 500 ## [127] 504 508 512 516 520 524 528 532 536 540 544 548 552 556 ## [141] 560 564 568 572 576 580 584 588 592 596 600 604 608 612 ## [155] 616 620 624 628 632 636 640 644 648 652 656 660 664 668 ## [169] 672 676 680 684 688 692 696 700 704 708 712 716 720 724 ## [183] 728 732 736 740 744 748 752 756 760 764 768 772 776 780 ## [197] 784 788 792 796 800 804 808 812 816 820 824 828 832 836 ## [211] 840 844 848 852 856 860 864 868 872 876 880 884 888 892 ## [225] 896 900 904 908 912 916 920 924 928 932 936 940 944 948 ## [239] 952 956 960 964 968 972 976 980 984 988 992 996 1000 Vectors can only contain elements of the same “class”. d &lt;- c(1, &quot;2&quot;, 5, 9) d ## [1] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;9&quot; as.numeric(d) ## [1] 1 2 5 9 2.1.3 Factors Factors are special types of vectors that can represent categorical data. You can change a vector into a factor object using factor() factor(c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;)) ## [1] male female male male female female male ## Levels: female male factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;)) ## [1] high low medium high high low medium medium ## Levels: high low medium f &lt;- factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;), levels = c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;)) f ## [1] high low medium high high low medium medium ## Levels: high medium low 2.1.4 Lists Lists are containers of objects. Unlike Vectors, Lists can hold different classes of objects. list(1, &quot;2&quot;, 2, 4, 9, &quot;hello&quot;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] &quot;hello&quot; You might have noticed that there are not only single brackets, but double brackets [[ ]] This is because Lists can hold not only single elements but can hold vectors, factors, lists, dataframes, and pretty much any kind of object. l &lt;- list(c(1,2,3,4), &quot;2&quot;, &quot;hello&quot;, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) l ## [[1]] ## [1] 1 2 3 4 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] &quot;hello&quot; ## ## [[4]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; You can see that the length of each element in a list does not have to be the same. To reference the elements in a list you need to use the double brackets [[ ]]. l[[1]] ## [1] 1 2 3 4 To reference elements within list elements you use double brackets followed by a single bracket l[[4]][2] ## [1] &quot;b&quot; You can even give names to the list elements person &lt;- list(name = &quot;Jason&quot;, phone = &quot;123-456-7890&quot;, age = 23, favorite_colors = c(&quot;blue&quot;, &quot;red&quot;, &quot;brown&quot;)) person ## $name ## [1] &quot;Jason&quot; ## ## $phone ## [1] &quot;123-456-7890&quot; ## ## $age ## [1] 23 ## ## $favorite_colors ## [1] &quot;blue&quot; &quot;red&quot; &quot;brown&quot; And you can use the names to reference elements in a list person[[&quot;name&quot;]] ## [1] &quot;Jason&quot; person[[&quot;favorite_colors&quot;]][3] ## [1] &quot;brown&quot; 2.1.5 Data Frames You are probably already familiar with this type of data structure. SPSS and Excel uses this type of structure. It is just rows and columns of data. A data table! This is the format that is used to perform statiscital analyses on. So let’s create a data frame so you can see what one looks like in RStudio data &lt;- data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) data ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 You can view the Data Frame by clicking on the object in the Environment window or by executing the command View(data) Notice that it created three columns labeled “id”, “x”, and “y”. Also notice that since we only specified a vector of length 2 for “x” this column is coerced into 10 rows of repeateding “a” and “b”. All columns in a dataframe need to have the same number of rows. You can use the $ notation to reference just one of the columns in the dataframe data$y ## [1] 10 20 30 40 50 60 70 80 90 100 Alternatively you can use data[&quot;y&quot;] ## y ## 1 10 ## 2 20 ## 3 30 ## 4 40 ## 5 50 ## 6 60 ## 7 70 ## 8 80 ## 9 90 ## 10 100 To reference only certain rows within a column data[1:5,&quot;y&quot;] ## [1] 10 20 30 40 50 data$y[1:5] ## [1] 10 20 30 40 50 2.2 If…then Statements If…then statements are useful for when you need to execute code only if a certain statement is TRUE. For instance,… First we need to know how to perform logical operations in R Okay, we have this variable a a &lt;- 5 Now let’s say we want to determine if the value of a is greater than 3 a &gt; 3 ## [1] TRUE You can see that the output of this statement a &gt; 3 is TRUE Here is a list of logical operations in R Now let’s write an if…then statement. If a is greater than 3, then multiply a by 2. if (a&gt;3){ a &lt;- a*2 } a ## [1] 10 The expression that is being tested is contained in parentheses, right after the if statement. If this expression is evaluated as TRUE then it will perform the next line(s) of code. The { is just a way of encasing multiple lines of code within one if statement. The lines of code then need to be closed of with }. In this case, since we only had one line of code b &lt;- a*2 we could have just written it as. a &lt;- 5 if (a&gt;3) a &lt;- a*2 a ## [1] 10 What if we want to do something to a if a is NOT greater than 3? In other words… if a is greater than 3, then multiple a by 2 else set a to missing a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 You can keep on chaining if…then… else… if… then statements together. a &lt;- 5 if (is.na(a)){ print(&quot;Missing Value&quot;) } else if (a&lt;0){ print(&quot;A is less than 0&quot;) } else if (a&gt;3){ print(&quot;A is greater than 3&quot;) } ## [1] &quot;A is greater than 3&quot; 2.3 For Loops For loops allow you iterate the same line of code over multiple instances. Let’s say we have a vector of numerical values c &lt;- c(1,6,3,8,2,9) c ## [1] 1 6 3 8 2 9 and want perform an if…then operation on each of the elements. Let’s use the same if…then statement we used above. If the element is greater than 3, then multiply it by 2 - else set it to missing. Let’s put the results of this if…then statement into a new vector d What we need to do is loop this if…then statement for each element in c We can start out by writing the for loop statement for (i in seq_along(c)){ } This is how it works. The statement inisde of parathenses after for contains two statements separated by in. The first statement is the variable that is going to change it’s value over each iteration of the loop. You can name this whatever you want. In this case I chose the label i. The second statement defines all the values that will be used at each iteration. The second statement will always be a vector. In this case the vector is seq_along(c). seq_along() is a function that creates a vector that contains a sequence of numbers from 1 to the length of the object. In this case the object is the vector c, which has a length of 6 elements. Therefore seq_along(c), creates a vector containing 1, 2, 3, 4, 5, 6. The for loop will start with i defined as 1, then on the next iteration the value of i will be 2 … and so until the last element of seq_along(c), which is 6. We can see how this is working by printing ‘i’ on each iteration. for (i in seq_along(c)){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 You can see how on each iteration it prints the values of seq_along(c) from the first element to the last element. What we will want to do is, on each iteration of the for loop, access the ith element of the vector c. Recall, you can access the element in a vector with [ ], for instance c[1]. Let’s print each ith element of c. for (i in seq_along(c)){ print(c[i]) } ## [1] 1 ## [1] 6 ## [1] 3 ## [1] 8 ## [1] 2 ## [1] 9 Now instead of printing i the for loop is printing each element of vector c. Let’s use the same if…then statement as above a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 But instead we need to replace a with c[i] For now let’s just print() the output of the if… then statement. for (i in seq_along(c)){ if (c[i] &gt; 3){ print(c[i]*2) } else { print(NA) } } ## [1] NA ## [1] 12 ## [1] NA ## [1] 16 ## [1] NA ## [1] 18 Now for each element in c, if it is is greater than 3, then multiply it by 2 - else set as missing value. You can see that on each iteration the output is either the ith element of c multiplied by 2 or NA. But just printing things to the console is useless! Let’s overwright the old values in c with the new values. for (i in seq_along(c)){ if (c[i] &gt; 3){ c[i] &lt;- c[i]*2 } else { c[i] &lt;- NA } } But what if we want to preserve the original vector c? Well we need to put it into a new vector, let’s call it vector d. This get’s a little more complicated but is something you might find yourself doing fairly often so it is good to understand how this works. But if you are goind to do this to a “new” vector that is not yet created you will run into an error. c &lt;- c(1,6,3,8,2,9) for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } You first need to create vector d - in this case we can create an empty vector. d &lt;- c() So the logic of our for loop, if…then statement is such that; on the ith iteration - if c[i] is greater than 3, then set d[i] to c[i]*2 - else set d[i] to NA. c &lt;- c(1,6,3,8,2,9) d &lt;- c() for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } c ## [1] 1 6 3 8 2 9 d ## [1] NA 12 NA 16 NA 18 Yay! Good job. 2.4 Functions Basically anything you do in R is by using functions. In fact, learning R is just learning what functions are available and how to use them. Not much more to it than that. You have only seen a couple of functions at this point. In this chapter, a common function used was c(). This function simply concatenates a series of numerical or string values into a vector. c(1,6,3,7). Functions start with the name of the function followed by parentheses function_name(). Inside the () is where you specify certain arguments separted by commas , . Some argruments are optional and some are required for the function to work. For example, another function you saw last chapter was data.frame(). This function creates a dataframe with the columns specified by arguments. data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 The arguments id, x, and y form the columns in the dataframe. These arguments themselves used functions. For instance y used the function seq(). This function creates a sequence of numbers in a certain range at a given interval. Sometimes arguments are not defined by an =. The first two arguments in in seq() specify the range of 10 to 100. The third argument by specified the interval to be 10. So seq(10, 100, by = 10) creates a sequence of numbers ranging from 10 to 100 in intervals of 10. seq(10, 100, by = 10) ## [1] 10 20 30 40 50 60 70 80 90 100 In the seq() function the by argument is not required. This is because there is a default by value of 1. seq(10, 100) ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## [18] 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 ## [35] 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 ## [52] 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 ## [69] 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 ## [86] 95 96 97 98 99 100 Obviously if you want to specify a different interval, then you will need to specify by =. 2.4.1 Creating Your Own Functions This section is optional. It will go over how to create your own functions. Even if you do not want to get too proficient in R, it can be a good idea to know how to create your own function. It also helps you better understand how functions actaully work. We are going to create a function that calculates an average of values. To define a function you use the function() and assign the output of function() to an object, which becomes the name of the function. For instance, function_name &lt;- function(){ } This is a blank function so it is useless. Before we put stuff inside of a function let’s work out the steps to calculate an average. Let’s say we have an array a that has 10 elements a &lt;- c(1,7,4,3,8,8,7,9,2,4) a ## [1] 1 7 4 3 8 8 7 9 2 4 To calculate an average we want to take the sum of all the values in a and divide it by the number of elements in a. To do this we can use the sum() and length() functions. sum(a) ## [1] 53 length(a) ## [1] 10 sum(a)/length(a) ## [1] 5.3 Easy! So now we can just put this into a function. average &lt;- function(x){ avg &lt;- sum(x)/length(x) return(avg) } When creating a function, you need to specify what input arguments the function is able to take. Here were are specifying the argument x. You can use whatever letter or string of letters you want, but a common notation is to use x for the object that is going to be evaluated by the function. Then, inside the function we use the same letter x to calculate the sum() and length() of x. What this means is that Arguments specified in a function become objects (or variables) passed inside the function You can create new objects inside a function. For instance we are creating an object, avg. However, these objects are created only inside the environment of the function. You cannot use those objects outside the function and they will not appear in your Environment window. To pass the value of an object outside of the function, you need to specify what you want to return() or what is the outpute of the function. In this case it is the object avg that we created inside the function. Let’s see the function in action average(a) ## [1] 5.3 Cool! You created your first function. Becuase the function only takes one argument x it knows that whatever object we specify in average() is the object we want to evaluate. But what if our vector contains missing values? b &lt;- c(1,NA,4,2,7,NA,8,4,9,3) average(b) ## [1] NA Uh oh. Here the vector b contains two missing values and the function average(b) returns NA. This is becuase in our function we use the function sum() without specifiying to ignore missing values. If you type in the console ?sum you will see that there is an argument to specify whether missing values should be removed or not. The default value of this argument is FALSE so if we want to remove the missing values we need to specify na.rm = TRUE. It is a good idea to make your functions as flexible as possible. Allow the user to decide what they want to happen. For instance, it might be the case that the user wants a value of NA returned when a vector contains missing values. So we can add an argument to our average() function that allows the user to decide what they want to happen; ignore missing values or return NA if missing values are present. Let’s label this argument na.ignore. We could label it na.rm like the sum() function but for the sake of this Tutorial I want you to learn that you can label these arguments however you want, it is arbitrary. The label should make sense however. Before we write the function let’s think about what we need to change inside the function. Basically we want our new argument na.ignore to change the value of na.rm in the sum() function. If na.ignore is TRUE then we want na.rm = TRUE. Remember that arguments become objects inside of a function. So we will want to change: avg &lt;- sum(x)/length(x) to avg &lt;- sum(x, na.rm = na.ignore)/length(x) Let’s try this out on our vector b na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b) ## [1] 3.8 We can test if our average function is calculating this correctly by using the actual base R function mean(). mean(b, na.rm = TRUE) ## [1] 4.75 Uh oh. We are getting different values. This is because length() is also not ignoring missing values. The length of b, is 10. The length of b ignoring missing values is 8. Unfortunately, length() does not have an argument to specify we want to ignore missing values. How we can tell length() to ignore missing values is by length(b[!is.na(b)]) ## [1] 8 This is saying, evaluate the length of elements in b that are not missing. Now we can modify our function with na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b[!is.na(b)]) ## [1] 4.75 to get average &lt;- function(x, na.ignore = FALSE){ avg &lt;- sum(x, na.rm = na.ignore)/length(x[!is.na(x)]) return(avg) } average(b, na.ignore = TRUE) ## [1] 4.75 mean(b, na.rm = TRUE) ## [1] 4.75 Walla! You did it. You created a function. Notice that we set the default value of na.ignore to FALSE. If we had set it as TRUE then we would not need to specify average(na.ignore = TRUE) since TRUE would have been the default. When using functions it is important to know what the default values are Both for loops and functions allow you to write more concise and readable code. If you are copying and pasting the same lines of code with only small modification, you can probably write those lines of code in a for loop or a function. 2.5 R Packages R comes with a basic set of functions. All the functions we have used so far are part of the R basic functions. But when you want to start doing more complex operations it would be nice to have more complex functions. This is where R Packages come in… An R Package is simply a collection of functions - that usually have some common theme to them. Now the most wonderful thing about R is that other R users have developed tons of packages with functions they created themselves. For instance, a group of users have developed an R package called lavaan that makes it extremely easy to conduct SEM in R. 2.6 Installing and Loading R Packages R packages are easy to install and load. You just need to know the name of the package. install.packages(&quot;name_of_package&quot;) or for multiple packages at once install.packages(c(&quot;package1&quot;, &quot;package2&quot;, &quot;package3&quot;)) Installing the package does not mean you can start using the functions. To be able to use the function you need to then load the package library of functions as such library(name_of_package) When loading packages you do not have to incase the package name in &quot; &quot; 2.7 More R Basic Resources For additional tips in the basics of coding R see: https://ramnathv.github.io/pycon2014-r/visualize/README.html https://www.datacamp.com/courses/free-introduction-to-r/?tap_a=5644-dce66f&amp;tap_s=10907-287229 http://compcogscisydney.org/psyr/ http://r4ds.had.co.nz/workflow-basics.html To learn about creating your own functions see We are almost ready to start working with some data, but first we need to go over functions and R packages "],
["workflow.html", "Chapter 3 Workflow", " Chapter 3 Workflow The image below represents a general workflow I like to use when going from raw data files to statistical analyses. The first step of the data processing workflow is to convert “messy” raw data files to “tidy” raw data files. The experiment software will produce a “messy” data file. In this file there are usually more rows and columns than you would ever be interested in, variable or values are named incoherently (i.e. stimSlide2.RT), or there may be separate files for each subject. It will be easier to work with a “tidy” raw data file. A “tidy” raw data file has only the rows and columns that are relevant (one row for each trial), variable and values are named coherently (i.e. RT), and there is one file that contains data for all subjects. The next step is to score the data. For most statistical analyses you will want to aggregate the trial-level data into one or more dependent measures for that task. The scored data file will have one row per subject (or possibly one row per subject per experimental conditions). These first two steps are all about Data Preparation. Getting your data prepared for your planned statistical analyses. Finally, you are now ready for the fun part, statistical analyses. This involves evaluating the univariate descriptive statistics of your data set, running statistical tests, and creating tables and figures to interpret and present your results. First thing is you will need to install R and R Studio on your computer. "],
["the-tidyverse.html", "Chapter 4 The tidyverse 4.1 readr - import 4.2 dplyr 4.3 readr - export", " Chapter 4 The tidyverse The tidyverse is a collection of R packages that share an underlying design philosophy, grammar, and data structures. Hadley Wickham has been the main contributor to developing the tidyverse. Although you will be learning R in this tutorial, it might be more appropriate to say that you are learning the tidyverse. The tidyverse consists of packages that are simple and intuitive to use and will take you from importing data (with readr), to transforming and manipulating data structures (with dplyr and tidyr), and to data vizualisation (with ggplot2). This chapter will cover the two primary packages you will use in every R script you write, the readr and the dplyr packages. 4.1 readr - import Every R script you write to do data preparation will require you to import a data file and create/save a new data file. The readr package contains useful functions for importing and saving data files. Go ahead and install the readr package. In the console type: install.packages(&quot;readr&quot;) If you have not done so already, open a new R script file. To create a new R script go to File -&gt; New File -&gt; R Script This should have opened a blank Script window called Untitled. The Script window is your workspace. This is where you will write, edit, delete, re-write, your code. To execute a line of code in the script, place the cursor anywhere on that line and press Ctrl + Enter Now load the package into your R session by typing this at the top of your R script file. Execute the line of code by placing the cursor anywhere on the line and press Ctrl + Enter library(readr) The nice thing about R Studio is that there is a GUI for importing data files. This makes it easier to figure what code you need to use in order to import a data file. Once you install readr, you will be able to access this package in the import GUI. In the Environment window click on “Import Dataset”. You will see several options available, these options all rely on different packages. For most data files the readr package will be adequate. Occasionally you may want to import excel or other data files that are not supported by readr. To illustrate how readr and dplyr work we will use an example data set from an Arrow Flanker task. Download Example Tidyverse Data For this tutorial, it will be a good idea to create a folder somewhere to save all your R scripts and example data files. Now let’s actually import the data file using the readr package. Click on “Import Dataset” -&gt; select “From Text (readr)…” You will see a data import window open up that looks like this Select “Browse” on the top right and select the data file you want to import. The “Data Preview” window will let you see if it is importing it in the right format. You might have to change some of the agruments at the bottom. It is likely you will have to change “Delimeter” to “Tab”. You might want to change the “Name” but you can always do this later in the R Script. Make sure all the settings are correct by assessing the “Data Preview” window. Does the dataframe look as you would expect it to? Finally, copy the code you need in the “Code Preview” box at the bottom right. You might not always need the library(readr) or View(data) lines. Rather than selecting “Import” I suggest just closing out of the window and pasting the code into your R script. You might have something like library(readr) import &lt;- read_delim(&quot;data/Flanker_raw.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) When I import a data file I typically assign it to an R object called import. You can use whatever name you like but I suggest being consistent across your R scripts. You can then view the data file with View(import) Take some time to get familiar with this data file. Notice that there is one row per trial per subject. There are also rows for practice trials and real trials. Then there are columns for their reaction time, accuracy, what the Condition is, and even the date and time of they performed the task. We will cover saving data files at the end of this chapter. 4.2 dplyr Every R script you write will involve transforming and manipulating a dataframe. Some refer to this process as data wrangling. The dplyr package will serve as the underlying framework for how you will think about wrangling a data set. You can do most tasks using the dplyr package with the occasional help from other packages. dplyr uses intuitive langauge that you are already familiar with. As with any R function, you can think of functions in the dplyr package as verbs - that refer to performing a particular action on a dataframe. The core dplyr functions are: rename() renames columns filter() filters rows based on their values in specified columns select() selects (or removes) columns mutate() creates new columns based on transformation from other columns, edits values within existing columns group_by() splits dataframe into separate groups based on specified columns summarise() aggregates across rows to create a summary statistic (means, standard deviations, etc.) For more information on these functions Visit the dplyr webpage If you have not done so already, install and load the dplyr package install.packages(&quot;readr&quot;) library(dplyr) Let’s see a quick example of all these functions with the example dataframe you just imported. You should always have in mind the final resulting dataframe you want to create. In this case, what we will want to do is calculate a Flanker Effect for each subject. This means the resulting dataframe will have one row per subject and a column that contains values on the Flanker Effect. Right now the example data set has one row per trial per subject. There are 102756 rows in the dataframe. It is important to evaluate that our script is working correctly. One way we can do this is to make sure the resulting data frame has the expected number of rows. Since we are creating a dataframe with one row per subject the number of rows should be equal to the number of subjects. The function unique() will create an array of all unique values in a column. Recall that to access a column in a dataframe we can use $. So we can get an array of all unique subject ids. unique(import$Subject) ## [1] 12000 12002 12006 12008 12012 12013 12014 12015 12016 12018 12021 ## [12] 12022 12023 12024 12025 12027 12028 12030 12033 12034 12035 12038 ## [23] 12039 12040 12041 12043 12047 12048 12049 12050 12051 12052 12053 ## [34] 12057 12058 12059 12060 12061 12062 12063 12064 12065 12068 12069 ## [45] 12070 12071 12072 12076 12078 12080 12081 12083 12084 12087 12089 ## [56] 12092 12094 12095 12096 12097 12098 12101 12102 12103 12104 12105 ## [67] 12107 12109 12110 12112 12113 12115 12116 12117 12118 12121 12122 ## [78] 12123 12124 12125 12126 12127 12129 12133 12134 12135 12136 12137 ## [89] 12138 12139 12140 12141 12142 12143 12144 12147 12148 12149 12150 ## [100] 12153 12154 12155 12160 12161 12162 12163 12166 12167 12168 12171 ## [111] 12172 12173 12174 12175 12176 12177 12178 12179 12180 12181 12183 ## [122] 12184 12186 12187 12194 12195 12202 12203 12204 12207 12211 12216 ## [133] 12218 12220 12222 12223 12225 12227 12230 12232 12235 12236 12237 ## [144] 12238 12239 12240 12241 12242 12243 12245 12246 12247 12249 12250 ## [155] 12251 12252 12253 12254 12255 12256 12257 12258 12259 12260 12261 ## [166] 12262 12263 12264 12266 12267 12269 12270 12271 12272 12274 12275 ## [177] 12280 12282 12284 12285 12287 12288 12290 12291 12292 12293 12294 ## [188] 12295 12296 12298 12300 12301 12302 12303 12304 12305 12306 12307 ## [199] 12308 12309 12310 12311 12313 12314 12316 12317 12318 12319 12320 ## [210] 12321 12322 12323 12324 12326 12328 12329 12330 12331 12332 12333 ## [221] 12337 12339 12341 12342 12343 12344 12345 12346 12348 12349 12351 ## [232] 12352 12353 12354 12355 12358 12359 12360 12361 12362 12363 12365 ## [243] 12366 12367 12369 12370 12371 12373 12374 12376 12378 12380 12381 ## [254] 12382 12383 12384 12385 12386 12387 12390 12391 12392 12395 12396 ## [265] 12399 12400 12401 12402 12403 12404 12405 12406 12407 12408 12409 ## [276] 12410 12411 12412 12414 12415 12416 12417 12418 12419 12420 12421 ## [287] 12422 12424 12426 12427 12428 12429 12430 12431 12432 12433 12434 ## [298] 12435 12437 12438 12439 12443 12444 12445 12446 12447 12448 12451 ## [309] 12453 12454 12455 12456 12457 12458 12460 12461 12462 12463 12464 ## [320] 12467 12468 12469 12470 12472 12473 12474 12475 12476 12477 12478 ## [331] 12480 12481 12482 12483 12479 12485 12486 12487 12488 12490 12491 ## [342] 12492 Rather than counting how many elements there are we can just use length() evaluate how many elements there are in the array. length(unique(import$Subject)) ## [1] 342 Notice how you just wrapped a function around another function. This is allowed, and can make your script more concise. Cool, so in our final dataframe at the end of the R script we should expect to have 342 rows 4.2.1 rename() We do not really need to, but let’s go ahead and rename() a column. How about instead of ACC let’s label it as Accuracy. Pretty simple data &lt;- rename(import, Accuracy = ACC) rename() is really only useful if you are not also using select() or mutate(). In select() you can also rename columns as you select them to keep. I’ll illustrate this later 4.2.2 filter() We do not want to include practice trials when calculating the mean and standard deviation on RTs. In other words we want to remove rows that correspond to practice trials. We will use filter() to do remove these rows. To do so we first need to know the name of the column that contains this information and the values in that column which identifies practice trials. The column name is TrialProc unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; And the value that identifies practice trials is practice. Real trials are identified as real filter() is inclusive so we can either specify data &lt;- filter(data, TrialProc != &quot;practice&quot;) or data &lt;- filter(data, TrialProc == &quot;real&quot;) There is a lot of consistency of how you specify arguments in the dplyr package. 1) You always first specify the dataframe that the function is being performed on, followed by the arguments for that function. 2) Column names can be called just like regular R objects, that is without putting the column name in &quot; &quot; like you do with strings. If all you know is dplyr, then this might not seem like anything special but it is. Most non-tidyverse functions will require you to put &quot; &quot; around column names. A filter is basically a logical statement. What we want to do is; If TrialProc is not equal to “practice” (or alternatively is equal to “real”), Then keep (include) the row - otherwise remove it. Notice that I passed the output of this function to a new object data. I like to keep the object import as the original imported file and any changes will be passed onto a new dataframe, such as data. This makes it easy to go back and see what the original data is. Because if we were to overwrite import then we would have to execute the read_delim() import function again to be able to see the original data file, just a little more tedious. Go ahead and view data. Did it properly remove practice trials? It can be hard to be certain about this when there are so many rows! To be certain evaulate unique values in data$TrialProc. unique(data$TrialProc) ## [1] &quot;real&quot; Only “real” trials! Good. Now that I think of it, let’s only calculate mean and standard deviation RTs on accurate trials. The column ACC contains information about whether the trial was accurate 1 or inaccurate 0. Go ahead and filter on accuracy. To make your script more concise you should include both filters in the same function data &lt;- filter(data, TrialProc == &quot;real&quot;, Accuracy==1) We are filtering on only real and accurate trials. 4.2.3 select() Let’s see an example of select(). This step is actually not necessary because summarise() will end up removing irrelevant columns anyways. Let’s keep at least one irrelevant column but remove the rest, so we can illustrate this when we get to summarise(). Let’s keep Subject, Condition, RT, Trial, and ACC and remove Response, TrialProc, TargetArrowDirection, SessionDate, and SessionTime. select() is actually quite versatile - you can remove columns by specifying certain patterns. I will only cover a couple here, but to learn more Visit the select() webpage We could just simply select all the columns we want to keep data &lt;- select(data, Subject, Condition, RT, Trial, Accuracy) alternatively we can specify which columns we want to remove by placing a - in front of the columns data &lt;- select(data, -Response, -TrialProc, -TargetArrowDirection, -SessionDate, -SessionTime) or we can remove (or keep) columns based on a pattern. For instance SessionDate and SessionTime both start with Session data &lt;- select(data, -Response, -TrialProc, -TargetArrowDirection, -starts_with(&quot;Session&quot;)) You might start realizing that there is always more than one way to perform the same operation. It is good to be aware of all the ways you can use a function because there might be certain scenarios where it is better or even required to use one method over another. In this example, you only need to know the most straightfoward method of simply selecting which columns to keep. You can also rename variables as you select() them… let’s change Accuracy back to ACC… just beacuse we are crazy! data &lt;- select(data, Subject, Condition, RT, Trial, ACC = Accuracy) We are keeping Subject, Condition, RT, Trial, and renaming ACC to Accuracy. 4.2.4 mutate() mutate() is a very powerful function. It basically allows you to do any computation or transformation on the values in the dataframe. You can create new columns based on transformations of other columns or simply change the values in already existing columns. Let’s see an example of both. 4.2.4.1 Changing values in an existing column Reaction times that are less than 200 milliseconds most likely do not reflect actual processing of the task. Therefore, it would be a good idea to not include these when calculating mean and standard deviations. We could use filter() to do this - and that might be the better way to do it but for the sake of the tutorial let’s use mutate(). What we are going to do is is set any RTs that are less than 200 milliseconds to missing, NA. First let’s make sure we even have trials that are less than 200 milliseconds. Two ways to do this. 1) View the dataframe and click on the RT column to sort by RT. You can see there are RTs that are as small as 1 millisecond! Oh my, that is definitely not a real reaction time. 2) you can just evaluate the minimum value in the RT column: min(data$RT) ## [1] 3 Now lets mutate() data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT)) Since we are replacing values in an already existing column we can just specify that column name, RT = followed by the transformation. Here we need to specify an if…then… else statment. To do so within the mutate() function we use the function called ifelse(). ifelse() evaluates a logical statement specified in the first argument, RT&lt;200. mutate() works on a row-by-row basis. So for each row it will evaluate whether RT is less than 200. If this logical statement is TRUE then it will perform the next agrument, in this case sets RT = NA. If the logical statement is false then it will perform the last argument, in this case sets RT = RT (leaves the value unchanged). 4.2.4.2 Creating a new column Let’s say for whatever reason we want to calculate the difference between the RT on a trial minus the overall grand mean RT (for now, accross all subjects and all trials). This is not necessary for what we want in the end but what the heck, let’s be a little crazy. (I just need a good example to illustrate what mutate() can do.) So first we will want to calculate a “grand” mean RT. We can use the mean() function to calculate a mean. mean(data$RT, na.rm = TRUE) ## [1] 585.7716 Since we replaced some of the RT values with NA we need to make sure we specify in the mean() function to remove NAs by setting na.rm = TRUE. We can use the mean() function inside of a mutate() function. Let’s put this “grand” mean in a column labeled grandRT. First take note of how many columns there are in data ncol(data) ## [1] 5 So after calculating the grandRT we should expect there to be one additional column for a total of 6 columns data &lt;- mutate(data, grandRT = mean(RT, na.rm=TRUE)) Cool! Now let’s calculate another column that is the difference between RT and grandRT. data &lt;- mutate(data, RTdiff = RT - grandRT) We can put all these mutate()s into one mutate() data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) Notice how I put each one on a seperate line. This is just for ease of reading and so the line doesn’t extend too far off the page. Just make sure the commas are still there at the end of each line. 4.2.5 group_by() This function is very handy if we want to perform functions seperately on different groups or splits of the dataframe. For instance, maybe instead of calculating an overall “grand” mean we want to calculate a “grand” mean for each Subject seperately. Instead of manually breaking the dataframe up by Subject, the group_by() function does this automatically in the background. Like this… data &lt;- group_by(data, Subject) data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) You will now notice that each subject has a different grandRT, simply because we specified group_by(data, Subject). Let’s say we want to do it not just grouped by Subject, but also Condition. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) group_by() does not only work on mutate() - it will work on any other functions you specify after group_by(). I suggest exercising caution when using group_by() because the grouping will be maintained until you specify a different group_by() or until you ungroup it using ungroup(). So I always like to ungroup() immediately after I am done with it. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) data &lt;- ungroup(data) 4.2.6 summarise() The summarise() function will reduce a data frame by summarising values in one or multiple columns. The values will be summarised on some statistical value, such as a mean, median, or standard deviation. Remember that in order to calculate the Flanker Effect for each subject, we first need to calculate each subject’s mean RT on incongruent trials and their mean RT on congruent trials We’ve done our filtering, selecting, mutating, now let’s aggergate RTs accross Condition to calculate mean and standard deviations. We will use a combo of group_by() and summarise(). summarise() is almost always used in conjunction with group_by(). data &lt;- group_by(data, Subject, Condition) data &lt;- summarise(data, RT.mean = mean(RT, na.rm = TRUE)) %&gt;% ungroup() To summarise() you need to create new column names that will contain the aggregate values. RT.mean seems to make sense to me. What does the resulting data frame look like? There should be three rows per subject, one for incongruent trials, one for congruent trials, and one for neutral trials. You can see that we now have mean RTs on all conditions for each subject. Also, notice how non-grouped by columns got removed: Trial, and ACC. 4.2.7 spread() Ultimately, we want to have one row per subject and to calculate the difference in mean RT between incongruent and congruent conditions. It is easier to calculate the difference between two values when they are in the same row. Currently, the mean RT for each condition is on a different row. What we need to do is reshape the dataframe. To do so we will use the spread() function from the tidyr package. The tidyr package, like readr and dplyr, is from the tidyverse set of packages. The spread() function will convert a long data frame to a wide dataframe. In other words, it will spread values on different rows to being on the same row but different columns. In our example, what we want to do is spread() the mean RT values for the two conditions across different columns. So we will end up with is one row per subject and one column for each condition. Rather than incongruent, congruent, and neutral trials being represented across rows we are spreading them across columns (widening the data frame). The two main arguments to specify in spread() are key: The column name that contains the variables to create new columns by (e.g. “Condition”) value: The colunn name that contains the values (e.g. “RT”) First of all if you have not done so, install the tidyr package install.package(&quot;tidyr&quot;) library(tidyr) data &lt;- spread(data, key = Condition, value = RT.mean) View the dataframe. There are now three columns for each condition that contain the RT.mean values. From here it is pretty easy, we just need to create a new column that is the difference between incongruent and congruent columns. We can use the mutate() function to do this data &lt;- mutate(data, FlankerEffect = incongruent - congruent) Perfect! Using the readr, dplyr, and tidyr packages we have gone from a “tidy” raw data file (one-row per trial) to a dataframe with one row per subject and a column of FlankerEffect scores. 4.2.8 Pipe Operator %&gt;% One last thing about the dplyr package. dplyr allows for passing the output from one function to another using what is called a pipe operatior. The pipe operator is: %&gt;% This makes code more concise, easier to read, and easier to edit. When you pass the output of one function to another with %&gt;% you do not need to specify the dataframe (input) on the next function. %&gt;% implies that the input is the output from the previous funciton, so this is made implicit. We can pipe all the functions in the chapter together as such library(readr) library(dplyr) import &lt;- read_delim(&quot;data/Flanker_raw.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) data &lt;- import %&gt;% rename(Accuracy = ACC) %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% select(Subject, Condition, RT, Trial, ACC = Accuracy) %&gt;% group_by(Subject, Condition) %&gt;% mutate(RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE)) %&gt;% ungroup() %&gt;% spread(key = Condition, value = RT.mean) 4.3 readr - export The point of an R script is not to create a dataframe in your Environment. The purpose is to 1) import a file (readr), 2) do some stuff to it (dplyr), then 3) save a file to your computer (readr). You have done the first two steps. Now, you need to save this created dataframe data to a file. Again we will utilize the readr package. write_delim(data, path = &quot;data/Flanker_Scored.txt&quot;, delim = &quot;\\t&quot;, na = &quot;&quot;) This is the standard function and arguments I will use when saving to a file. The first argument is the object or dataframe that you want to save. path is the entire file path to save to, including filename and extension. delim is how you want to delimit the text file. As a lab, we have a standard to save files in a tab delimited format, &quot;\\t&quot; na is what value to use for missing NA values. As a lab, we have a standard to use blanks &quot;&quot; for missing values. This is not always necessary but I put it in just to be safe. Alternatively, you might like to use SPSS to do statistical analyses. If your dataframe is ready to be imported to SPSS for statistical analyses then you can save the file as a .sav file using the haven package. Go ahead and install haven install.packages(&quot;haven&quot;) library(haven) To save as .sav is sipmle write_sav(data, path = &quot;data/Flanker_Scored.sav&quot;) Virtually all the R scripts you write will require the dplyr package. The more you know what it can do, the easier it will be for you to write R Scripts. I highly suggest checking out these introductions to dplyr. https://dplyr.tidyverse.org https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html Cool! Before moving on to writing more R scripts, we need to talk about organization and workflow "],
["englelab-packages.html", "Chapter 5 EngleLab Packages 5.1 englelab 5.2 datawrangling", " Chapter 5 EngleLab Packages I have created R packages that contain functions to more easily prepare your data for statistical analysis. In this chapter you will learn about the functions from two packages I created: englelab https://github.com/EngleLab/englelab datawrangling https://dr-jt.github.io/datawrangling I am hosting these packages on GitHub and can be downloaded using the devtools package install.packages(&quot;devtools&quot;) devtools::install_github(&quot;EngleLab/englelab&quot;) devtools::install_github(&quot;dr-JT/datawrangling&quot;) 5.1 englelab The functions in the englelab package are to create “tidy” raw data files and scored data files from the complex span and fluid intelligence tasks we frequently use. It also contains a function to calculate scores using the binning method from [insert citation here]. There are also functions to calcualte the reliability measures; cronbach’s alpha and split-half reliability. This package is intended to eventually share with other researchers who download the tasks from our website and use R to do data analysis. 5.1.1 “tidy” raw data functions I suggest to only use the “tidy” raw functions rather than the scoring functions. The reason for this is that the scoring function does not create a “tidy” raw data file - which is useful for doing internal consitency analyses, and is just a good idea to have a “tidy” raw data file for trial-by-trial performance. Here is the list of “tidy” raw data functions: raw_ospan() raw_symspan() raw_rotspan() raw_rapm() raw_numberseries() raw_lettersets() These functions take as input an imported E-Merged or edat file. For the complex span tasks you need to specify the number of blocks administered with the blocks argument. ex. raw_ospan(data, blocks = 2) The output of the raw functions will also contain columns for the subject’s final score on the task. This is why it is suggested to just use the raw functions, you can still easily grab the final score on the task from the outputed file. 5.1.1.1 Example Here is an example script that uses the raw_ospan() function to import an E-merged data file and output a “tidy” raw data file. ## Set up #### ## Load packages library(readr) library(englelab) ## Set import/output directories import.file &lt;- &quot;data/raw/emerge/ospan.txt&quot; output.file &lt;- &quot;data/raw/ospan_raw.txt&quot; ############## ## Import import &lt;- read_delim(import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Clean up raw data file and save data_raw &lt;- raw_ospan(import, blocks = 2) ## Save data file write_delim(data_raw, path = output.file, &quot;\\t&quot;, na = &quot;&quot;) 5.1.2 score data functions Again, it is suggested to use the raw functions instead of the score functions. However, here is the list of the score functions: score_ospan() score_symspan() score_rotspan() score_rapm() score_numberseries() score_lettersets() Like the raw functions the score functions take as input a “messy” raw E-Merged or edat file. For the complex span tasks you need to specify the number of blocks adminestered. 5.1.3 Calculating Bin Scores The bin_score() function will calculate bin scores. These are the arguments you will need to specify: x: a dataframe with trial level data. Needs to have RT and Accuracy DVs rt.col: Column name in dataframe that contains the reaction time data accuracy.col: Column name in dataframe that contains the accuracy data condition.col: Column name in dataframe that contains the trial condition values baseline.condition: The values that specify the baseline condition (e.g. “congruent”) type: How should Bin trials be aggregated, “sum” or “mean” (Default: “mean”) id: Column name in dataframe that contains the subject identifiers The default argument values are: bin_score(x, rt.col = &quot;RT&quot;, accuracy.col = &quot;Accuracy&quot;, condition.col = &quot;Condition&quot;, baseline.condition = &quot;congruent&quot;, type = &quot;mean&quot;, id = &quot;Subject&quot;) Your data file may already be setup with these default value column names. If so, then you just need to specify the dataframe. 5.1.4 Reliability functions 5.1.4.1 cronbach_alpha() This function takes as input a “tidy” raw trial-level dataframe. x: x a dataframe with trial level data trial.col: The column name that identifies trial number value: The column name that identifies the values to be used id: The column name that identifies the Subject IDs. Cronbach’s alpha is calculated using the alpha() function from the psych package. The difficulty in simply using the alpha() function is getting the dataframe in the correct structure. To use alpha() the values that reliability is going to be assessed over need to be in columns. The dataframe, then is one subject per row with a column for each value. For most of our tasks, the values that will be assessed over are the individual Trial level DV (RT or Accuracy). So there needs to be one column for each Trial. This is an unusual data structure and is really only useful for calculating reliability. cronbach_alpha() will save you time by creating the correct data structure for you based on a more common structure that is contained in your “tidy” raw data files (one row per trial per subject, with RT and Accuracy as columns). You should be able to take your “tidy” raw data as input to cronbach_alpha(). The output of cronbach_alpha() is a single value representing Cronbach’s Alpha. 5.1.4.2 splithalf() This function takes as input a “tidy” raw trial-level dataframe. x: x a dataframe with trial level data trial.col: The column name that identifies trial number value: The column name that identifies the values to be used id: The column name that identifies the Subject IDs. The default values are splithalf(data, trial.col = &quot;Trial&quot;, value = NULL, id = &quot;Subject&quot;) The data is split in half by even and odd trials. You should be able to take your “tidy” raw data as input to splithalf(). The output of splithalf() is a single value representing split-half reliability. 5.2 datawrangling It would take too long to cover each of the functions in this package one-by-one. I will cover just a few that are the most commonly used functions. For a descriptions of each function see https://dr-jt.github.io/datawrangling/reference/index.html 5.2.1 Merging Data Files You might find yourself in a situation where you need to merge multiple text files together. There are two types of merge operations that can be performed. In R, a “join” is merging dataframes together that have at least some rows in common (e.g. Same Subject IDs) and have at least one column that is different. The rows that are common serve as the reference for how to “join” the dataframes together. In R, a “bind” is combining datarames together by staking either the rows or columns. It is unlikely that we you will need to do a column bind so we can skip that. A row “bind” takes dataframes that have the same columns but different rows. This will happen if you have separate data files for each subject from the same task. Each subject data file will have their unique rows (subject by trial level data) but they will all have the same columns. The E-Merge software program is performing a row “bind” of each subject .edat file. In E-Prime 2 we have to go through E-Merge to do this process. However, in E-Prime 3.0 there is the option to output an exported .edat file as a tab-delimited .txt file. Using the files_bind() function from the datawrangling package will allow us to skip the E-Merge step. The datawrangling package contains two functions to merge data files together: files_join() files_bind() They both work in a similar way. The files you want to merge need to be in the same folder on your computer. You specify the location of this folder using the path = argument. You need to specify a pattern that uniquely identifies the files you want to merge (e.g. “.txt”, or “Flanker”) using the pattern = argument. Then specify the directory and filename you want to save the merge file to using the output.file = argument. Here are the arguments that can be specified: path: Folder location of files to be merged pattern: Pattern to identify files to be merged delim: Delimiter used in files. Passed onto readr::read_delim() na: How are missing values defined in files to be merged. Passed to readr::write_delim() output.file: File name and path to be saved to. id: Subject ID column name. Passed onto plyr::join_all(by = id). ONLY for files_join() bind: The type of bind to perform (default = “rows”). ONLY for files_bind() 5.2.2 Reshpaing Data To run a statistical test usually requires your data is in the right format. For instance, to run a correlation or regression, you should have only one subject per row, and the columns contain values of each variable. The “messy” or “tidy” raw trial level data you start with are not in this format and so it is likely you will have to reshape your data to prepare it for statistical analyses. In Chapter 4 you saw how to do this using the spread() function from the tidyr package. One limitation of spread() is that you can only spread on one key column or value column at a time. The reshape_spread() function allows you to spred() on multiple key columns or value colunns. If you are spreading on multiple key columns you need to specify how you want the names of those values to be merged. The arguments you can specify for reshape_spread() are: x: dataframe variables: The variable used for spreading variables_combine.name: If using more than one variables column then specify name of a new combined column variables_combine.sep: if using more than one variables column then specifiy how values should be seperated when combine values: A vector of columns that contain the values to be spread on id: What column is not being reorganized and needs to be preserved. Usually “Subject” fill: Passed to spread() fill parameter For example, if you had a dataframe two rows per subject containing mean RT and mean Accuracy on congruent and incongruent trials, and you want to spread on both RT and Accuracy by Condition. data &lt;- reshape_spread(data, variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;), id = &quot;Subject&quot;) 5.2.3 Transformations There are a set of function is datawrangling to allow you to more easily transform column values into new variables and to do data cleaning. 5.2.3.1 Create Composites The function composite() will create composite scores out of specified columns. Right now you can only create “mean” composite scores. In the future I plan on adding “sum” and “factor score” composite types. Here is a list of the arguments you can specifiy: x: dataframe variables: c() of columns to create the composite from type: What type of composite should be calculated?, i.e. mean or sum. (Default = “mean”). standardize: Logical. Do you want to calculate the composite based on standardized (z-score) values? (Default = TRUE) name: Name of the new composite variable to be created missing.allowed: Criteria for the number of variables that can having missing values and still calculate a composite for that subject Example: library(datawrangling) composite(data, variables = c(&quot;RAPM&quot;, &quot;NumberSeries&quot;, &quot;LetterSets&quot;), type = &quot;mean&quot;, standardize = TRUE, name = &quot;Gf&quot;, missing.allowed = 1) 5.2.3.2 Centering and Z-scoring The function center() will create either unstandardized or standardized (z-scored) centered variables. The list of arguments that can be passed onto the function are: x: dataframe variables: c() of columns to center standardize: Logical. Do you want to calculate zscores? (Default = FALSE) Example: library(datawrangling) center(data, variables = c(&quot;RT&quot;), standardize = TRUE) 5.2.4 Data Cleaning 5.2.4.1 Trimming The function trim() will replace outlier scores that exceed a certain z-score cutoff. There are several options for how to replace the outlier scores. Replace with “NA” (missing value) “cutoff” (the z-score cutoff value, e.g. 3.5 SDs) “mean” “median” The arguments that can be specified are: x: dataframe variables: c() of variables to be trimmed. option to set variables = &quot;all&quot; to trim all variables in a dataframe. But then must specify id = cutoff: z-score cutoff to use for trimming (default: 3.5) replace: What value should the outlier values be replaced with. (default: replace = “NA”) id: Column name that contains subject IDs. **ONLY needs to be used if variables = &quot;all&quot; Example: library(datawrangling) trim(data, variables = &quot;RT&quot;, cutoff = 3.5 replace = &quot;cutoff&quot;) 5.2.4.2 Latent variable criteria The function remove_latent() will remove subjects who have missing data on too many tasks for a construct(s). The factor structure needs to be specified in the factor.list argument. For example, factor.list = list(WMC = c(&quot;OSpan&quot;, &quot;SymSpan&quot;, &quot;RotSpan&quot;), Gf = c(&quot;RAPM&quot;, &quot;LetterSets&quot;, &quot;NumberSeries&quot;)) The arguments that can be specified are: x: dataframe factor.list: list of factors and tasks missing.allowed: Proportion of tasks allowed to be missing id: Subject ID variable output.dir: File directory to save removed subjects to [OPTIONAL] output.file: File name to save removed subjects to [OPTIONAL] Example: library(datawrangling) remove_latent(data, factor.list = list(WMC = c(&quot;OSpan&quot;, &quot;SymSpan&quot;, &quot;RotSpan&quot;), Gf = c(&quot;RAPM&quot;, &quot;LetterSets&quot;, &quot;NumberSeries&quot;)), missing.allowed = .5, id = &quot;Subject&quot;) Now on to scoring the “tidy” raw data file "],
["organization-and-good-practices.html", "Chapter 6 Organization and Good Practices 6.1 R Scripts 6.2 Working Directory 6.3 The masterscript 6.4 Folder Organization 6.5 Naming R Scripts 6.6 Summary", " Chapter 6 Organization and Good Practices Having organziation and implementing “good” practices will make working with and executing your R scripts much easier. This is the workflow process of getting from “messy” raw data files to beautiful looking output of figures and statistical analyses. Steps 1 and 2 will use a very similar workflow process and be implemented in R Scripts, which have the file extension .R. Step 3 will actually use what is called an R Markdown document, which has the file extension .Rmd. 6.1 R Scripts Steps 1 and 2 are all about transforming data files to create a final output file that is ready for statistical analysis. 6.1.1 General Workflow Every R script you create for steps 1 and 2 import a file -&gt; do stuff to the dataframe -&gt; output a saved file, no more, and no less. The general workdflow in every script will look like Load required packages using library() Import data file using read_delim() from the readr package Do stuff to the imported dataframe using dplyr functions, such as filter(), select(), group_by(), mutate(), and summarise() Save dataframe to a file using write_delim() from readr or write_sav() from haven Honestly there is not much more to it then that. And because your R scripts for steps 1 and 2 have the same workflow process this makes it very easy to implement a standard organization in your scripts. 6.1.2 Building a Template Workflow It is good practice to load required packages at the top of your R script. After loading any required packages it is good practice to assign the import and output file path directories to an object. I use the same object name for the import file path and output file path in every R script I write. This does a couple of things: You can easily see what the import and output directories are for an R script since it occurs at the top and in the same location with the same names for every R script. You can use the exact same or nearly the same read_delim() and write_delim() lines of code for every R script. Just copy and paste. This allow me to focus on writing the meat of the R script - the Do stuff step in the general workfolow process. A template R script might look something like ## Set up #### ## Load required packages library(readr) library(dplyr) ## Set import and output directories import.dir &lt;- &quot;data/import&quot; import.file &lt;- &quot;task.txt&quot; output.dir &lt;- &quot;data/output&quot; output.file &lt;- &quot;task_score.txt&quot; ############## ## Import data import &lt;- read_delim(paste(import.dir, import.file, sep=&quot;/&quot;), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Do Stuff data &lt;- import %&gt;% filter() %&gt;% group_by() %&gt;% mutate() %&gt;% select() ## Save data write_delim(data, path = paste(ouptut.dir, output.file, sep = &quot;/&quot;), delim = &quot;\\t&quot;, na = &quot;&quot;) rm(list=ls()) Notice how I use comments to help organize the R script. # is how you can insert comments in the script. You can even have a hierchical structure in your comments where it will allow you to “fold” chunks of code. This is helpful if your R script is getting really long and you would like to temporarily hide chunks of code. To do this you need to have a certain number of # at the top of the code chunk. See how I use four #### after the ## Set up comment. Four #s is usually enough. Then at the end of the code chunk I place a bunch more #########. You can put as many as you want, but again usually 4 is enough. I like to make it the same length as the the top of the code chunk. 6.1.3 rm(list=ls()) It is good practice to clear out any objects in the Enviroment window before or after running a script. To do this you can use rm(list=ls()). Use this as either the first or last line of code in every R Script. Every R script you write can have this same layout That is, everything on the “left” side (of the assignment operator &lt;-) can stay the exactly same. Whereas what happens on the “right” side depends on what data file you are working with. But even what is happening on the “right” side is similar and may only require the readr and dplyr packages. My actual template R script looks slightly different but I have to first explain working directories and more good practices to use when writing scripts in R. 6.2 Working Directory We need to go over the idea of a “working directory”. In R, and many other programming languages, a “working directory” is a point of reference that can be used to create “relative” file paths. Instead of having to specify the entire file path to a directory (“User/Documents/DropBox/Projects/Study 1/Data Files/Raw Data”) you can use a “relative” file path starting from the “working directory”. This is useful because it allows scripts to be reproducible across different systems, computers, and users. Every computer has different absolute file paths. Therefore, it is essential to use only relative file paths. In this examlple, if the “working directory” was the Study 1 folder - &quot;User/Documents/DropBox/Projects/Study 1/Data Files/Raw Data&quot; - then a relative file path might look like &quot;Data Files/Raw Data&quot; If you need to go back one folder - into the “Projects” folder - you tpye two dots &quot;../Study 2/Data Files/Raw Data&quot; would take you to &quot;User/Documents/DropBox/Projects/Study 2/Data Files/Raw Data&quot; You can set the working directoy either using the R Studio GUI or using the function setwd(). But… DO NOT DO THIS! This is not a good workflow practice. Especially if you use the R Studio GUI (which I won’t even tell you how to do). But even setwd() is a bad idea. The reason boils down to: You want to avoid any manual steps in your workflow process (using the GUI is a manual process) Absolute file paths are computer and user specific. When you use setwd() you need to use an absolute file path. This makes your script not reproducible. And reproducibility is one of the main advantages to learning R in the first place, so don’t undermine yourself! Okay okay… well what are good practices to set a working directory then? There are two good practices that I know of and they are best used in combination. R Projects here() 6.2.1 R Projects Use R Projects R Projects are a great way to have the working directory be automatically set. Visit this page for more information on how to use R Projects. Basically an R Project allows you to open a fresh session of R that automatically sets the working directory to the directory where the R Project is saved. R Projects have the file extension .Rproj. Go ahead and create an R Project for this tutorial. Navigate to File -&gt; New Project… -&gt; Choose Existing Directory if you already created an R Tutorial folder on your computer or New Directory -&gt; New Project if you have not. Create the R Project in the root folder of your R Tutorial. Now exit out of R Studio. Then open R Studio by opening the R Project. You can evaluate what the working directory is by getwd(). In your R Scripts you can now simply use relative paths from the working directory. And it doesn’t matter what directory you save your R scripts to. As long as you open the R Studio from the .Rproj file and not one of the script .R files. Try exiting out of R Studio and then opening R Studio by opening one of your .R script files that are not saved in the same location as your .Rproj file. Now evaluate the working directory getwd(). Oops… you see the working directory is no longer where the .Rproj file is saved. This is where the here package comes in handy. 6.2.2 here() Use the here() function from the here package. This method is simple to use and is a great way to specify relative paths. It will allow you to open R Studio using any of your .R script files and maintain the some working directory as your .Rproj file. First go ahead and install the here package, install.packages(&quot;here&quot;). For a passionate ode to the here package see: https://github.com/jennybc/here_here Basically, when here is loaded, library(here), it will search for one of two files to locate the working directory. 1) a hidden .here file or 2) an .Rproj file. It will recursively keep going backwards in directories to locate a root directory that contains one of these files. The .here file can be created by set_here(), but is unnecessary if you are using the R Project method. What this allows is pretty cool. Let’s say your working directory is &quot;R Tutorial&quot;, which is the directory where your .Rproj file is saved. Even if you have an .R script saved in &quot;R Tutorial/A folder/R scripts/scriptexample.R&quot;, you can open a new fresh session of R by opening the file. If you load library(here) at the top of the scriptexample.R file then it will automatically set the working directory to &quot;R Tutorial&quot; since it will detect an .Rproj file in that directory. Then tomorrow maybe you want to work on a different .R script located somewhere else but still within this same project. Let’s say it is located at &quot;R Tutorial/B folder/something dumb/scripts/thisisascript.R&quot;. Again, you can open a new fresh session of R by opening this file AND still have the same working directory set when you load library(here). Then to reference a relative file path you can use here(). For instance here(&quot;B folder&quot;, &quot;somethingdumb&quot;, &quot;scripts&quot;, &quot;thisisascript.R&quot;) # Or here(&quot;B folder/somethingdumb/scripts&quot;, &quot;thisisascript.R&quot;) Bothe of these will give you the same relative file path &quot;B folder/somethingdumb/scripts/thisisascript.R&quot; I think this is pretty amazing because, again, it allows you to focus on the meat of your script rather than fidling around with file paths and working directories. You don’t have to remember to open the .Rproj every time you want to work on writing a .R script. You can sipmly open the .R script and it just works. This also makes it much easier to share projects with others or to collaborate on R scripts for a project with your colleagues. Using here() we can slightly modify the R script template from above to look like ## Set up #### ## Load required packages library(readr) library(dplyr) ## Set import and output directories import.dir &lt;- &quot;data/import&quot; import.file &lt;- &quot;task.txt&quot; output.dir &lt;- &quot;data/output&quot; output.dir &lt;- &quot;task_score.txt&quot; ############## ## Import data import &lt;- read_delim(here(import.dir, import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Do Stuff data &lt;- import %&gt;% filter() %&gt;% group_by() %&gt;% mutate() %&gt;% select() ## Save data write_delim(data, path = here(ouptut.dir, output.file), delim = &quot;\\t&quot;, na = &quot;&quot;) rm(list=ls()) Rather than using paste() you can simply use here(). And if you are using the R Project method, then it will automatically set the working directory based on where .Rproj is saved. Go ahead and set up an R Tutorial .Rproj file and modify your R script from the previous chapter to implement here(). Anywhere you specify a filepath you should be using the here() function. here(&quot;file/path&quot;, &quot;filename&quot;). 6.3 The masterscript I like to use a masterscript that runs multiple R scripts one after another. I started doing this just to keep my sanity. In our lab we often times have many many tasks that we need to analyze data from. Well it can get a little crazy trying to keep all the R scripts organized and running the scripts in the correct order. Using a masterscript can help with this. The source() function will execute all the lines of code in an R script. You simply specify the file path of the R script, such as source(&quot;scripts/A_script.R&quot;). Your masterscript might simply contain only lines of code using source(). I like to also create an object called directories that contains a list of the relative file paths (from the working directory). A typical masterscript of mine will look like ## Setup #### ## Load Packages library(here) library(rmarkdown) ## Specify the directory tree directories &lt;- list(scripts = &quot;Data Files/R Scripts&quot;, data = &quot;Data Files&quot;, raw = &quot;Data Files/Raw Data&quot;, emerge = &quot;Data Files/Raw Data/E-Merge&quot;, scored = &quot;Data Files/Scored Data&quot;) saveRDS(directories, here(&quot;directories.rds&quot;)) ############# ################################# #------ &quot;messy&quot; to &quot;tidy&quot; ------# ################################# ## Create raw data file from e-merged files #### source(here(&quot;Data Files/R Scripts&quot;, &quot;1_wmc_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_gf_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_gda_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_antisaccade_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_avc_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_flanker_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_flankerDL_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_flankerPR_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_stroop_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_stroopDL_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_va4_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_pvt_raw.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;1_sact_raw.R&quot;), echo=TRUE) ################################################ ################################# #------ Task Scoring ------# ################################# ## Score task data from raw data files #### source(here(&quot;Data Files/R Scripts&quot;, &quot;2_wmc_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_gf_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_gda_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_antisaccade_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_avc_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_flanker_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_flankerDL_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_flankerPR_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_stroop_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_stroopDL_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_va4_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_pvt_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_sact_score.R&quot;), echo=TRUE) source(here(&quot;Data Files/R Scripts&quot;, &quot;2_demographics_score.R&quot;), echo=TRUE) ########################################### ## Merge scored files to create a single data file #### source(here(&quot;Data Files/R Scripts&quot;, &quot;3_merge.R&quot;), echo=TRUE) ################################################### rm(list=ls()) 6.4 Folder Organization I suggest adopting a consistent folder organization for all your research projects. Again, this is just about allowing you to focus on the meat of your R scripts. This is my organization: Working Directory Data Files Raw Data E-Merge Scored Data R Scripts Results In the Data Files/Raw Data/E-Merge folder are the “messy” raw data files In the Data Files/Raw Data folder are the “tidy” raw data files In the Data Files/Scored Data folder are the scored data files In the Results folder are the outpued results In the R Scripts folder are ALL the R Scripts that are used You can see how this organization corresponds to the data processing workflow I introduced earlier 6.5 Naming R Scripts If you have a lot of R Scripts for a project it can make it easier to use a certain naming convention to organize the scripts. First of all, I definitely reccomend putting all your scripts into one folder. There is nothing more annoying then having to search all of your computer for the script you are looking for. I personally like to name my R scripts with a number prefix folowed by an underscore and end it with the name of the data processing step it belongs to (i.e. 1_taskname_raw.R). The numbered prefix denotes what step in the data processing procedure given the organization of the masterscript. This makes it SO MUCH easier to search for the script you need to work on. For instance, my R Script directory might look like Where all the scripts that create “tidy” raw data files have the prefix 1 and the suffix _raw whereas the scripts for scoring data files has the prefix 2 and the suffix _score. The prefix number will order the scripts by their data processing workflow step. 6.6 Summary Every R Script should import a file -&gt; Do stuff to the dataframe -&gt; output a saved file. No more, no less than that. At the top of your R script: Load required packages with library() Set the import and output directories (and optionally import and output filenames) Clear out all objects in the Environment using rm(list=ls()) Make your R Scripts reproducible by: Avoid using manual steps Use only relative file paths Using R Projects Using here() Create your own template or use my template workflow (See Chapter 11) Use a masterscript Put all R Scripts for a project in one file Create naming conventions that match the data processing steps in the masterscript In the next two chapters we will go over an example of writing R Scripts for steps 1 and 2 of the general data processing workflow. If you have not doen so already, you should create an .Rproj and organize your folders. If you would like to use a similar workflow in your R Scripts as I do, you can check out the R Script templates I have created for the Engle lab. For more information on the templates and how they work see Chapter 11. "],
["example-data-preparation.html", "Chapter 7 Example: Data Preparation 7.1 Initial Setup 7.2 Masterscript 7.3 Creating “messy” Merged File 7.4 “messy” to “tidy” 7.5 “tidy” raw to scored 7.6 source() in Masterscript", " Chapter 7 Example: Data Preparation Chapters 4 through 6 should have provided you all you need to know in order to start writing scripts to get your data ready for statistical analysis. The data preparation steps include 1) converting “messy” raw data to “tidy” raw data and 2) creating data file with task scores that have gone through data cleaning. Based on the information from Chapters 4 through 6. This Chapter will go over step-by-step how to create R scripts to do data preparation using an example data set from the Flanker task. 7.1 Initial Setup 7.1.1 Set up directory structure First let’s setup our directory organization. In the directory where your .Rproj file is located, you should create the following folders: R Scripts - A folder to put all your scripts in one place Data Files - A folder where any data files will be stored Results - A folder where any outputed results and figures will be stored When conducting a study you might have other directores such as “Tasks” where the task files are located, or “Methods” where any methods documents or materials are located. The Results directory will become relevant later when we get into performing statistical analyses on data. Within the Data Files directory you should create the following folders: Raw Data - A folder containing raw data files Scored Data - A folder containig scored data files Within the Raw Data folder you should create the following directory E-Merge - A folder containing E-Merged and exported .txt merged files This structure helps to keep clear where we are importing and outputing data files to in the data workflow process. The “messy” raw data files are located in the E-Merge folder. The “tidy” raw data files are located in the Raw Data folder. The scored task files are located in the Scored Data folder. Our R Scripts should be able to completely (100%) recreate ALL files in the Raw Data and Scored Data files based on what is in the E-Merge folder. 7.2 Masterscript In Chapter 6, you saw an example masterscript. Lets’ go over how to begin creating one. First open a new script window File -&gt; New File -&gt; R Script Save the script as “masterscript_RTutorial.R” to where your .Rproj file is located (the home directory). Alternatively you might prefer to save the script as “0_masterscript_RTutorial.R” in the R Scripts folder. The reason for the 0 will become evident later. At the top of every script I like to have a “Setup” section where I load packages, set import and output directories, and set any other important variables. In the masterscript I like to create a list object that contains the directory structure we created above. And then save that directory tree as an R object (.rds) to the home directory. What this allows is for each script that we are going to create to access the same directory tree by importing the directory tree R object file (.rds). It will not be apparent to you now, but this will save a lot of time when you have many R scripts. For instance: If the folder names have changed, you will not have to update the file paths in each R script, rather you can just do it from the masterscript. Also, if you want to copy and paste the same scripts to use in a different study (that used the same tasks) then, again, you do not have to update each R script to reflect the different folder names for that study. You can just update the masterscript. This allows you to foucs on writing and executing the scripts rather than wasting time on getting the file path names perfect across every script. ## Setup #### library(here) ## Set the directory tree directories &lt;- list(scripts = &quot;R Scripts&quot;, data = &quot;Data Files&quot;, raw = &quot;Data Files/Raw Data&quot;, messy = &quot;Data Files/Raw Data/E-Merge&quot;, scored = &quot;Data Files/Scored Data&quot;, results = &quot;Results&quot;) saveRDS(directories, here(&quot;directories.rds&quot;)) ############# Now if we want to access the E-Merge folder path in an R Script we could do so by directories &lt;- readRDS(here(&quot;directories.rds&quot;)) directories$messy ## [1] &quot;Data Files/Raw Data/E-Merge&quot; For now we are done with the masterscript. We will come back later and add lines of code to run the scripts you will create in these examples. Go ahead and source the masterscript to create the directories object file. 7.3 Creating “messy” Merged File 7.3.1 Flanker Data Set For this tutorial we will use an example data set from the Arrow Flanker task. You can download this example data set here Download Example Flanker Data Unzip the file to the E-Merge folder. One of the files is an E-Merged file, the other is the E-Merged file exported to a tab-delimited .txt file. There is also a folder labled subj. Before we can begin the data preparation procedures, we need to create a “messy” raw data file that is a merge of the individual subject files. Typically, when you are doing a study there are a couple of steps you need to do in order to create this file. You need to create an E-Merged file using the E-Merge software. You need to export that E-Merged file to a .txt file. I’ve already done these two steps for you. Altenratively, in E-Prime 3.0 we will have the option to skip over these two steps completely. In E-Prime 3.0 there is the option to output a .txt version of the .edat file that is created when a subject finishes a task. This means we will be able to directly import the individual subject data files into R without going through E-Prime software. 7.3.1.1 Skipping E-Merge with E-Prime 3.0 The subj folder, in the zip folder you downloaded, contains individual subject .edat files exported as a .txt file. We will cover how to merge these individual files into one Merged file (which has typically been done through E-Merge software). Create a new script and save it as “._study_finish.txt&quot; in the R Scripts folder. The purpose of this script is to get to a single Merged file for all subject. This is a step that is typically only done once or as few times as possible. Because of this will not put it into the masterscript and we do not assign the script name a number (use “.” in place). First create the setup section at the top of the script. We will need the following packages; here, datawrangling, and readr. ## Setup #### library(here) library(datawrangling) library(readr) ## Set import and output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- paste(directories$messy, &quot;subj&quot;, sep = &quot;/&quot;) output.dir &lt;- directories$messy ############# We will use the files_bind() function from the datawrangling package to stack each subject file ontop of one another. ## Merge data files files_bind(path = here(import.dir), pattern = &quot;Flanker&quot;, output.file = here(output.dir, &quot;Flanker.txt&quot;)) This should do it. So your entire script should look something like ## Setup #### library(here) library(datawrangling) library(readr) ## Set import and output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- paste(directories$messy, &quot;subj&quot;, sep = &quot;/&quot;) output.dir &lt;- directories$messy ############# ## Merge data files files_bind(path = here(import.dir), pattern = &quot;Flanker&quot;, output.file = here(output.dir, &quot;Flanker.txt&quot;)) rm(list=ls()) Source and save your script. 7.4 “messy” to “tidy” The next step in the data preparation procedure is to creat a “tidy” raw data file from the “messy” raw data file. 7.4.1 What to Include Creating a “tidy” raw data file is essentialy a process of elimination. Getting rid of columns and rows that have no value. You may also want to rename columns and values. So what should you keep? You want to keep all columns that are essential to the design of the task. This might include a column that specifies the condition for each trial, a column that specifies a feature of the target stimulus, performance variables, and more. As for rows, I suggest keeping both practice and real trials. It is easy to filter out practice trials later. The “tidy” raw data file should contain only one row per trial per subject. You should probably rename variables and values to be easy for those unfamiliar with how the task was programmed to understand. Also if you have similar tasks (Flanker and Stroop) it is probably a good idea to give similar names to variables and values. For instance, give the same name to the column that contains the condition type. Also use the same value names for each condition (not congruent for one task, and cong for another). Use a standard name for columns with reaction time and accuracy values (RT, Accuracy). 7.4.2 Import “messy” data Open a new script file and save it to the R Scripts folder as “1_flanker_raw” The “1” denotes that this is the first step in our workflow process; converting “messy” raw data files to “tidy” raw data files. Again, create the setup section at the top ## Setup #### library(here) library(readr) library(dplyr) library(datawrangling) ## Set import and output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$messy output.dir &lt;- directories$raw ############# Okay, first thing is to import the “messy” merged .txt data file. To do this use the read_delim function from the readr package. ## Import import &lt;- read_delim(here(import.dir, &quot;Flanker.txt&quot;), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) You can then view the data file with View(import) It is a mess, right? Here are some things you need to know about the “messy” raw data file. These are the columns and what type of values they contain: Subject: Subject number Procedure[Trial]: Procedure type (keep: TrialProc and PracTrialProc) PracTrialList.Sample: Trial number for practice trials TrialList.Sample: Trial number for real trials FlankerType: condition for real and practice trials (Values are: congruent, incongruent, and neutral) PracSlideTarget.RT: Reaction time for practice trials PracSlideTarget.ACC: Accuracy for practice trials PracSlideTarget.RESP: Response for practice trials ({LEFTARROW} = left and {RIGHTARROW} = right) SlideTarget.RT: Reaction time for real trials SlideTarget.ACC: Accuracy for real trials SlideTarget.RESP: Response for real trials ({LEFTARROW} = left and {RIGHTARROW} = right) TargerDirection: direction of the target arrow for practice trials TargetDirection: direction of the target arrow for real trials SessionDate: Date of session SessionTime: Time of session 7.4.3 Remove Duplicate Subjects Now it happens on occasion that the wrong subject number is entered in when an RA is starting up a task. This can result in duplicate Subject numbers in the E-Merge file. Luckily I have created a function to remove the duplicate subjects, and put their information (with session date and time) into a specific file. This file will be created in a new folder called “duplicates”. The function is duplicates_remove() from the datawrangling package on my GitHub. It can be difficult to remember what arguments you need to include in a function. To see helpful documentation about a function you can type in the console library(datawrangling) ?duplicates_remove ## Import import &lt;- read_delim(here(import.dir, &quot;Flanker.txt&quot;), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) import &lt;- duplicates_remove(import, taskname = &quot;Flanker&quot;, output.folder = here(output.dir)) 7.4.4 Filter Filter only relevant rows. We want to keep only the rows that contain trials from the practice and real trials. To do this we use the filter() function of the dplyr package. ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) The column names are contained in single quotes because the names contain the special characters [ ]. There are certain characters R does not like to use as variable names and one of them is square brackets. 7.4.5 Rename Now the column specifying real vs practice trials is a little tedious to keep typing out since it requires the single quotes and brackets. We will also want to rename this column to be more coherent in the final “tidy” data format anyways. To rename columns we can use rename() function in dplyr. ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) 7.4.6 Mutate Let’s change the values in TrialProc. Right now real trials have the value of “TrialProc”. The same name as the column, not good! And the “practice” trials have the value of “PractTrialProc”. Let’s simply change these values to “real” and “practice”, respectively. ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = ifelse(TrialProc==&quot;TrialProc&quot;, &quot;real&quot;, &quot;practice&quot;)) Note that we are not being very specific with ifelse(). Rows that do not equal &quot;TrialProc&quot; get set as &quot;practice&quot;. This is okay ONLY because we already applied a filter to only include rows with the value &quot;TrialProc&quot; or with the value &quot;PracTrialProc&quot;. If we did not apply this filter, or if there were more than two values for the column TrialProc, then rows that were not &quot;PracTrialProc&quot; would get set to &quot;practice&quot; as well. The point is, be careful how you are using ifelse(). An alternative to ifelse() that I like is the case_when() function from dplyr. You can be more specific with case_when(). ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc==&quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc==&quot;PracTrialProc&quot; ~ &quot;practice&quot;, TRUE ~ as.character(NA))) In case_when() you still specify a logical argument TrialProc==&quot;TrialProc&quot; and you set what should happen when that argument is TRUE by using the ~ symbol. So, cases when TrialProc==&quot;TrialProc&quot; set ~ value to &quot;real&quot;. At the End of case_when() you need to specify what should happen if none of those cases specified above are TRUE. You do this by typing TRUE ~ followed by what to do. Usually you will want to set the value to missing. The tricky thing here is that NA is a logical value. A column of values can only be of one type (e.g. a column cannot contain both logical and character values). To get around this we just set NA to whatever value the column should take as.character(NA). Okay now let’s move on to figuring out what other columns we want to keep and if we need to do any more computations on them. We want to keep the columns that specify the following information Subject number TrialProc (real vs practice) Trial number Condition (congruent vs incongruent) Reaction time Accuracy Response Target arrow direction (left or right) Session Date Session Time This gets a little more tricky here because the information for some of these variables are in one column for practice trials and a different column for real trials. That means we need to merge the information from these two columns into one. We can do this using themutate() function from the dplyr package. For instance the RT data for practice trials is contained in the column PracSlideTarget.RT and for real trials RT data is in SlideTarget.RT. ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc==&quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc==&quot;PracTrialProc&quot; ~ &quot;practice&quot;, TRUE ~ as.character(NA)), RT = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RT, PracSlideTarget.RT)) So the new column RT gets set to the value that is contained in SlideTarget.RT if it is a real trial, if not then the RT gets a value contained in PracSlideTarget.RT We can do the same thing for trial, accuracy, response, and target arrow direction. Combining them all into one mutate() function ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc==&quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc==&quot;PracTrialProc&quot; ~ &quot;practice&quot;, TRUE ~ as.character(NA)), RT = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RT, PracSlideTarget.RT), Accuracy = ifelse(TrialProc==&quot;real&quot;, SlideTarget.ACC, PracSlideTarget.ACC), TargetArrowDirection = ifelse(TrialProc==&quot;real&quot;, TargetDirection, TargerDirection), Response = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RESP, PracSlideTarget.RESP)) You might want to change the values in the Response and CorrectResponse columns to be more clear (left and right). ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc==&quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc==&quot;PracTrialProc&quot; ~ &quot;practice&quot;, TRUE ~ as.character(NA)), RT = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RT, PracSlideTarget.RT), Accuracy = ifelse(TrialProc==&quot;real&quot;, SlideTarget.ACC, PracSlideTarget.ACC), TargetArrowDirection = ifelse(TrialProc==&quot;real&quot;, TargetDirection, TargerDirection), Response = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RESP, PracSlideTarget.RESP), Response = ifelse(Response==&quot;{LEFTARROW}&quot;, &quot;left&quot;, ifelse(Response==&quot;{RIGHTARROW}&quot;, &quot;right&quot;, NA))) Notice how I included an ifelse() function inside of an ifelse() function. The inner ifelse() will occur if Response does not equal &quot;{LEFTARROW}&quot;. This is another way to be more specific when using ifelse() instead of case_when(). It is up to you which you prefer to use. You have to be careful with ifelse() statements because sometimes it does something you do not expect it to. That is why it is always important to check to make sure your code is performing as you want it to. View the dataframe to make sure everything is good. The new columns will be added at the end of the dataframe. 7.4.7 Select We are getting closer to a “tidy” raw data file. The only thing left is to select the columns we want to keep. We do this by using the select() function from the dplyr package. Remember we want to only select columns with the following information Subject number Trial number Condition Reaction time Accuracy Response Correct Response Target arrow direction Session Date Session Time ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc==&quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc==&quot;PracTrialProc&quot; ~ &quot;practice&quot;, TRUE ~ as.character(NA)), RT = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RT, PracSlideTarget.RT), Accuracy = ifelse(TrialProc==&quot;real&quot;, SlideTarget.ACC, PracSlideTarget.ACC), TargetArrowDirection = ifelse(TrialProc==&quot;real&quot;, TargetDirection, TargerDirection), Response = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RESP, PracSlideTarget.RESP), Response = ifelse(Response==&quot;{LEFTARROW}&quot;, &quot;left&quot;, ifelse(Response==&quot;{RIGHTARROW}&quot;, &quot;right&quot;, NA))) %&gt;% select(Subject, TrialProc, Trial, Condition = FlankerType, RT, Accuracy, Response, TargetArrowDirection, SessionDate, SessionTime) 7.4.8 Save to File The function of an R script is to import a dataframe -&gt; transform or analyze the dataframe -&gt; output a final product (a new dataframe or analysis output). The objects which an R script creates (which you can see in the Environemnt window) are not the final end point. These are just temporary objects that are used to go from an input -&gt; output. You have done the importing and transforming; now you need to output the final product - which is a saved .txt file of the “tidy” raw data. To save the dataframe to a .txt file you will use the write_delim() function of the readr package. ## Save write_delim(data_flanker, path = here(output.dir, &quot;Flanker_raw.txt&quot;), delim = &quot;\\t&quot;, na = &quot;&quot;) Then if we were to put it all together, using the template from the previous chapter: ## Setup #### library(here) library(readr) library(dplyr) library(datawrangling) ## Set import and output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$messy output.dir &lt;- directories$raw ############# ## Import import &lt;- read_delim(here(import.dir, &quot;Flanker.txt&quot;), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) %&gt;% duplicates_remove(taskname = &quot;Flanker&quot;, output.folder = here(output.dir)) ## Tidy data_flanker &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc==&quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc==&quot;PracTrialProc&quot; ~ &quot;practice&quot;, TRUE ~ as.character(NA)), RT = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RT, PracSlideTarget.RT), Accuracy = ifelse(TrialProc==&quot;real&quot;, SlideTarget.ACC, PracSlideTarget.ACC), TargetArrowDirection = ifelse(TrialProc==&quot;real&quot;, TargetDirection, TargerDirection), Response = ifelse(TrialProc==&quot;real&quot;, SlideTarget.RESP, PracSlideTarget.RESP), Response = ifelse(Response==&quot;{LEFTARROW}&quot;, &quot;left&quot;, ifelse(Response==&quot;{RIGHTARROW}&quot;, &quot;right&quot;, NA))) %&gt;% select(Subject, TrialProc, Trial, Condition = FlankerType, RT, Accuracy, Response, TargetArrowDirection, SessionDate, SessionTime) ## Save write_delim(data_flanker, path = here(output.dir, &quot;Flanker_raw.txt&quot;), delim = &quot;\\t&quot;, na = &quot;&quot;) rm(list=ls()) 7.5 “tidy” raw to scored Great! You have written an R script for the first step in Data Preparation, converting a “messy” raw data file to a “tidy” raw data file. Next we will go over how to write an R script for the second step - which involves data cleaning and task scoring. This step is more complicated and often times requires some forethought. But we don’t always have the best forethought so you will likely re-write previous lines of code. One thing you must think about before writing the script for this stage is the statistical analyses you eventually plan on conducting. This is because the final resulting dataframe will depend on what analyses you do. The data structure required for conducting a between-subject mean comparison (t-tests or ANOVA) will be different from the data structure required for a regression. The type of statistical analyses you plan on conducting will determine the final dataframe you want to end up at in this stage of data preparation. Another thing you must think about before hand are the final dependent variables (or task scores) you want to calaculate. For instance, in the Flanker task there are several task scores we might want to calculate (in a regression context). FlankerEffect on RT: Mean reaction time difference between incongruent and congruent trials FlankerEffect on Accuracy: Mean accuracy difference between incongruent and congruent trials Flanker Binned Scores: A scoring method to combine accuracy and reaction time (an alternative to difference scores) Finally, you should also think about what sort of data cleaning procedures you want to use. For instance, maybe you only want to calculate the FlankerEffect on RT for accurate trials and not innaccurate trials. Or perhaps you want to remove trials that are less than 200ms (too fast of responding to reflect cognitive processing). In the example we are about to go through we will implement the following data cleaning procedures when calculating the three scores listed above. They will be implemented in the following order Remove trials with less than 200ms reaction time In order to calculate FlankerEffect on RT for accurate trials, - Set RT on innacurate trials to missing NA. Trim RTs. Replace Outlier RTs that are above or below 3.5 SDs of the mean, with values exactly at 3.5 SDs above or below the mean. This is evaluated for each Subject by each condition seprately. Finally remove subjects that on any Trial condition (congruent, incongruent, neutral) performed less than 3.5 standard deviations below the mean on that condition. First start by opening a new script file and saving it to the R Scripts folder as “2_flanker_score” 7.5.1 Set up Create the Setup section of the R script. In addition to the import and output directories we will set the data cleaning paramters as well. ## Setup #### library(here) library(readr) library(dplyr) library(datawrangling) library(englelab) ## Set import and output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set data cleaning parameters min_RT.criteria &lt;- 200 sd.criteria &lt;- 3.5 ############# Import directory is the Raw Data folder where the “tidy” raw file is located. The output directory is the Scored Data folder. min_RT.criteria for data cleaning procedure 1 above. sd.criteria for data cleaning procedures 3 and 4 7.5.2 Import ## Import import &lt;- read_delim(here(import.dir, &quot;Flanker_raw.txt&quot;), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) 7.5.3 Calculate FlankerEffect on RT and Accuracy The general strategy we will take is to create two different dataframes. 1) data_flanker in which we calculate the FlankerEffect on RT and Accuracy and 2) data_binned in which we calculate the Flanker Binned scores. In calculating the FlankerEffect, we first need to do some data cleaning 7.5.3.1 Data Cleaning First we we should remove too fast of RTs ## Calculate FlankerEffect on RT and Accuracy data_flanker &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) Then set RTs to NA on innacurate trials ## Calculate FlankerEffect on RT and Accuracy data_flanker &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% mutate(RT = ifelse(Accuracy==0, NA, RT)) Trim RTs individual per subject per condition. To do this we will use the group_by() (from Chapter 4) and trim() (from Chapter 6) functions ## Calculate FlankerEffect on RT and Accuracy data_flanker &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% mutate(RT = ifelse(Accuracy==0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) Note the replace argument in trim(). Outliers are being replaced with the value at 3.5 SDs of the mean (the cutoff value). Outliers can also be replaced with the &quot;mean&quot;, &quot;median&quot;, or &quot;NA&quot;. 7.5.3.2 Calculate Mean RTs and Mean Accuracy Before removing subjects with too low accuracy for each condition we need to calculate mean Accuracy for each condition. ## Calculate FlankerEffect on RT and Accuracy data_flanker &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% mutate(RT = ifelse(Accuracy==0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() Be sure to ungroup() when you are done with it! And then spread “RT” and “Accuracy” across conditions. Creating columns for mean RT and mean Accuracy for each separate condition. ## Calculate FlankerEffect on RT and Accuracy data_flanker &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% mutate(RT = ifelse(Accuracy==0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;), id = &quot;Subject&quot;) And we might as well calaculate the FlankerEffects ## Calculate FlankerEffect on RT and Accuracy data_flanker &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% mutate(RT = ifelse(Accuracy==0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;), id = &quot;Subject&quot;) %&gt;% mutate(FlankerEffect_RT = incongruent_RT.mean - congruent_RT.mean, FlankerEffect_ACC = incongruent_Accuracy.mean - congruent_Accuracy.mean) 7.5.3.3 Remove poor performing subjects The approach I like to take with entirely removing subjects is to keep a record of those subjects in a data file somewhere. To do this we will create a new dataframe of subjects that will be removed. We will then use a function I created, remove_save() from the datawrangling package. This function is a short hand way of doing two things at one. Removing the subjects from the full data file Saving the removed subjects to a specified directory. The criteria we are removing subjects based on are those who performed 3.5 SDs below the mean. So we first need to calaculate a column of z-scores (on SD units), then filter those who are below 3.5 z-scores. ## Remove poor performing subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &gt; sd.criteria | congruent_Accuracy.mean_z &lt; (-1*sd.criteria) | incongruent_Accuracy.mean_z &gt; sd.criteria | incongruent_Accuracy.mean_z &lt; (-1*sd.criteria) | neutral_Accuracy.mean_z &gt; sd.criteria | neutral_Accuracy.mean_z &lt; (-1*sd.criteria)) Then use remove_save() data_flanker &lt;- remove_save(data_flanker, data_remove, output.dir = here(output.dir, &quot;removed&quot;), output.file = &quot;Flanker_removed.txt&quot;) The first argument is the full data frame (data_flanker) and the second argument is the dataframe that contains the subjects to be removed (data_remove). You should now see a folder called removed in the Scored Data folder with a file called “Flanker_removed.txt”. 7.5.4 Calculate Binned Scores Great! We have now calculated FlankerEffects scores and perfomred the data cleaning procedures. Now we need to calculate Binned scores. The data_flanker dataframe is no longer in a formate that we can calculate bin scores. We need to the original import dataframe that has trial level data. First, We should do the same RT data cleaning procedures as we did for calculating FlankerEffect on RT. Let’s call this new dataframe data_binned. ## Calculate Binned scores data_binned &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% ungroup() We should also remove the poor performing subjects. This step is actually really important for the binning procedure because bin scores are relative to other subjects in the data. ## Calculate Binned scores data_binned &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% ungroup() %&gt;% filter(!(Subject %in% data_remove$Subject)) We also need to remove neutral trials to calculate bin scores. Bin scores are based on comparing one condition to a baseline condition. In this case we want to compare the incongruent condition to the baseline congruent condition. So we need to get rid of neutral conditions. ## Calculate Binned scores data_binned &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% ungroup() %&gt;% filter(!(Subject %in% data_remove$Subject), Condition != &quot;neutral&quot;) And finally calculate bin scores using bin_score() from the englelab package. ## Calculate Binned scores data_binned &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% ungroup() %&gt;% filter(!(Subject %in% data_remove$Subject), Condition != &quot;neutral&quot;) %&gt;% bin_score(rt.col = &quot;RT&quot;, accuracy.col = &quot;Accuracy&quot;, type = &quot;mean&quot;, condition.col = &quot;Condition&quot;, baseline.condition = &quot;congruent&quot;, id = &quot;Subject&quot;) %&gt;% rename(FlankerBin = &quot;BinScore&quot;) Awesome! Now we have two dataframes, one, data_flanker, with FlankerEffect scores and another, data_binned with FlankerBin scores. They both have one row per subject. Now we can merge these two dataframes together using the merge() function from base R. ## Merge dataframes data_flanker &lt;- merge(data_flanker, data_binned, by = &quot;Subject&quot;, all = TRUE) 7.5.5 Save data file And finally save the datafile ## Save data write_delim(data_flanker, path = here(output.dir, &quot;Flanker_Scores.txt&quot;), delim = &quot;\\t&quot;, na = &quot;&quot;) rm(list=ls()) If we put it all together your R script should look something like: ## Setup #### library(here) library(readr) library(dplyr) library(datawrangling) library(englelab) ## Set import and output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set data cleaning parameters min_RT.criteria &lt;- 200 sd.criteria &lt;- 3.5 ############# ## Import import &lt;- read_delim(here(import.dir, &quot;Flanker_raw.txt&quot;), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Calculate FlankerEffect on RT and Accuracy data_flanker &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% mutate(RT = ifelse(Accuracy==0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;), id = &quot;Subject&quot;) %&gt;% mutate(FlankerEffect_RT = incongruent_RT.mean - congruent_RT.mean, FlankerEffect_ACC = incongruent_Accuracy.mean - congruent_Accuracy.mean) ## Remove poor performing subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &gt; sd.criteria | congruent_Accuracy.mean_z &lt; (-1*sd.criteria) | incongruent_Accuracy.mean_z &gt; sd.criteria | incongruent_Accuracy.mean_z &lt; (-1*sd.criteria) | neutral_Accuracy.mean_z &gt; sd.criteria | neutral_Accuracy.mean_z &lt; (-1*sd.criteria)) data_flanker &lt;- remove_save(data_flanker, data_remove, output.dir = here(output.dir, &quot;removed&quot;), output.file = &quot;Flanker_removed.txt&quot;) ## Calculate Binned scores data_binned &lt;- import %&gt;% filter(RT &gt;= min_RT.criteria) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = sd.criteria, replace = &quot;cutoff&quot;) %&gt;% ungroup() %&gt;% filter(!(Subject %in% data_remove$Subject), Condition != &quot;neutral&quot;) %&gt;% bin_score(rt.col = &quot;RT&quot;, accuracy.col = &quot;Accuracy&quot;, type = &quot;mean&quot;, condition.col = &quot;Condition&quot;, baseline.condition = &quot;congruent&quot;, id = &quot;Subject&quot;) %&gt;% rename(FlankerBin = &quot;BinScore&quot;) ## Merge dataframes data_flanker &lt;- merge(data_flanker, data_binned, by = &quot;Subject&quot;, all = TRUE) ## Save data write_delim(data_flanker, path = here(output.dir, &quot;Flanker_Scores.txt&quot;), delim = &quot;\\t&quot;, na = &quot;&quot;) rm(list=ls()) 7.6 source() in Masterscript Now we can add lines of code in the manuscript to execute or source() the two scripts, “1_flanker_raw.R” and “2_flanker_score.R”. ## Setup #### library(here) ## Set the directory tree directories &lt;- list(scripts = &quot;R Scripts&quot;, data = &quot;Data Files&quot;, raw = &quot;Data Files/Raw Data&quot;, messy = &quot;Data Files/Raw Data/E-Merge&quot;, scored = &quot;Data Files/Scored Data&quot;, results = &quot;Results&quot;) saveRDS(directories, here(&quot;directories.rds&quot;)) ############# ## &quot;messy&quot; to &quot;tidy&quot; raw data source(here(&quot;R Scripts&quot;, &quot;1_flanker_raw.R&quot;), echo=TRUE) ## &quot;tidy&quot; to scored data source(here(&quot;R Scripts&quot;, &quot;2_flanker_score.R&quot;), echo=TRUE) rm(list=ls()) In using the mastersript you can either execute the entire script by clicking on “Source” or you can run one line of code at a time. For instance, maybe you have already created the “tidy” raw data files. You can open up the masterscript and simply execute the line of code that sources the script to score the task data. This gives you flexibility in controlling your scripts. This becomes more useful when you have a lot of scripts to run (we only have two in this case so it is not too big of a deal). Now on to scoring the “tidy” raw data file "],
["analyzing-data-in-r.html", "Chapter 8 Analyzing Data in R 8.1 R Markdown", " Chapter 8 Analyzing Data in R Phew! You’ve made it this far, good job. Up until now you have been learning how to do data preparation steps in R. Now for the fun part, statistical analyses and data visualization! This is the third and final step in the data workflow process depicted above. Traditionally you have likely done these analyses in SPSS or EQS and have created figures in Excel or PowerPoint. The rest of the tutorial will cover how to do these steps in R. Writing scripts to do statistical analyses is an entirely different process than writing scripts for data preparation. Therefore, we should first go over the general process of conducting and outputing statistical analyses in R. In programs like SPSS when you run a statistical analysis, it will be outputed to a viewable .spv document. One dowfall of this is that .spv files are proprietry format so can only be opened if you have SPSS installed. However, there is the option to export a .spv file as a PDF. One downfall about R is that unlike SPSS, there is not a native way to create output documents from statistical analyses. Fortunately, RStudio has an output document format called R Markdown. 8.1 R Markdown R Markdown is a powerful way to create reports of statistical analyses. Reports can be outputed in a lot of different formats; html, Microsoft Word, PDF, presentation slides, and more. In fact, this tutorial was created using R Markdown. The easiest format to output as is html. html documents are opened in a web browser and therefore can be opened on any computer and device (phones, tablets, Windows, Mac). For a brief intro to R Markdown see https://rmarkdown.rstudio.com/lesson-1.html First, you need to install the rmarkdown package install.packages(&quot;rmarkdown&quot;) To open an R Markdown document go to File -&gt; New File -&gt; R Markdown… Select HTML and click OK An example R Markdown document will open. Go ahead and read the contents of the document. There are three types of content in an R Markdown document: A YAML header R code chunks Formatted text 8.1.1 YAML header The YAML header contains metadata about how the document should be rendered and the output format. It is located at the very top of the document and is surrounded by lines of three dashe, --- title: &quot;Title of document&quot; output: html_document --- There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html 8.1.2 R code chunks Unlike a typical R script file (.R), an R Markdown document (.Rmd) is a mixture of formated text and R code chunks. Not everything in an R Markdown document is executed in the R console, only the R code chunks. To run chunks of R code you can click on the green “play” button on the top right of the R code chunk. Go ahead and do this for the three R code chunks in the R Markdown document you opened. (cars and pressure are just example dataframes that come pre-loaded with R). We have not gone over these functions yet, but you can see that the results of the R code are now displayed in the document. The first R code chunk is just setting some default options of how the output of R code chunks should be displayed. We will cover these options in more detail later. 8.1.3 Formatted text The formatted text sections are more than just adding comments to lines of code. You can write up descriptive reports, create bulleted or numbered lists, embed images or web links, create tables, and more. The text is formatted using a language known as Markdown, hence the name R Markdown. Markdown is a convenient and flexible way to format text. When a Markdown document is rendered into some output (such as html or PDF), the text will be formatted as specified by the Markdown syntax. In the R Markdown document you have open you can see some Markdown syntax. The pound signs ## at the beggining of a line are used to format headers. One # is a level one header, two ## is a level two header and so on. Also notice in the second paragraph, the word Knit is surrounded by two asterisks on each side. When this document is redered, thw word Knit will be bolded. Go ahead and render the R Markdown document by clicking on the Knit button at the top of the window. Once it is done rendering you will see a new window pop up. This is the outputed html file. You can see how the document has formated text based on the Markdown syntax. There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. One guide that I frequently reference is https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Now on to scoring the “tidy” raw data file "]
]
