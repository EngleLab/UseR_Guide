[["index.html", "EngleLab: useRguide Welcome", " EngleLab: useRguide Jason Tsukahara Welcome This is a guide on how to use R for common data procedures and analyses we conduct in the Attention and Working Memory Lab (EngleLab) at Georgia Tech. This guide is in no way a comprehensive guide on using R. A more comprehensive guide to working with data in R is provided in the popular R for Data Science. The first two sections of this guide are about basic R skills and working with data. The Analyze Data section will be the most useful for getting started quickly on using R in our lab. It will cover common steps for tidying raw data, scoring and cleaning data, and merging data. The last two sections cover Data Visualization (something that R is excellent at) and Statistical Analysis. "],["installation.html", "1 Installation 1.1 Installing R 1.2 Installing R Studio 1.3 The R Studio Environemnt 1.4 R Studio Settings", " 1 Installation 1.1 Installing R First you need to download the latest version of R from their website https://www.r-project.org Select CRAN on the left, just under Download Select the first option under 0-Cloud Select the download option depending on your computer Select the base installation (for Windows) or the Latest Release (for Mac) Open and Run the installation file 1.2 Installing R Studio The easiest way to interact with R is through the R Studio environment. To do this you need to install R Studio Select the Free version of R Studio Desktop Select the download option depending on your computer 1.3 The R Studio Environemnt Go ahead an open the RStudio application on your computer. When you open a fresh session of RStudio there are 3 window panes open. The Console window, the Environment window, and the Files window. Go ahead and navigate to File -&gt; New File -&gt; R Script. You should now see something similar to the image below There are 4 window panes and each one has it’s own set of tabs associated with it: The Console window (the bottom left window pane) is where code is executed and output is displayed. The Source window (the top left window pane) is where you will write your code to create a script file. When you open a new script file you will see a blank sheet where you can start writing the script. When you execute lines of code from here you will see it being executed in the Console window. The Source window is also where you can view data frames you have just imported or created. In the image above, notice the different tabs in the Source window. There are two “Untitled” script files open and one data frame called ‘data’. The Environment window (top right window pane) is where you can see any data frames, variables, or functions you have created. Go ahead and type the following in your Console window and hit enter. hello &lt;- &quot;hello&quot; You should now see the object hello in the Environment window pane The Files window (the bottom right window pane) is where you can see your computer’s directories, plots you create, manage packages, and see help documentation. 1.4 R Studio Settings There are a few changes to R Studio settings I suggest you make. I will not go into why these are a good idea - so just do what I say! If you want to know you can talk to me about it. Navigate to Tools -&gt; Global Options Change the settings to look like this: Be sure to set ‘Save workspace to .RData on exit’ to Never You can also change the “Editor Theme” if you navigate to the “Appearance” tab in Settings. Dark themes are easier on the eyes. I use Material dark theme. "],["basic-r.html", "2 Basic R 2.1 Creating R objects 2.2 If…then Statements 2.3 R Packages 2.4 More R Basic Resources", " 2 Basic R This chapter will cover the basics of how to assign values to objects, create and extract information from vectors, lists, and data frames. If you have not done so already, open a new R script file. To create a new R script go to File -&gt; New File -&gt; R Script This should have opened a blank Script window called Untitled. The Script window is a file where you are saving your code. This is where you will write, edit, delete, re-write, your code. To follow along with the tutorial, you should type the lines of code I display in the tutorial into your script. Go ahead and save your empty script as 2_basics.R 2.1 Creating R objects In R, everything that exists is an object and everything you do to objects are functions. You can define an object using the assignment operator &lt;-. Everything on the left hand side of the &lt;- assignment operator is an object. Everything on the right hand side of &lt;- are functions or values. Go ahead and type the following two lines of code in your script string &lt;- &quot;hello&quot; string ## [1] &quot;hello&quot; You can execute/run a line of code by placing the cursor anywhere on the line and press Ctrl + Enter. Go ahead an run the two lines of code. In this example, the first line creates a new object called string with a value of “hello”. The second line simply prints the output of string to the Console window. In the second line there is no assignment operator. When there is no &lt;- this means you are essentially just printing to the console. You can’t do anything with stuff that is just printed to the console, it is just for viewing purposes. For instance, if I wanted to calculate 1 + 2 I could do this by printing it to the console 1 + 2 ## [1] 3 However, if I wanted to do something else with the result of that calculation then I would not be able to unless I assigned the result to an object using &lt;- result &lt;- 1 + 2 result &lt;- result * 5 result ## [1] 15 The point is, you are almost always going to assign the result of some function or value to an object. Printing to the console is not very useful. Almost every line of code, then, will have an object name on the left hand side of &lt;- and a function or value on the right hand side of &lt;- In the first example above, notice how I included \" \" around hello. This tells R that hello is a string, not an object. If I were to not include \" \", then R would think I am calling an object. And since there is no object with the name hello it will print an error string &lt;- hello ## Error in eval(expr, envir, enclos): object &#39;hello&#39; not found Do not use \" \" for Numerical values a &lt;- &quot;5&quot; + &quot;1&quot; ## Error in &quot;5&quot; + &quot;1&quot;: non-numeric argument to binary operator You can execute lines of code by: Typing them directly into the Console window Typing them into the Script window and then on that line of code pressing Ctrl+Enter. With Ctrl+Enter you can execute one line of your code at a time. Clicking on Source at the top right of the Script window. This will run ALL the lines of code contained in the script file. It is important to know that EVERYTHING in R is case sensitive. a &lt;- 5 a + 5 ## [1] 10 A + 5 ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found 2.1.1 Classes Classes are types of values that exist in R: character \"hello\", \"19\" numeric (or double) 2, 32.55 integer 5, 99 logical TRUE, FALSE To evaluate the class of an object you can use the typeof() typeof(a) ## [1] &quot;double&quot; To change the class of values in an object you can use the function as.character() , as.numeric() , as.double() , as.integer() , as.logical() functions. as.integer(a) ## [1] 5 as.character(a) ## [1] &quot;5&quot; as.numeric(&quot;hello&quot;) ## Warning: NAs introduced by coercion ## [1] NA 2.1.2 Vectors Okay so now I want to talk about creating more interesting objects than just a &lt;- 5. If you are going to do anything in R it is important that you understand the different data types and data structures you can use in R. I will not cover all of them in this tutorial. For more information on data types and structures see this nice Introduction to R Vectors contain elements of data. The length of a vector is the number of elements in the vector. For instance, the variable a we created earlier is actually a vector of length 1. It contains one element with a value of 5. Now let’s create a vector with more than one element. b &lt;- c(1,3,5) c() is a function. Functions contain arguments that are inputs for the function. Arguments are separated by commas. In this example the c() function concatenates the arguments (1, 3, 5) into a vector. We are passing the result of this function to the object b. What do you think the output of b will look like? b ## [1] 1 3 5 You can see that we now have a vector that contains 3 elements; 1, 3, 5. If you want to reference the value of specific elements of a vector you use brackets [ ]. For instance, b[2] ## [1] 3 The value of the second element in vector b is 3. Let’s say we want to grab only the 2nd and 3rd elements. We can do this at least two different ways. b[2:3] ## [1] 3 5 b[-1] ## [1] 3 5 Now, it is important to note that we have not been changing vector b. If we display the output of b, we can see that it still contains the 3 elements. b ## [1] 1 3 5 To change vector b we need to define b as vector b with the first element removed b &lt;- b[-1] b ## [1] 3 5 Vector b no longer contains 3 elements. Now, let’s say we want to add an element to vector b. c(5,b) ## [1] 5 3 5 Here the c() function created a vector with the value 5 as the first element followed by the values in vector b Or we can use the variable a that has a value of 5. Let’s add this to vector b b &lt;- c(a,b) b ## [1] 5 3 5 What if you want to create a long vector with many elements? If there is a pattern to the sequence of elements in the vector then you can create the vector using seq() seq(0, 1000, by = 4) ## [1] 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 ## [29] 112 116 120 124 128 132 136 140 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 216 220 ## [57] 224 228 232 236 240 244 248 252 256 260 264 268 272 276 280 284 288 292 296 300 304 308 312 316 320 324 328 332 ## [85] 336 340 344 348 352 356 360 364 368 372 376 380 384 388 392 396 400 404 408 412 416 420 424 428 432 436 440 444 ## [113] 448 452 456 460 464 468 472 476 480 484 488 492 496 500 504 508 512 516 520 524 528 532 536 540 544 548 552 556 ## [141] 560 564 568 572 576 580 584 588 592 596 600 604 608 612 616 620 624 628 632 636 640 644 648 652 656 660 664 668 ## [169] 672 676 680 684 688 692 696 700 704 708 712 716 720 724 728 732 736 740 744 748 752 756 760 764 768 772 776 780 ## [197] 784 788 792 796 800 804 808 812 816 820 824 828 832 836 840 844 848 852 856 860 864 868 872 876 880 884 888 892 ## [225] 896 900 904 908 912 916 920 924 928 932 936 940 944 948 952 956 960 964 968 972 976 980 984 988 992 996 1000 Vectors can only contain elements of the same “class”. d &lt;- c(1, &quot;2&quot;, 5, 9) d ## [1] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;9&quot; as.numeric(d) ## [1] 1 2 5 9 2.1.3 Factors Factors are special types of vectors that can represent categorical data. You can change a vector into a factor object using factor() factor(c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;)) ## [1] male female male male female female male ## Levels: female male factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;)) ## [1] high low medium high high low medium medium ## Levels: high low medium f &lt;- factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;), levels = c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;)) f ## [1] high low medium high high low medium medium ## Levels: high medium low 2.1.4 Lists Lists are containers of objects. Unlike Vectors, Lists can hold different classes of objects. list(1, &quot;2&quot;, 2, 4, 9, &quot;hello&quot;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] &quot;hello&quot; You might have noticed that there are not only single brackets, but double brackets [[ ]] This is because Lists can hold not only single elements but can hold vectors, factors, lists, data frames, and pretty much any kind of object. l &lt;- list(c(1,2,3,4), &quot;2&quot;, &quot;hello&quot;, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) l ## [[1]] ## [1] 1 2 3 4 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] &quot;hello&quot; ## ## [[4]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; You can see that the length of each element in a list does not have to be the same. To reference the elements in a list you need to use the double brackets [[ ]]. l[[1]] ## [1] 1 2 3 4 To reference elements within list elements you use double brackets followed by a single bracket l[[4]][2] ## [1] &quot;b&quot; You can even give names to the list elements person &lt;- list(name = &quot;Jason&quot;, phone = &quot;123-456-7890&quot;, age = 23, favorite_colors = c(&quot;blue&quot;, &quot;red&quot;, &quot;brown&quot;)) person ## $name ## [1] &quot;Jason&quot; ## ## $phone ## [1] &quot;123-456-7890&quot; ## ## $age ## [1] 23 ## ## $favorite_colors ## [1] &quot;blue&quot; &quot;red&quot; &quot;brown&quot; And you can use the names to reference elements in a list person[[&quot;name&quot;]] ## [1] &quot;Jason&quot; person[[&quot;favorite_colors&quot;]][3] ## [1] &quot;brown&quot; 2.1.5 Data Frames You are probably already familiar with this type of data structure. SPSS and Excel uses this type of structure. It is just rows and columns of data. A data table! This is the format that is used to perform statistical analyses on. So let’s create a data frame so you can see what one looks like in RStudio data &lt;- data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) data ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 You can view the Data Frame by clicking on the object in the Environment window or by executing the command View(data) Notice that it created three columns labeled id, x, and y. Also notice that since we only specified a vector of length 2 for x this column is coerced into 10 rows of repeating “a” and “b”. All columns in a data frame must have the same number of rows. You can use the $ notation to reference just one of the columns in the data frame data$y ## [1] 10 20 30 40 50 60 70 80 90 100 Alternatively you can use data[&quot;y&quot;] ## y ## 1 10 ## 2 20 ## 3 30 ## 4 40 ## 5 50 ## 6 60 ## 7 70 ## 8 80 ## 9 90 ## 10 100 To reference only certain rows within a column data$y[1:5] ## [1] 10 20 30 40 50 data[1:5,&quot;y&quot;] ## [1] 10 20 30 40 50 2.2 If…then Statements If…then statements are useful for when you need to execute code only if a certain statement is TRUE. For instance,… First we need to know how to perform logical operations in R Okay, we have this variable a a &lt;- 5 Now let’s say we want to determine if the value of a is greater than 3 a &gt; 3 ## [1] TRUE You can see that the output of this statement a &gt; 3 is TRUE Here is a list of logical operations in R Now let’s write an if…then statement. If a is greater than 3, then multiply a by 2. if (a&gt;3){ a &lt;- a*2 } a ## [1] 10 The expression that is being tested is contained in parentheses, right after the if statement. If this expression is evaluated as TRUE then it will perform the next line(s) of code. The { is just a way of encasing multiple lines of code within one if statement. The lines of code then need to be closed of with }. In this case, since we only had one line of code b &lt;- a*2 we could have just written it as. a &lt;- 5 if (a&gt;3) a &lt;- a*2 a ## [1] 10 What if we want to do something to a if a is NOT greater than 3? In other words… if a is greater than 3, then multiple a by 2 else set a to missing a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 You can keep on chaining if…then… else… if… then statements together. a &lt;- 5 if (is.na(a)){ print(&quot;Missing Value&quot;) } else if (a&lt;0){ print(&quot;A is less than 0&quot;) } else if (a&gt;3){ print(&quot;A is greater than 3&quot;) } ## [1] &quot;A is greater than 3&quot; 2.3 R Packages R comes with a basic set of functions. All the functions we have used so far are part of the R basic functions. But when you want to start doing more complex operations it would be nice to have more complex functions. This is where R Packages come in… An R Package is simply a collection of functions - that usually have some common theme to them. Now the most wonderful thing about R is that other R users have developed tons of packages with functions they created themselves. For instance, a group of users have developed an R package called lavaan that makes it extremely easy to conduct SEM in R. 2.3.1 Installing and Loading R Packages R packages are easy to install and load. You just need to know the name of the package. install.packages(&quot;name_of_package&quot;) or for multiple packages at once install.packages(c(&quot;package1&quot;, &quot;package2&quot;, &quot;package3&quot;)) Installing the package does not mean you can start using the functions. To be able to use the function you need to then load the package library of functions as such library(name_of_package) When loading packages you do not have to in case the package name in \" \" 2.4 More R Basic Resources For additional tips in the basics of coding R see: https://ramnathv.github.io/pycon2014-r/visualize/README.html https://www.datacamp.com/courses/free-introduction-to-r/?tap_a=5644-dce66f&amp;tap_s=10907-287229 http://compcogscisydney.org/psyr/ http://r4ds.had.co.nz/workflow-basics.html "],["intermediate-r.html", "3 Intermediate R 3.1 For Loops 3.2 Functions", " 3 Intermediate R This chapter will cover more intermediate R programming, such as for loops, and functions. Save a new R script as 3_intermediate.R 3.1 For Loops For loops allow you iterate the same line of code over multiple instances. Let’s say we have a vector of numerical values c &lt;- c(1,6,3,8,2,9) c ## [1] 1 6 3 8 2 9 and want perform an if…then operation on each of the elements. Let’s use the same if…then statement we used above. If the element is greater than 3, then multiply it by 2 - else set it to missing. Let’s put the results of this if…then statement into a new vector d What we need to do is loop this if…then statement for each element in c We can start out by writing the for loop statement for (i in seq_along(c)){ } This is how it works. The statement inside of parentheses after for contains two statements separated by in. The first statement is the variable that is going to change it’s value over each iteration of the loop. You can name this whatever you want. In this case I chose the label i. The second statement defines all the values that will be used at each iteration. The second statement will always be a vector. In this case the vector is seq_along(c). seq_along() is a function that creates a vector that contains a sequence of numbers from 1 to the length of the object. In this case the object is the vector c, which has a length of 6 elements. Therefore seq_along(c), creates a vector containing 1, 2, 3, 4, 5, 6. The for loop will start with i defined as 1, then on the next iteration the value of i will be 2 … and so until the last element of seq_along(c), which is 6. We can see how this is working by printing ‘i’ on each iteration. for (i in seq_along(c)){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 You can see how on each iteration it prints the values of seq_along(c) from the first element to the last element. What we will want to do is, on each iteration of the for loop, access the ith element of the vector c. Recall, you can access the element in a vector with [ ], for instance c[1]. Let’s print each ith element of c. for (i in seq_along(c)){ print(c[i]) } ## [1] 1 ## [1] 6 ## [1] 3 ## [1] 8 ## [1] 2 ## [1] 9 Now instead of printing i the for loop is printing each element of vector c. Let’s use the same if…then statement as above a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 But instead we need to replace a with c[i] For now let’s just print() the output of the if… then statement. for (i in seq_along(c)){ if (c[i] &gt; 3){ print(c[i]*2) } else { print(NA) } } ## [1] NA ## [1] 12 ## [1] NA ## [1] 16 ## [1] NA ## [1] 18 Now for each element in c, if it is is greater than 3, then multiply it by 2 - else set as missing value. You can see that on each iteration the output is either the ith element of c multiplied by 2 or NA. But just printing things to the console is useless! Let’s overwright the old values in c with the new values. for (i in seq_along(c)){ if (c[i] &gt; 3){ c[i] &lt;- c[i]*2 } else { c[i] &lt;- NA } } But what if we want to preserve the original vector c? Well we need to put it into a new vector, let’s call it vector d. This get’s a little more complicated but is something you might find yourself doing fairly often so it is good to understand how this works. But if you are goind to do this to a “new” vector that is not yet created you will run into an error. c &lt;- c(1,6,3,8,2,9) for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } You first need to create vector d - in this case we can create an empty vector. d &lt;- c() So the logic of our for loop, if…then statement is such that; on the ith iteration - if c[i] is greater than 3, then set d[i] to c[i]*2 - else set d[i] to NA. c &lt;- c(1,6,3,8,2,9) d &lt;- c() for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } c ## [1] 1 6 3 8 2 9 d ## [1] NA 12 NA 16 NA 18 Yay! Good job. 3.2 Functions Basically anything you do in R is by using functions. In fact, learning R is just learning what functions are available and how to use them. Not much more to it than that. You have only seen a couple of functions at this point. In this chapter, a common function used was c(). This function simply concatenates a series of numerical or string values into a vector. c(1,6,3,7). Functions start with the name of the function followed by parentheses function_name(). Inside the () is where you specify certain arguments separated by commas , . Some arguments are optional and some are required for the function to work. For example, another function you saw last chapter was data.frame(). This function creates a data frame with the columns specified by arguments. data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 The arguments id, x, and y form the columns in the data frame. These arguments themselves used functions. For instance y used the function seq(). This function creates a sequence of numbers in a certain range at a given interval. Sometimes arguments are not defined by an =. The first two arguments in in seq() specify the range of 10 to 100. The third argument by specified the interval to be 10. So seq(10, 100, by = 10) creates a sequence of numbers ranging from 10 to 100 in intervals of 10. seq(10, 100, by = 10) ## [1] 10 20 30 40 50 60 70 80 90 100 In the seq() function the by argument is not required. This is because there is a default by value of 1. seq(10, 100) ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ## [36] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 ## [71] 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Obviously if you want to specify a different interval, then you will need to specify by =. 3.2.1 Creating Your Own Functions This section is optional. It will go over how to create your own functions. Even if you do not want to get too proficient in R, it can be a good idea to know how to create your own function. It also helps you better understand how functions actually work. We are going to create a function that calculates an average of values. To define a function you use the function() and assign the output of function() to an object, which becomes the name of the function. For instance, function_name &lt;- function(){ } This is a blank function so it is useless. Before we put stuff inside of a function let’s work out the steps to calculate an average. Let’s say we have an array a that has 10 elements a &lt;- c(1,7,4,3,8,8,7,9,2,4) a ## [1] 1 7 4 3 8 8 7 9 2 4 To calculate an average we want to take the sum of all the values in a and divide it by the number of elements in a. To do this we can use the sum() and length() functions. sum(a) ## [1] 53 length(a) ## [1] 10 sum(a)/length(a) ## [1] 5.3 Easy! So now we can just put this into a function. average &lt;- function(x){ avg &lt;- sum(x)/length(x) return(avg) } When creating a function, you need to specify what input arguments the function is able to take. Here were are specifying the argument x. You can use whatever letter or string of letters you want, but a common notation is to use x for the object that is going to be evaluated by the function. Then, inside the function we use the same letter x to calculate the sum() and length() of x. What this means is that Arguments specified in a function become objects (or variables) passed inside the function You can create new objects inside a function. For instance we are creating an object, avg. However, these objects are created only inside the environment of the function. You cannot use those objects outside the function and they will not appear in your Environment window. To pass the value of an object outside of the function, you need to specify what you want to return() or what is the outpute of the function. In this case it is the object avg that we created inside the function. Let’s see the function in action average(a) ## [1] 5.3 Cool! You created your first function. Becuase the function only takes one argument x it knows that whatever object we specify in average() is the object we want to evaluate. But what if our vector contains missing values? b &lt;- c(1,NA,4,2,7,NA,8,4,9,3) average(b) ## [1] NA Uh oh. Here the vector b contains two missing values and the function average(b) returns NA. This is because in our function we use the function sum() without specifying to ignore missing values. If you type in the console ?sum you will see that there is an argument to specify whether missing values should be removed or not. The default value of this argument is FALSE so if we want to remove the missing values we need to specify na.rm = TRUE. It is a good idea to make your functions as flexible as possible. Allow the user to decide what they want to happen. For instance, it might be the case that the user wants a value of NA returned when a vector contains missing values. So we can add an argument to our average() function that allows the user to decide what they want to happen; ignore missing values or return NA if missing values are present. Let’s label this argument na.ignore. We could label it na.rm like the sum() function but for the sake of this Tutorial I want you to learn that you can label these arguments however you want, it is arbitrary. The label should make sense however. Before we write the function let’s think about what we need to change inside the function. Basically we want our new argument na.ignore to change the value of na.rm in the sum() function. If na.ignore is TRUE then we want na.rm = TRUE. Remember that arguments become objects inside of a function. So we will want to change: avg &lt;- sum(x)/length(x) to avg &lt;- sum(x, na.rm = na.ignore)/length(x) Let’s try this out on our vector b na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b) ## [1] 3.8 We can test if our average function is calculating this correctly by using the actual base R function mean(). mean(b, na.rm = TRUE) ## [1] 4.75 Uh oh. We are getting different values. This is because length() is also not ignoring missing values. The length of b, is 10. The length of b ignoring missing values is 8. Unfortunately, length() does not have an argument to specify we want to ignore missing values. How we can tell length() to ignore missing values is by length(b[!is.na(b)]) ## [1] 8 This is saying, evaluate the length of elements in b that are not missing. Now we can modify our function with na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b[!is.na(b)]) ## [1] 4.75 to get average &lt;- function(x, na.ignore = FALSE){ avg &lt;- sum(x, na.rm = na.ignore)/length(x[!is.na(x)]) return(avg) } average(b, na.ignore = TRUE) ## [1] 4.75 mean(b, na.rm = TRUE) ## [1] 4.75 Walla! You did it. You created a function. Notice that we set the default value of na.ignore to FALSE. If we had set it as TRUE then we would not need to specify average(na.ignore = TRUE) since TRUE would have been the default. When using functions it is important to know what the default values are Both for loops and functions allow you to write more concise and readable code. If you are copying and pasting the same lines of code with only small modification, you can probably write those lines of code in a for loop or a function. "],["tidyverse.html", "4 Tidyverse", " 4 Tidyverse In this section you will learn how to work with data in R by using a collection of packages known as the tidyverse The tidyverse is a collection of R packages that share an underlying design philosophy, grammar, and data structures. Hadley Wickham has been the main contributor to developing the tidyverse. Although you will be learning R in this tutorial, it might be more appropriate to say that you are learning the tidyverse. The tidyverse consists of packages that are simple and intuitive to use and will take you from importing data (with readr), to transforming and manipulating data structures (with dplyr and tidyr), and to data visualization (with ggplot2). "],["read-and-write-data.html", "5 Read and Write Data 5.1 CSV 5.2 Tab-Delimited 5.3 SPSS 5.4 RStudio Import GUI 5.5 Import and Merge Multiple Data Files", " 5 Read and Write Data Every R script that you write will require you to import a data file and output a new data file. In this Chapter you will learn various functions to import and output comma-separate value (csv), tab-delimited, and SPSS data files. For most of these data types we can use the readr package The readr package contains useful functions for importing and outputting data files. Go ahead and install the readr package. In the console type: install.packages(&quot;readr&quot;) We will also use the foreign and haven packages for SPSS data files install.packages(&quot;foreign&quot;) install.packages(&quot;haven&quot;) You do not really need to save an R script file for this Chapter. We will use some example data files for this chapter. Go ahead and download these files. You will have to unzip the file. For now just unzip it in your downloads folder. Inside the unzipped folder you will see a number of data files in different file formats. Download Example Import Data Files 5.1 CSV csv files are by far the easiest files to import into R and most software programs. For this reason, I suggest any time you want to save/output a data file to your computer, do it in csv format. 5.1.1 Import .csv We can import csv files using read_csv() from the readr package. library(readr) read_csv(&quot;filepath/datafile.csv&quot;) You can see this is very simple. We just need to specify a file path to the data. I will talk more about file paths later but for now we will use absolute file paths, although it is highly suggested not to use them. In general, DO NOT USE ABSOLUTE FILE PATHS! This chapter is more about the different functions to import various types of data files. First, figure out the absolute file path to your downloads folder (or wherever the unzipped data folder is located). On Windows the absolute file path will usually start from the C:/ drive. On Macs, it starts from ~/ Import the Flanker_Scores.csv file. You might have something that looks like read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) However, this just printed the output of read_csv() to the console. To actually import this file into R, we need to assign it to an object in our Environment. import_csv &lt;- read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) You can name the object whatever you like. I named it import_csv. To view the data frame View(import_csv) 5.1.2 Output .csv We can output a csv file using write_csv() from the readr package. write_csv(object, &quot;filepath/filename.csv&quot;) Let’s output the object import_csv to a csv file named: new_Flanker_Scores.csv to the downloads folder write_csv(import_csv, &quot;~/Downloads/new_Flanker_Scores.csv&quot;) Note that whenever writing (outputting) a file to our computer there is no need to assign the output to an object. 5.2 Tab-Delimited tab-delimited files are a little more tedious to import just because they require specifying more arguments. Which means you have to memorize more to import tab-delimited files. 5.2.1 Import .txt To import a tab-delimited file we can use read_delim() from the readr package. read_delim(&quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) There are three additional arguments we have to specify: delim, escape_double, and trim_ws. The notation for tab-delimted files is \"\\t\". Let’s import the Flanker_raw.txt file import_tab &lt;- read_delim(&quot;~/Downloads/Flanker_raw.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) View the import_tab object 5.2.2 Output .txt We can output a tab-delimited file using write_delim() from the readr package. write_delim(object, path = &quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;) Output the import_tab object to a file named: new_Flanker_raw.txt write_delim(import_tab, path = &quot;~/Downloads/Flanker_raw.txt&quot;, delim = &quot;\\t&quot;) 5.3 SPSS As horrible as it might sound, there might be occasions where we need to import an SPSS data file. And worse, we might need to output an SPSS data file! I will suggest to use different packages for importing and outputing spss files. 5.3.1 Import .sav To import an SPSS data file we can use read.spss() from the foreign package. library(foreign) read.spss(&quot;filepath/filename.sav&quot;, to.data.frame = TRUE, use.value.labels = TRUE) The use.value.labels argument allows us to import the value labels from an SPSS file. Import and View the sav file CH9 Salary Ex04.sav import_sav &lt;- read.spss(&quot;~/Downloads/CH9 Salary Ex04.sav&quot;) 5.3.2 Output .sav To output an SPSS data file we can use write_sav() from the haven packge. library(haven) write_sav(object, &quot;filepath/filename.sav&quot;) Go ahead and output the import_sav object to a file: new_CH9 Salary Ex04.sav write_sav(import_sav, &quot;~/Downloads/new_CH9 Salary Ex04.sav&quot;) 5.4 RStudio Import GUI The nice thing about R Studio is that there is also a GUI for importing data files. When you are having difficulty importing a file correctly or unsure of the file format you can use the RStudio Import GUI. In the Environment window click on “Import Dataset”. You will see several options available, these options all rely on different packages. Select whatever data type you want to import You will see a data import window open up that looks like this Select “Browse” on the top right and select the data file you want to import. The “Data Preview” window will let you see if it is importing it in the right format. You can change the import options below this. You might want to change the “Name” but you can always do this later in the R Script. Make sure all the settings are correct by assessing the “Data Preview” window. Does the data frame look as you would expect it to? Finally, copy and paste the code you need in the “Code Preview” box at the bottom right. You might not always need the library(readr) or View(data) lines. Rather than selecting “Import” I suggest just closing out of the window and pasting the code into your R script. csv files have a nice feature in that RStudio knows that these are file types we might want to import. So instead of navigating through the Import Dataset GUI we can just click on the file in the Files window pane. 5.5 Import and Merge Multiple Data Files You might find yourself in a situation where you need to import multiple data files and merge them into a single data frame. You could import each data file one at a time using the functions above and then merge them using some type of merging function. However, if you have a lot of data files you need to import and merge this can be very tedious. Therefore, I have written a general use function to do this for us. In R, a “join” is merging data frames together that have at least some rows in common (e.g. Same Subject IDs) and have at least one column that is different. The rows that are common serve as the reference for how to “join” the data frames together. In R, a “bind” is combining data frames together by staking either the rows or columns. It is unlikely that we you will need to do a column bind so we can skip that. A row “bind” takes data frames that have the same columns but different rows. This will happen if you have separate data files for each subject from the same task. Each subject data file will have their unique rows (subject by trial level data) but they will all have the same columns. For example, the E-Merge software program is performing a row “bind” of each subject .edat file. For E-Prime data we have to go through the E-Merge software program to bind individual subject files. However, you might have individual subject data files not from E-Prime that you need to merge. Or you may want to merge data files from multiple tasks into one big merged file. My datawrangling package contains two functions to merge data files together: files_join() files_bind() They both work in a similar way. The files you want to merge need to be in the same folder on your computer. You specify the location of this folder using the path = argument. You need to specify a pattern that uniquely identifies the files you want to merge (e.g. “.txt”, or “Flanker”) using the pattern = argument. Then specify the directory and file name you want to save the merge file to using the output.file = argument. If you do not specify output.file then it will not save the file to your computer but it will import the files as a single merged data frame in your R environment. Here are the arguments that can be specified: path: Folder location of files to be merged pattern: Pattern to identify files to be merged delim: Delimiter used in files. output.delim: Delimiter to be used in output file. Default is , (csv) na: How are missing values defined in files to be merged. Default is NA output.file: File name and path to be saved to. id: Subject ID column name. ONLY for files_join() For example: library(datawrangling) files_bind(&quot;filepath/data/subj&quot;, pattern = &quot;Flanker&quot;, delim = &quot;\\t&quot;, output.file = &quot;filepath/data/filename_merged.csv&quot;) This will bind any files in the directory filepath/data/subj that contain the string \"Flanker\" and output the merged data to a file called filename_merged.csv to the directory filepath/data. "],["data-manipulation-using-dplyr.html", "6 Data Manipulation using dplyr 6.1 Setup 6.2 Import 6.3 rename() 6.4 filter() 6.5 select() 6.6 mutate() 6.7 case_when() 6.8 group_by() 6.9 summarise() 6.10 pivot_wider() 6.11 pivot_longer() 6.12 Pipe Operator %&gt;%", " 6 Data Manipulation using dplyr In this Chapter you will learn the fundamentals of data manipulation in R. In the Getting Started in R section you learned about the various types of objects in R. The most important object you will be using is the dataframe. Last Chapter you learned how to import data files into R as dataframes. Now you will learn how to do stuff to that data frame using the dplyr package (which is of course part of the tidyverse) dplyr is one of the most useful packages in R. It uses a Grammar of Data Manipulation that is intuitive and easy to learn. The language of dplyr will be the underlying framework for how you will think about manipulating a dataframe. Not only is the language of dplyr intuitive but it allows you to perform data manipulations all within the dataframe itself, without having to create external variables, lists, for loops, etc. It can be tempting to hold information outside of a data frame but in general I suggest avoiding this strategy. Instead, hold the information in a new column within the data frame itself. For example: A common strategy I see any many R scripts is to hold the mean or count of a column of values outside the dataframe and in a new variable in the Environment. data &lt;- data.frame(x = c(1,6,4,3,7,5,8,4), y = c(2,3,2,1,4,6,4,3)) x_mean &lt;- mean(data$x) This variable then could be used to subtract out the mean from the values in column y data &lt;- mutate(data, y_new = y - x_mean) head(data) ## x y y_new ## 1 1 2 -2.75 ## 2 6 3 -1.75 ## 3 4 2 -2.75 ## 4 3 1 -3.75 ## 5 7 4 -0.75 ## 6 5 6 1.25 mutate() is a dplyr function you will learn about in a second. In general, I would advise against this strategy. A better strategy is to do all this without leaving the data frame data. data &lt;- data.frame(x = c(1,6,4,3,7,5,8,4), y = c(2,3,2,1,4,6,4,3)) data &lt;- mutate(data, x_mean = mean(x), y_new = y - x_mean) head(data) ## x y x_mean y_new ## 1 1 2 4.75 -2.75 ## 2 6 3 4.75 -1.75 ## 3 4 2 4.75 -2.75 ## 4 3 1 4.75 -3.75 ## 5 7 4 4.75 -0.75 ## 6 5 6 4.75 1.25 It can tempting to also think about writing for loops in your R script, but honestly for the most part for loops are avoidable thanks to a dplyr function called group_by(). The only time I end up needing a for loop is when importing a long list of files, or when creating code to put into a function. dplyr uses intuitive language that you are already familiar with. As with any R function, you can think of functions in the dplyr package as verbs - that refer to performing a particular action on a data frame. The core dplyr functions are: rename() renames columns filter() filters rows based on their values in specified columns select() selects (or removes) columns mutate() creates new columns based on transformation from other columns, edits values within existing columns group_by() splits data frame into separate groups based on specified columns summarise() aggregates across rows to create a summary statistic (means, standard deviations, etc.) For more information on these functions Visit the dplyr webpage If you have not done so already, install the dplyr package install.packages(&quot;dplyr&quot;) You will also need the tidyr package (a tidyverse package) for this Chapter install.packages(&quot;tidyr&quot;) Save a new R script file as 5_dplyr.R For this Chapter we will use an example data set from the Flanker task. This data set is a tidy raw data file for over 100 subjects on the Flanker task. There is one row per Trial per Subject and there is RT and Accuracy data on each Trial. Each Trial is either congruent or incongruent. What we will want to do is calculate a FlankerEffect for each Subject so that we end up with one score for each Subject. Go ahead and download the example data set and save it wherever you wish. We will talk about how to organize your data and R scripts in section III. Workflow. Download Example Tidyverse Data 6.1 Setup At the top of your script load the three packages you will need for this Chapter ## Setup library(readr) library(dplyr) library(tidyr) Notice how I added a commented line at the top. Adding comments to your scripts is highly advisable, as it will help you understand your scripts when you come back to them after not working on them for a while. You only need to add a single # to create a commented line. You will also notice that it printed out some warning messages. Sometimes different packages have the same function names. So when you load a package it may override or mask functions from other packages that are already loaded. 6.2 Import Import the data file you downloaded. Refer to Chapter 5 for importing data into R. import &lt;- read_csv(&quot;Data Files/tidyverse_example.csv&quot;) It is always a good idea to get to know your dataframe before you start messing with it. What are the column names? What kind of values are stored in each column? How many observations are there? How many Subjects? How many Trials? etc. What are the column names? use colnames() for a quick glance at the column names colnames(import) ## [1] &quot;Subject&quot; &quot;TrialProc&quot; &quot;Trial&quot; &quot;Condition&quot; &quot;RT&quot; &quot;ACC&quot; ## [7] &quot;Response&quot; &quot;TargetArrowDirection&quot; &quot;SessionDate&quot; &quot;SessionTime&quot; To take a quick look at the first few rows of a dataframe use head(). head(import) ## # A tibble: 6 x 10 ## Subject TrialProc Trial Condition RT ACC Response TargetArrowDirection SessionDate SessionTime ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; ## 1 14000 practice 1 incongruent 1086 1 left left 08-30-2017 10:30:25 ## 2 14000 practice 2 incongruent 863 1 left left 08-30-2017 10:30:25 ## 3 14000 practice 3 congruent 488 1 right right 08-30-2017 10:30:25 ## 4 14000 practice 4 incongruent 588 1 right right 08-30-2017 10:30:25 ## 5 14000 practice 5 congruent 581 1 right right 08-30-2017 10:30:25 ## 6 14000 practice 6 incongruent 544 1 right right 08-30-2017 10:30:25 This gives you a good idea of what column names you will be working with and what kind of values they contain. To evaluate what are all the unique values in a column you can use unique(). You can also use this in combination with length() to evaluate how many unique values are in a column. unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; unique(import$Trial) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## [36] 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 ## [71] 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 ## [106] 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 ## [141] 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 ## [176] 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 max(import$Trial) ## [1] 192 length(unique(import$Subject)) ## [1] 410 unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$ACC) ## [1] 1 0 All these functions we just used from colnames() to unique() were to temporarily evaluate our data. They are not required to perform the actual data analysis. Therefore, I usually just type these in the console. A general rule of thumb is that if it is not required to be saved in your Script file then just type it in the console. Okay let’s take a look at how to use the dplyr functions to score this data. 6.3 rename() We do not really need to, but let’s go ahead and rename() a column. How about instead of ACC let’s label it as Accuracy. Pretty simple data &lt;- rename(import, Accuracy = ACC) rename() is really only useful if you are not also using select() or mutate(). In select() you can also rename columns as you select them to keep. This will be illustrated this later Notice that I passed the output of this function to a new object data. I like to keep the object import as the original imported file and any changes will be passed onto a new data frame, such as data. This makes it easy to go back and see what the original data is. Because if we were to overwrite import then we would have to execute the read_csv() import function again to be able to see the original data file, just a little more tedious. 6.4 filter() filter() is an inclusive filter and requires the use of logical statements. In Chapter 2: Basic R I talked a little bit about logical statements. Here is a list of logical operators in R: In addition to these logical operators, these functions can be used infilter(): is.na() - include if missing !is.na() - include if not missing between() - values that are between a certain range of numbers near() - values that are near a certain value We do not want to include practice trials when calculating the mean on RTs. We will use filter() to remove these rows. First let’s evaluate the values in these columns unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; We can specify our filter() in a couple of different ways data &lt;- filter(data, TrialProc != &quot;practice&quot;, Condition != &quot;neutral&quot;) or data &lt;- filter(import, TrialProc == &quot;real&quot;, Condition == &quot;congruent&quot; | Condition == &quot;incongruent&quot;) Specifying multiple arguments separated by a comma , in filter() is equivalent to an &amp; (and) statement. In the second option, since there are two types of rows on Condition that we want to keep we need to specify Condition == twice, separated by | (or). We want to keep rows where Condition == \"congruent\" or Condition == \"incongruent\" Notice that the arguments have been separated on different lines. This is okay to do and makes it easier to read the code. Just make sure the end of the line still has a comma. Go ahead and view data. Did it properly remove practice trials? How about neutral trials? unique(data$TrialProc) ## [1] &quot;real&quot; unique(data$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; Again you should type these in the console NOT in the R Script! There is a lot of consistency of how you specify arguments in the dplyr package. You always first specify the data frame that the function is being performed on, followed by the arguments for that function. Column names can be called just like regular R objects, that is without putting the column name in \" \" like you do with strings. If all you know is dplyr, then this might not seem like anything special but it is. Most non-tidyverse functions will require you to put \" \" around column names. 6.5 select() select() allows you to select which columns to keep and/or remove. Let’s keep Subject, Condition, RT, Trial, and Accuracy and remove TrialProc, TargetArrowDirection, SessionDate, and SessionTime. select() is actually quite versatile - you can remove columns by specifying certain patterns. I will only cover a couple here, but to learn more Visit the select() webpage We could just simply select all the columns we want to keep data &lt;- select(data, Subject, Condition, RT, Trial, Accuracy) alternatively we can specify which columns we want to remove by placing a - in front of the columns data &lt;- select(data, -TrialProc, -TargetArrowDirection, -SessionDate, -SessionTime) or we can remove (or keep) columns based on a pattern. For instance SessionDate and SessionTime both start with Session data &lt;- select(data, -TrialProc, -TargetArrowDirection, -starts_with(&quot;Session&quot;)) You might start realizing that there is always more than one way to perform the same operation. It is good to be aware of all the ways you can use a function because there might be certain scenarios where it is better or even required to use one method over another. In this example, you only need to know the most straightforward method of simply selecting which columns to keep. You can also rename variables as you select() them… let’s change Accuracy back to ACC… just because we are crazy! data &lt;- select(data, Subject, Condition, RT, Trial, ACC = Accuracy) We are keeping Subject, Condition, RT, Trial, and renaming ACC to Accuracy. 6.6 mutate() mutate() is a very powerful function. It basically allows you to do any computation or transformation on the values in the data frame. You can change the values in already existing columns create new columns based on transformation of other columns 6.6.1 Changing values in an existing column Reaction times that are less than 200 milliseconds most likely do not reflect actual processing of the task. Therefore, it would be a good idea to not include these when calculating means. What we are going to do is is set any RTs that are less than 200 milliseconds to missing, NA. First let’s make sure we even have trials that are less than 200 milliseconds. Two ways to do this. 1) View the data frame and click on the RT column to sort by RT. You can see there are RTs that are as small as 1 millisecond! Oh my, that is definitely not a real reaction time. 2) you can just evaluate the minimum value in the RT column: min(data$RT) ## [1] 0 Now lets mutate() data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT)) Since we are replacing values in an already existing column we can just specify that column name, RT = followed by the transformation. Here we need to specify an if…then… else statement. To do so within the mutate() function we use the function called ifelse(). ifelse() evaluates a logical statement specified in the first argument, RT &lt; 200. mutate() works on a row-by-row basis. So for each row it will evaluate whether RT is less than 200. If this logical statement is TRUE then it will perform the next argument, in this case sets RT = NA. If the logical statement is FALSE then it will perform the last argument, in this case sets RT = RT (leaves the value unchanged). 6.6.2 Creating a new column Let’s say for whatever reason we want to calculate the difference between the RT on a trial minus the overall grand mean RT (for now, across all subjects and all trials). This is not necessary for what we want in the end but what the heck, let’s be a little crazy. (I just need a good example to illustrate what mutate() can do.) So first we will want to calculate a “grand” mean RT. We can use the mean() function to calculate a mean. mean(data$RT, na.rm = TRUE) ## [1] 529.1414 Since we replaced some of the RT values with NA we need to make sure we specify in the mean() function to remove NAs by setting na.rm = TRUE. We can use the mean() function inside of a mutate() function. Let’s put this “grand” mean in a column labeled grandRT. First take note of how many columns there are in data ncol(data) ## [1] 5 So after calculating the grandRT we should expect there to be one additional column for a total of 6 columns data &lt;- mutate(data, grandRT = mean(RT, na.rm=TRUE)) Cool! Now let’s calculate another column that is the difference between RT and grandRT. data &lt;- mutate(data, RTdiff = RT - grandRT) We can put all these mutate()s into one mutate() data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) Notice how I put each one on a separate line. This is just for ease of reading and so the line doesn’t extend too far off the page. Just make sure the commas are still there at the end of each line. 6.7 case_when() Often times you will want to mutate() values conditionally based on values in other columns. There are two functions that will help you do this, ifelse() and case_when(). ifelse() is a base R function and case_when() is a dplyr function. ifelse() takes the format: ifelse(conditional argument, value if TRUE, value if FALSE) As an example, lets say we want to code a new variable that indicates whether the reaction time on a trial met a certain response deadline or not. Let’s call this column Met_ResponseDeadline and give a value of 1 to trials that met the deadline and 0 to trials that did not meet the deadline. Let’s set the response deadline at a reaction time of 500 milliseconds. The conditional argument will take the form: RT is less than or equal to 500. If this statement is TRUE, then we will assign a value of 1 to the column Met_ResponseDeadline. If this statement is FALSE, then we will assign a value of 0 to the column Met_ResponseDeadline. The code looks like: data &lt;- import %&gt;% mutate(Met_ResponseDeadline = ifelse(RT &lt;= 500, 1, 0)) Check out data to make sure it worked. You can even combine multiple ifelse() statements into one. Let’s say we actually want to recode the column ACC to reflect not just correct and incorrect response but also whether they met the response deadline or not. That is, a value of 1 will represent responses that were correct AND met the response deadline and values of 0 represent responses that were either incorrect, did not meet the response deadline, or both. data &lt;- import %&gt;% mutate(ACC = ifelse(ACC == 1, ifelse(RT &lt;= 500, 1, 0), 0)) The arguments for the first ifelse() are as follows: Accuracy is equal to 1. If TRUE, then second ifelse() statement. If FALSE, then 0. This makes sense because if the accuracy is 0 (incorrect), then the value needs to remain 0. However, if the accuracy is 1, the value will depend on whether the reaction time is less than 500 (thus the second ifelse()). If accuracy is equal to 1, then if reaction time is less than or equal to 500, then set accuracy to 1. If FALSE, then set accuracy to 0. Know that you can place the additional ifelse() statement in either the TRUE or FALSE argument and can keep iterating on ifelse() statements for as long as you need (however that can get pretty complicated). case_when() is an alternative to an ifelse(). Anytime you need multiple ifelse() statements case_when() tends to simplify the code and logic involved. Let’s see examples of the two examples provided for ifelse() as a comparison. data &lt;- import %&gt;% mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1, RT &gt; 500 ~ 0)) Notice that the notation is quite different here. Each argument contains the format: conditional statement followed by the symbol ~ (this should be read as “then set as”) and then a value to be assigned when the conditional statement is TRUE. There is no value to specify when it is FALSE. Therefore, it is important when using the case_when() function to either 1) include enough TRUE statement arguments to cover ALL possible values or 2) use the uncharacteristically non-intuitive notation - TRUE ~ \"some value\". In the example above, all possible RT values are included in the two arguments RT &lt;= 500 and RT &gt; 500. To provide an example of the second option: data &lt;- import %&gt;% mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1, TRUE ~ 0)) The case_when() function will evaluate each argument in sequential order. So when it gets to the last argument (and this should always be the last argument), this is basically saying, when it is TRUE that none of the above arguments were TRUE (hence why this argument is being evaluated) then (~) set the value to “some value” (whatever value you want to specify). Now this function gets a little more complicated if you want to set values to NA. NA values are technically logical values like TRUE or FALSE. The values in a column can only be of one type; numerical, character, logical, etc. Therefore, if you have numerical values in a column but want to set some to NA, then this becomes an issue when using case_when() (hopefully this will be fixed in future updates to dplyr). For now, how to get around this is changing the type of value that NA is. For instance; as.numeric(NA), as.character(NA). data &lt;- import %&gt;% mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1, RT &gt; 500 ~ 0, TRUE ~ as.numeric(NA))) Now on to the example in which we used two ifelse() statements. data &lt;- import %&gt;% mutate(ACC = case_when(ACC == 1 &amp; RT &lt;= 500 ~ 1, ACC == 1 &amp; RT &gt; 500 ~ 0, ACC == 0 ~ 0)) When you have multiple ifelse() statements case_when() becomes easier to read. Compare this use of case_when() with the equivalent ifelse() above. The case_when() function makes it very explicit what is happening. There are three conditional statements, therefore three categories of responses. A correct response and reaction time that meets the deadline. A correct response and reaction time that DOES NOT meet the deadline. An incorrect response These three options cover all possible combinations between the the two columns ACC and RT. Accuracy should only be set to 1 (correct) with the first option and that is made quite clearly because it is the only one with ~ 1. This is not as obvious in the ifelse() example. Let’s move on to the next dplyr function. 6.8 group_by() This function is very handy if we want to perform functions separately on different groups or splits of the data frame. For instance, maybe instead of calculating an overall “grand” mean we want to calculate a “grand” mean for each Subject separately. Instead of manually breaking the data frame up by Subject, the group_by() function does this automatically in the background. Like this… data &lt;- group_by(data, Subject) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) You will now notice that each subject has a different grandRT, simply because we specified group_by(data, Subject). Let’s say we want to do it not just grouped by Subject, but also Condition. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) group_by() does not only work on mutate() - it will work on any other functions you specify after group_by(). Therefore, it can essentially replace most uses of for loops. I suggest exercising caution when using group_by() because the grouping will be maintained until you specify a different group_by() or until you ungroup it using ungroup(). So I always like to ungroup() immediately after I am done with it. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) data &lt;- ungroup(data) 6.9 summarise() The summarise() function will reduce a data frame by summarizing values in one or multiple columns. The values will be summarised on some statistical value, such as a mean, median, or standard deviation. Remember that in order to calculate the FlankerEffect for each subject, we first need to calculate each subject’s mean RT on incongruent trials and their mean RT on congruent trials We’ve done our filtering, selecting, mutating, now let’s aggregate RTs across Condition to calculate mean RT. We will use a combo of group_by() and summarise(). summarise() is almost always used in conjunction with group_by(). Let’s also summarise the mean accuracy across conditions. data &lt;- group_by(data, Subject, Condition) data &lt;- summarise(data, RT.mean = mean(RT, na.rm = TRUE), ACC.mean = mean(ACC, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;Subject&#39;. You can override using the `.groups` argument. data &lt;- ungroup(data) To summarise() you need to create new column names that will contain the aggregate values. RT.mean seems to make sense to me. What does the resulting data frame look like? There should be three rows per subject, one for incongruent trials, one for congruent trials, and one for neutral trials. You can see that we now have mean RTs on all conditions for each subject. Also, notice how non-group_by columns got removed: Trial, and ACC. 6.10 pivot_wider() Our data frame now looks like head(data) ## # A tibble: 6 x 4 ## Subject Condition RT.mean ACC.mean ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 congruent 401. 0.931 ## 2 14000 incongruent 510. 0.574 ## 3 14001 congruent 392. 0.980 ## 4 14001 incongruent 423. 0.852 ## 5 14002 congruent 462. 0.765 ## 6 14002 incongruent 536. 0.463 Ultimately, we want to have one row per subject and to calculate the difference in mean RT between incongruent and congruent conditions. It is easier to calculate the difference between two values when they are in the same row. Currently, the mean RT for each condition is on a different row. What we need to do is reshape the data frame. To do so we will use the pivot_wider() function from the tidyr package. The tidyr package, like readr and dplyr, is from the tidyverse set of packages. The pivot_wider() function will convert a long data frame to a wide data frame. In other words, it will spread values on different rows across different columns. In our example, what we want to do is pivot_wider() the mean RT values for the two conditions across different columns. So we will end up with is one row per subject and one column for each condition. Rather than incongruent, and congruent trials being represented down rows we are spreading them across columns (widening the data frame). The three main arguments to specify in pivot_wider() are id_cols: The column names that uniquely identifies (e.g. “Subject”) each observation and that you want to be retained when reshaping the data frame. names_from: The column name that contains the variables to create new columns by (e.g. “Condition”). The values in this column will become Column names in the wider data format values_from: The column name that contains the values (e.g. “RT”). data_wide &lt;- pivot_wider(data, id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = &quot;RT.mean&quot;) Now our data frame looks like head(data_wide) ## # A tibble: 6 x 3 ## Subject congruent incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. ## 2 14001 392. 423. ## 3 14002 462. 536. ## 4 14003 567. 679. ## 5 14004 548. 655. ## 6 14005 472. 559. Notice that the ACC.mean column and values were dropped. To add more transparency to our data frame it would be a good idea to label what values the “congruent” and “incongruent” columns contain. You can do this with the optional names_prefix argument. For instance: data_wide &lt;- pivot_wider(data, id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = &quot;RT.mean&quot;, names_prefix = &quot;RT_&quot;) head(data_wide) ## # A tibble: 6 x 3 ## Subject RT_congruent RT_incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. ## 2 14001 392. 423. ## 3 14002 462. 536. ## 4 14003 567. 679. ## 5 14004 548. 655. ## 6 14005 472. 559. Now a stranger (or a future YOU) will be able to look at this data frame and immediately know that reaction time values are contained in these columns. From here it is pretty easy, we just need to create a new column that is the difference between incongruent and congruent columns. We can use the mutate() function to do this data_wide &lt;- mutate(data_wide, FlankerEffect_RT = RT_incongruent - RT_congruent) head(data_wide) ## # A tibble: 6 x 4 ## Subject RT_congruent RT_incongruent FlankerEffect_RT ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. 109. ## 2 14001 392. 423. 31.3 ## 3 14002 462. 536. 74.0 ## 4 14003 567. 679. 113. ## 5 14004 548. 655. 107. ## 6 14005 472. 559. 87.1 Perfect! Using the readr, dplyr, and tidyr packages we have gone from a “tidy” raw data file to a data frame with one row per subject and a column of FlankerEffect scores. What if we have multiple columns we want to get id_cols, names_from, or values_from? pivot_wider() allows for this very easily. For instance: data_wide &lt;- pivot_wider(data, id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = c(&quot;RT.mean&quot;, &quot;ACC.mean&quot;)) head(data_wide) ## # A tibble: 6 x 5 ## Subject RT.mean_congruent RT.mean_incongruent ACC.mean_congruent ACC.mean_incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. 0.931 0.574 ## 2 14001 392. 423. 0.980 0.852 ## 3 14002 462. 536. 0.765 0.463 ## 4 14003 567. 679. 0.294 0.0926 ## 5 14004 548. 655. 0.5 0.0370 ## 6 14005 472. 559. 0.686 0.204 Now you can see that we have four columns corresponding to reaction times and accuracy values across the two conditions. You can use the same notation c() if you want to use multiple column for id_cols, names_from, values_from. Now we can calculate a FlankerEffect for both RT and Accuracy values data_wide &lt;- mutate(data_wide, FlankerEffect_RT = RT.mean_incongruent - RT.mean_congruent, FlankerEffect_ACC = ACC.mean_incongruent - ACC.mean_congruent) 6.11 pivot_longer() For our goal with this data set, we do not need to switch back to a longer data format, however reshaping your data to a longer format may be something you want to do one day. Let’s try to reshape the data_wide back to a long format that we originally started with. When you have multiple value columns this is not as intuitive as pivot_wider(). To see more documentation and examples use ?tidyr::pivot_longer(). data_long &lt;- pivot_longer(data_wide, contains(&quot;mean&quot;), names_to = c(&quot;.value&quot;, &quot;Condition&quot;), names_sep = &quot;_&quot;) 6.12 Pipe Operator %&gt;% One last thing about the dplyr package. dplyr allows for passing the output from one function to another using what is called a pipe operator. The pipe operator is: %&gt;% This makes code more concise, easier to read, and easier to edit. When you pass the output of one function to another with %&gt;% you do not need to specify the data frame (input) on the next function. %&gt;% implies that the input is the output from the previous function, so this is made implicit. We can pipe all the functions in the chapter together as such ## Setup library(readr) library(dplyr) library(tidyr) ## Import import &lt;- read_csv(&quot;Data Files/tidyverse_example.csv&quot;) ## Score data &lt;- import %&gt;% rename(Accuracy = ACC) %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% select(Subject, Condition, RT, Trial, ACC = Accuracy) %&gt;% group_by(Subject, Condition) %&gt;% mutate(RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), ACC.mean = mean(ACC, na.rm = TRUE)) %&gt;% ungroup() %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = c(&quot;RT.mean&quot;, &quot;ACC.mean&quot;)) %&gt;% mutate(FlankerEffect_RT = RT.mean_incongruent - RT.mean_congruent, FlankerEffect_ACC = ACC.mean_incongruent - ACC.mean_congruent) Virtually all the R scripts you write will require the dplyr package. The more you know what it can do, the easier it will be for you to write R Scripts. I highly suggest checking out these introductions to dplyr. https://dplyr.tidyverse.org https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html "],["common-data-manipulations.html", "7 Common Data Manipulations 7.1 Descriptive Statistics 7.2 Centering and Standardizing Variables 7.3 Trimming 7.4 Composites 7.5 Scale Transformations 7.6 Custom Transformations 7.7 Row-wise Computations 7.8 Remove subjects with too many missing values", " 7 Common Data Manipulations In R, the term data wrangling is often times used to refer to performing data manipulation and transformations. Some of the functions you will learn about in this Chapter come from the datawrangling package I developed. There are certain data transformations we use on a regular basis that would require several steps and lines of code to do. datawrangling allows you to perform these transformation in a single line of code. I am hosting the datawrangling package on GitHub. To download packages on GitHub you first need to download the devtools package. install.packages(&quot;devtools&quot;) Now install the datawrangling package: devtools::install_github(&quot;dr-JT/datawrangling&quot;) Save a new R script file as 6_transform.R For this Chapter, let’s create a data frame to use as an example for common data manipulations using datawrangling. Don’t worry about what this code means for now, just copy it into your script and run it. import &lt;- data.frame(ID = c(1:100), Group = rep(c(1:2), 50), Score1 = rnorm(100, mean = 2, sd = .8), Score2 = rnorm(100, mean = 7, sd = 1.1), Score3 = rnorm(100, mean = 10, sd = 1.8), Score4 = rnorm(100, mean = 20, sd = 2.3)) head(import) ## ID Group Score1 Score2 Score3 Score4 ## 1 1 1 2.108583 6.909211 9.569533 21.10629 ## 2 2 2 1.729564 6.348592 9.488073 17.62381 ## 3 3 1 1.168912 8.357614 12.028537 21.40660 ## 4 4 2 1.612341 5.694310 11.830850 22.75000 ## 5 5 1 3.006348 6.261088 14.137273 20.82632 ## 6 6 2 2.751853 6.430017 12.013493 21.24679 7.1 Descriptive Statistics First you should know how to compute some basic descriptive statistics. Basic descriptive statistics include mean, median, standard deviation, max, min, skew, kurtosis, etc… The functions to calculate these are pretty straightforward: Base R maximum: max() minimum: min() count:n() mean: mean() median: median() standard deviation: sd() variance: var() quantiles (percentiles): quantile() specify the percentiles with the argument probs = (default is c(0, .25, .5, .75, 1)) From the e1071 package skewness: skewness(variable, na.rm = TRUE, type = 2) kurtosis: kurtosis(variable, na.rm = TRUE, type = 2) For all of these you need to specify na.rm = TRUE if the variable column has missing data. It is best to just always set na.rm = TRUE. For example, mean(variable, na.rm = TRUE) To calculate the overall mean on Score1 would look like library(dplyr) data &lt;- import %&gt;% mutate(Score1.mean = mean(Score1, na.rm = TRUE)) 7.2 Centering and Standardizing Variables The function datawrangling::center() will create either unstandardized or standardized (z-scored) centered variables. The list of arguments that can be passed onto the function are: x: data frame variables: c() of columns to center standardize: Logical. Do you want to calculate zscores? (Default = FALSE) Example: library(datawrangling) data &lt;- center(import, variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), standardize = TRUE) View the data frame data. You will notice that there are now 4 additional columns: Score1_z, Score2_z, Score3_z, and Score4_z. If you choose to to calculate centered (unstandardized) scores, then standardize = FALSE. And it will create variables with the suffix _c. This can be combined with a group_by() to calculate standardized values within each group separately. data &lt;- import %&gt;% group_by(Group) %&gt;% center(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), standardize = TRUE) %&gt;% ungroup() 7.3 Trimming The function datawrangling::trim() will replace outlier scores that exceed a certain z-score cutoff. There are several options for how to replace the outlier scores. Replace with “NA” (missing value) “cutoff” (the z-score cutoff value, e.g. 3.5 SDs) “mean” “median” The arguments that can be specified are: x: data frame variables: c() of variables to be trimmed. option to set variables = \"all\" to trim all variables in a data frame. But then must specify id = cutoff: z-score cutoff to use for trimming (default: 3.5) replace: What value should the outlier values be replaced with. (default: replace = “NA”) id: Column name that contains subject IDs. **ONLY needs to be used if variables = \"all\" Example: data &lt;- import %&gt;% trim(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), cutoff = 3.5, replace = &quot;NA&quot;, id = &quot;ID&quot;) Notice how you don’t even need to center() the variables first. The centering is being done inside of trim(). You can evaluate outliers and replace with different values (replace =) all in one function and one line of code. 7.4 Composites The datawrangling::composite() function allows you to easily create a composite score from multiple variables and also specify a certain criteria for how many missing values are allowed. data &lt;- import %&gt;% composite(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;), name = &quot;Score_comp&quot;, type = &quot;mean&quot;, standardize = TRUE, missing.allowed = 1) The function composite() will create composite scores out of specified columns. Right now you can only create “mean” composite scores. In the future I plan on adding “sum” and “factor score” composite types. Here is a list of the arguments you can specify: x: data frame variables: c() of columns to create the composite from name: Name of the new composite variable to be created type: What type of composite should be calculated?, i.e. mean or sum. (Default = “mean”). standardize: Logical. Do you want to calculate the composite based on standardized (z-score) values? (Default = TRUE) missing.allowed: Criteria for the number of variables that can having missing values and still calculate a composite for that subject The remaining functions do not come from the datawrangling package but you may find them useful nonetheless. 7.5 Scale Transformations 7.5.1 polynomial You can create orthogonal polynomials of variables using the poly() function and specify the degree of polynomial to go up to with degree = poly(import$Score1, degree = 3) You can see it creates up to three degrees of polynomials on the Score1 variable. The first degree is a linear, second is a quadratic, and third is cubic. Let’s say we want to create three new columns with each of these three polynomials. To do so we need to individually access each vector such as poly(import$Score1, degree = 3)[,1] library(dplyr) data &lt;- import %&gt;% mutate(Score1.linear = poly(Score1, degree = 3)[ , 1], Score1.quadratic = poly(Score1, degree = 3)[ , 2], Score1.cubic = poly(Score1, degree = 3)[ , 3]) Here is plot to show you visually what happened 7.6 Custom Transformations In general, with mutate() you can specify any custom transformation you want to do on a variable. For instance, if you want to subtract each score by 5, and divide by 10 then you can do it! I don’t know why you would ever want to do that, but you can. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 - 5) / 10) Or take the sum of Score1 and Score2 and divide by the difference between Score3 and Score4. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 + Score2) / (Score3 - Score4)) 7.7 Row-wise Computations The examples for Custom Transformations allowed you to calculate a new variable across multiple columns and within a row. For those examples that was no problem. However, sometimes you want to do more complicated computations or use certain functions such as mean() to calculate the average score across multiple columns. The problem is that mean() works across rows and only within one column. How can we get around that? In the near future (May 15th, 2020), dplyr version 1.0.0 will be released and includes a new function rowwise() for this very purpose. Until then we can use a base R function rowMeans() and rowSums(). data &lt;- import %&gt;% mutate(score_mean = rowMeans(data[ ,c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;)])) 7.8 Remove subjects with too many missing values In our lab, when doing latent variable analyses we often times like to remove subjects that have too many missing values on any given latent factor. For instance, we often times have three task indicators for a latent factor. If a subject has missing values for 2 out of 3 of those task indicators (therefore only has one indicator) then we will remove them. But how do go about doing this in R? First let’s modify the import data frame to be more suited for this by adding some missing values import &lt;- import %&gt;% mutate(Score1 = ifelse(Score1 &lt; 1, NA, Score1), Score2 = ifelse(Score2 &lt; 6, NA, Score2), Score3 = ifelse(Score3 &lt; 9, NA, Score3), Score4 = ifelse(Score4 &lt; 16, NA, Score4), Score5 = rnorm(100, mean = 13, sd = 5)) View the data frame and notice how there are quite a few missing values. Some subjects have missing values on multiple columns. The steps to get rid of subjects with too many missing values on a select number of columns are: Create a column with a count of how many columns has missing values for each subject (a row-wise computation) Filter subjects that have a certain criteria of missing values Just to get a sense of how these steps work we can do this in multiple ways. First let’s calculate the total number of missing values across all columns and remove subjects that have missing values on 3 of the columns. data &lt;- import %&gt;% mutate(missing.total = rowSums(is.na(import[,c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;, &quot;Score5&quot;)]))) View the data frame and notice that some subjects have 0, 1, 2, or 3 columns with missing values. unique(data$missing.total) ## [1] 0 1 2 3 Now let’s remove those with 3 or more columns missing. What we will actually specify is to keep those with less than 3 missing columns data &lt;- import %&gt;% mutate(missing.total = rowSums(is.na(import[,c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;, &quot;Score5&quot;)]))) %&gt;% filter(missing.total &lt; 3) Compare the number of rows between import and data nrow(import) ## [1] 100 nrow(data) ## [1] 98 Two subjects were removed. Now let’s calculate the total number of missing values for two different latent factors. Let’s say latent factor 1, latent1, includes the column indicators Score1 and Score2, whereas latent factor 2, latent2, includes the column indicators Score3, Score4, and Score5. Let’s remove subjects that have missing values on 1 out of the 2 columns for latent1 and 2 out of 3 for latent2. data &lt;- import %&gt;% mutate(missing.1 = rowSums(is.na(import[,c(&quot;Score1&quot;, &quot;Score2&quot;)])), missing.2 = rowSums(is.na(import[,c(&quot;Score3&quot;, &quot;Score4&quot;, &quot;Score5&quot;)]))) %&gt;% filter(missing.1 &lt; 1, missing.2 &lt; 2) View and evaluate data. Also compare the number of rows nrow(import) ## [1] 100 nrow(data) ## [1] 70 There are now only 70 subjects, we removed 30 total subjects! Yikes. "],["project-organization.html", "8 Project Organization 8.1 Data Collection vs. Data Analysis 8.2 Data Preparation 8.3 Data Analysis 8.4 workflow package", " 8 Project Organization In the EngleLab, we often conduct large-scale data collection studies in which there are many research projects (with their own research questions and analyses) going on at once. Although many of the tasks will be shared between these research projects, there will be a unique set of tasks for each project. Ideally, for the tasks that are shared between research projects, the scoring methods and data cleaning criteria are the same across projects. In practice, this might not actually be the case but we do try to standardize across projects when we can. In the past, the way we achieved this was sharing a data file that contained the scores for every task across research projects. However, there are some serious issues with this. You cannot calculate reliability with just the task scores, you need the raw data. There is no transparency as to how the scores were calculated. Although whoever created the shared data file might know, this information will get lost over time. This is not good for reproducibility and open science practices. You cannot go back and calculate task scores using different criteria and decisions, again you need the raw data to do that. R provides a convenient way to solve these problems. The solution is to share only the raw data files and R scripts used to score the tasks. Sharing the raw data files allows you to calculate reliability and to go back and make different decisions on how to score the task. It also makes it much easier for someone to go back and use the data from a research project and do reanalyses. Sharing the R scripts allows for transparency in how task scores were calculated. In other words, DO NOT SHARE scored data files from one research project to another. 8.1 Data Collection vs. Data Analysis The organizational structure we use in the lab is to have separate directories for the data collection study and for the various data analysis projects. The Data Collection directory will serve as a central repository for the raw data files from the study. A new Data Analysis directory can be created by copying over the raw data files for the tasks relevant to that research project. This relationship between Data Collection and Data Analysis Repositories are depicted here: It is critical that you copy the raw data files only from the data collection directory to a data analysis directory. You do not copy data files from one data analysis directory to another. You can copy R Scripts from one data analysis directory to another. For instance, if you or someone else already created an R Script (for a separate data analysis project) to score a particular task that you are also using, then there is no problem in copying the R Script. This is because you can reproduce the data files from your R Scripts but not vice-versa. If you copy data files you lose reproducibility and the transparency of how the data file was created. 8.2 Data Preparation This step is performed in the data collection directory. At some point you will need to start analyzing the data. However, you first need to prepare the data so that it is ready to analyze. There are several steps in this process and it can be quite tedious. Nevertheless, undergraduate RAs are trained on how to do most of these steps, so recruit their help. There are also step-by-step instructions for Data Preparation. Once you are ready for Data Preparation you will need to create a Data Files and R Scripts folders in the Data Collection directory. There are two scenarios in which you may need to start processing and analyzing data: Before data collection has finished After data collection has finished For both of these scenarios, you will start with messy raw data files in some file format. Messy raw data files are hard to understand, have poor column and value labels, contain way too many columns and rows, and are just hard to work with. Data preparation is all about getting raw data files that are easy to work with. The end product of the data preparation stage is tidy raw data files. Tidy raw data files are easy to understand, have sensible column and value labels, contain only relevant columns and rows, and are very easy to work with. The Data Preparation stage is only required because the data files created from the E-Prime or other software program are usually not in a format that is easy to use or understand. I am referring to this format as a messy raw data file. Also, there are typically other preparation steps one needs to take before they can start looking at the data. These might include merging individual subject data files and exporting the data to a non-proprietary format so we can import the data into R. The purpose of the data preparation stage is simply to create tidy raw data files from the messy raw data files. Tidy raw data files are easy to use and understand. There will be one row per trial, column labels that are easy to understand (e.g. Trial, Condition, RT, Accuracy, etc.), and values in columns that make sense. If values in a column are categorical, then the category names will be used rather than numerical values. Ideally, someone not involved in the research project should be able to look at a tidy raw data file and understand what each column represents, and what the values in the column refer to. 8.3 Data Analysis Okay, now say you are ready to analyze some data! It is tempting to do your analysis in the original Data Collection directory where the data are already stored. I highly suggest not doing this. You will be mixing up a Data Collection directory with a Data Analysis directory. This distinction is particularly important when we conduct large-scale studies with many data analysis projects for a single data collection study. Instead, you should copy over the tidy raw data files from the data preparation stage to a separate Data Analysis directory. You also might as well create an Archival Backup of the Data Collection directory on some other hard drive. That way you are at less risk of a hard drive crashing and losing all your precious data. In the Data Analysis directory you have three main folders: Data Files R Scripts Results Data Files is where you will store tidy raw data files, scored data files, and a single merged data file ready for statistical analysis. It is advisable to store all your R scripts in one single place. I also like to prefix them with numbers corresponding to the order they need to be executed - that way they will be organized in an easy to find way. Finally, you should create a separate folder to hold all your outputs from statistical analysis and data visualization in a Results folder. There are three main stages to data analysis; 1) scoring, cleaning, and merging data 2) data visualization, and 3) statistical analyses. Data analysis tends to be more cyclic and iterative therefore you may end up going back and forth between these stages. The first stage takes the tidy raw data files from the data preparation stage and converts them into a scored data file, usually by aggregating performance across trials. Data cleaning procedures (such as removing outliers) also occurs during this stage. The format of the scored data file will depend on the type of statistical analysis one plans on performing. The data analysis stages usually occurs in tandem with one another. Visualizing our data, running statistical analyses, visualizing our statistical models, etc. If we have more exploratory data, then based on these visualizations and analyses we may decide that we want to use different scoring or cleaning procedures. Or we want to explore our data to further understand our findings. We may then go back to the scoring and cleaning stage, and on and on. You may have other folders in your Data Analysis directory: Figures Manuscript Presentations Figures is where any image files, that are used in a manuscript or presentations, are stored. You may also have a PowerPoint file stored here. Manuscript is where the manuscript and any drafts for this project are stored. Presentations is where any PowerPoint presentation files related to this project can be stored. These other folders are more optional. 8.4 workflow package I will show you how to automatically create Data Collection and Data Analysis directories using RStudio Projects and my workflow package 8.4.1 Install Install the workflow package # if you do not have devtools installed then do this first install.packages(&quot;devtools&quot;) # install workflow devtools::install_github(&quot;dr-JT/workflow&quot;) 8.4.2 Create a New R Project One of the features this package allows is for you to automatically setup the organization of a Data Collection or Data Analysis project. Navigate to __File -&gt; New Project… -&gt; New Directory And browse until you see the option: Research Study Click on that and you will see a dialogue box like this Here are what the different options mean: Directory Name: This will be the name of the folder for the study Create project as subdirecotry of: Select Browse and choose where the folder (Directory Name) should be located. Repository Type: data collection or data analysis. Depending on which one you choose it will create the corresponding directories and files: Notice that if you choose the data collection repository it will download a generic template for converting “messy” raw data files to “tidy” raw data files. And if you choose the data analysis repository it will download generic templates for creating scored data files from “tidy” raw data files and to merge the Scored data files into one final data file. # of Sessions: How many sessions will the study have? This will create folders in the Tasks folder for each session. For instance, if there will be 4 sessions it will create the the folders “Session 1”, “Session 2”, “Session 3”, and “Session 4”. Obviously this is not needed for a data analysis repository. Other Directories: I talked earlier about some other folders you may want to include in a Data Analysis repository. Well you can automatically add them here. "],["import-and-output-data-files.html", "9 Import and Output Data Files 9.1 RStudio Projects and here", " 9 Import and Output Data Files The previous chapter was more of a high-level overview of the organizational structure and workflow of preparing and analyzing data in R. In the next few chapters we will dive into more of the details for actually creating R scripts at each of the stages of data preparation and analysis. First let’s talk about importing and outputting data files in R. 9.1 RStudio Projects and here You need to be using RStudio Projects for anything you do in R. RStudio Projects allow you to open isolated instances of R and RStudio for each of your projects. In combination with the here package it will also provide a simple and fool proof way of specifying file paths. For instance, I have an .Rproj file saved in my “UseR_Guide” folder. When I use here() it will output a file path to that location. library(here) here() ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Programming/R Books/UseR_Guide&quot; You can then use a relative file path inside of here(). here(&quot;Data Files/Raw Data/flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Programming/R Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; This is equivalent to here(&quot;Data Files&quot;, &quot;Raw Data&quot;, &quot;flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Programming/R Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; I typically like to set the first argument as the relative file path and the second argument as the file name. This visually separates the file path and the file name, making your script easier to read. here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Programming/R Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; You can then use here() directly in import and output functions: import &lt;- read_csv(here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;)) write_csv(data, here(&quot;Data Files/Scored Data&quot;, &quot;flanker_scored.csv&quot;)) Every time you use here() you know that the file path will start at where you have your .Rproj file saved. Instead of messing around with working directories with setwd() or getwd(), just use here() and RStudio Projects. This becomes especially helpful when working with RMarkdown documents. "],["tidy-raw-data.html", "10 Tidy Raw Data 10.1 Raw Script Template 10.2 Filter Rows 10.3 Change Values in Columns 10.4 Keep only a few Columns", " 10 Tidy Raw Data Watch a video tutorial of this chapter! For this chapter you will need to: install the workflow package # if you do not have devtools installed then do this first install.packages(&quot;devtools&quot;) # install workflow devtools::install_github(&quot;dr-JT/workflow&quot;) be familiar with using dplyr (see chapter on Data Manipulation using dplyr) 10.1 Raw Script Template It is easiest to start an R script from a template. You can download an R script template for tidying raw data with workflow::template(rawscript = TRUE) 10.1.1 Setup #### Setup #### ## Load packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Merged&quot; output_dir &lt;- &quot;Data Files&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import_file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ################ Load packages Any packages required for this script are loaded at the top. For this task all we will need are the here, readr, and dplyr packages so we do not need to change anything. Set Import/Output Directories To make this example easier you will not have to actually import/output any files. Set Import/Output Filenames The only line we need to change here is the task &lt;- \"taskname\" to task &lt;- \"VAorient_S\". 10.1.2 Import This section can stay exactly the same. As long as you are using these templates this line of code can always remain untouched. In fact, if you are using these templates, every line of code except task &lt;- \"taskname\" can likely stay exactly the same. #### Import #### data_import &lt;- read_delim(here(import_dir, import_file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ################ 10.1.3 Tidy raw data This is the meat of the script, where the action happens. It will also be different for every task - obviously. I will cover some common steps that you will need to tidy raw data. #### Tidy raw data #### data_raw &lt;- data_import %&gt;% rename() %&gt;% filter() %&gt;% mutate() %&gt;% select() ####################### 10.1.4 Output data No need to change anything here. Isn’t that nice? #### Output #### write_csv(data_raw, here(output_dir, output_file)) ################ rm(list=ls()) 10.2 Filter Rows One of the first things that is useful to do is get rid of rows in the messy data file that you don’t need. For E-Prime data, Procedure[Trial] is usually the column name you need to only keep rows for practice and real trials procedures. Hint: Type colnames(data_import) in the console window to get a read out of all the column names in your data. It is much faster and easier to see column names in the console than navigating the data frame itself. You need to figure out the value names that correspond to the rows you want to keep. Use unique(``Procedure[Trial]``) Let’s say we only want to keep rows that have a value in the Procedure[Trial] column as either TrialProc or PracProc. rename(TrialProc = `Procedure[Trial]`) filter(TrialProc == &quot;TrialProc&quot; | TrialProc == &quot;PracProc&quot;) 10.3 Change Values in Columns You will likely want to change some of the value labels in columns to make more sense and standardize it across tasks. In general, you should avoid numeric labels for categorical data. Instead, you should just use word strings that describe the category intuitively (e.g., “red”, “blue”, “green” instead of 1, 2, 3). Let’s change the TrialProc values so they are more simple and easy to read. mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracProc&quot; ~ &quot;practice&quot;, TRUE ~ as.character(NA))) You may want to do more complex changing of values or creating entirely new columns. See the **Working with Data* section for more details. 10.4 Keep only a few Columns You will also likely want to select only a subset of columns to keep in the tidy raw data file. select(Subject, TrialProc, Trial, Condition, Accuracy, RT, Response, CorrectResponse, AdminTime, SessionDate, SessionTime) "],["score-and-clean-data.html", "11 Score and Clean Data 11.1 Overview 11.2 Score Script Template 11.3 Data Scoring 11.4 Clean Data 11.5 Calculate Reliability", " 11 Score and Clean Data For this chapter you will need to: install the workflow package # if you do not have devtools installed then do this first install.packages(&quot;devtools&quot;) # install workflow devtools::install_github(&quot;dr-JT/workflow&quot;) be familiar with using dplyr (see chapter on Data Manipulation using dplyr) install the psych package install.packages(&quot;psych&quot;) 11.1 Overview Data Analysis requires many steps and decisions that need to be made along the way. Often times the goal is to create a single data file that is ready for statistical analysis. But before we can get to this single data file a couple of steps are involved. Create a scored data file for each task At this stage we also remove any subjects that are suspect or have too poor of performance, and remove univariate outliers. This is also a good place to calculate reliability estimates. Merge the scored data files into a single data file At this stage we also can create composite factors if needed. Hint: You should create a separate project folder for each data analysis project and create an RStudio Project file in that directory. 11.2 Score Script Template It is easiest to start an R script from a template. You can download an R script template for scoring and cleaning data workflow::template(scorescript = TRUE) 11.2.1 Setup Load packages Any packages required for this script are loaded at the top. For this task we will need the here, readr, dplyr packages in addition to tidyr and knitr. Go ahead and add library(tidyr) and library(knitr) to this section. Set Import/Output Directories To make this example easier you will not have to actually import/output any files. Set Import/Output Filenames The only line we need to change here is the task &lt;- \"taskname\" to task &lt;- \"VAorient_S\". Set Data Cleaning Params In this section of the script we can set certain data cleaning criteria to variables. This makes it easy to see what data cleaning criteria were used right at the top of the script rather than having to read through and try to interpret the script. For the visual arrays task we should remove subjects who had low accuracy scores - lets say those with accuracy lower than 3.5 SDs Add acc_criteria &lt;- -3.5 to this section of the script. Optionally it would be a good idea to add a comment about what this criteria is for. #### Setup #### ## Load Packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Raw Data&quot; output_dir &lt;- &quot;Data Files/Scored Data&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output_file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ## Set Data Cleaning Params ############### 11.2.2 Import As long as you are using these templates this first line of code can always remain untouched. The second line filter() should be used to filter out practice trials or anything you defintely do not want to analyze. #### Import #### data_import &lt;- read_csv(here(import_dir, import_file)) %&gt;% filter() ################ 11.3 Data Scoring This is where the action happens. It will also be different for every task - obviously. However, there are a few steps that are pretty common. #### Score Data #### data_scores &lt;- data_import %&gt;% group_by() %&gt;% summarise() #################### 11.3.1 Group by Columns You will almost certainly use this step everytime you score data because you will want to aggregate scores grouped by Subject, Condition, or other variables. In R, you can specify how you want to group the data. Then, any subsequent functions you use will be performed separately for each group. # by subject group_by(Subject) # by subject and condition group_by(Subject, Condition) 11.3.2 Trim Reaction Time To trim reaction times less than 200ms: mutate(RT = ifelse(RT &lt; 200, NA, RT)) To trim reaction times less than 200ms or greater than 10000ms mutate(RT = ifelse(RT &lt; 200 | RT &gt; 10000, NA, RT)) 11.3.3 Summary Statistic Once you group and clean the data, you can calculate a summary statistic such as a mean, median, or standard deviation. To calculate the mean accuracy and reaction time summarise(Accuracy.mean = mean(Accuracy, na.rm = TRUE), RT.mean = mean(RT, na.rm = TRUE)) 11.3.4 Transform Data to Wide If you are grouping by Subject and Condition, then you will likely want to transform the aggregated data into a wide format. This is because summarise() will produce a row for each Condition per Subject. What you might want is a single row per subject, with the conditions spread out across columns. Hint: If you forget how to use a function or what the argument names are then type ?functionName() in the console (e.g. ?pivot_wider()). pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = &quot;Accuracy.mean&quot;) 11.3.5 More Complex Scoring This is an example of how to calculate k scores for the visual arrays task. You can see this is a little more involved. data_scores &lt;- data_import %&gt;% group_by(Subject, SetSize) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR - 1)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;SetSize&quot;, names_prefix = &quot;VA.k_&quot;, values_from = &quot;k&quot;) %&gt;% mutate(VA.k = (VA.k_5 + VA.k_7) / 2) 11.4 Clean Data The next section of the script template is for cleaning the data by removing problematic subjects and/or removing outliers. #### Clean Data #### #################### 11.4.1 Remove Problematic Subjects If the score for the task is not simply an aggregate of accuracy or reaction time, then you may want to evaluate problematic subjects based on their aggregated accuracy. acc_criteria &lt;- -3.5 data_remove &lt;- data_scores %&gt;% mutate(Accuracy.mean_z = scale(Accuracy.mean, center = TRUE, scale = TRUE)) %&gt;% filter(Accuracy.mean_z &lt;= acc_criteria) data_scores &lt;- filter(data_scores, !(Subject %in% data_remove$Subject)) 11.4.2 Remove Outliers Remove outliers based on their final task scores. outlier_criteria &lt;- 3.0 data_outliers &lt;- data_scores %&gt;% mutate(Task_Score_z = scale(Task_Score, center = TRUE, scale = TRUE)) %&gt;% filter(Task_Score_z &gt;= outlier_criteria | Task_Score_z &lt;= -1*outlier_criteria) data_scores &lt;- filter(data_scores, !(Subject %in% data_outliers$Subject)) 11.5 Calculate Reliability #### Calculate Reliability #### ############################### There are two standard ways of calculating reliability: split-half and cronbach’s alpha. 11.5.1 Split-half reliability Here is an example if the task score was an aggregate of accuracy. splithalf &lt;- data_import %&gt;% filter(Subject %in% data_scores$Subject) group_by(Subject) %&gt;% mutate(Split = ifelse(Trial %% 2, &quot;odd&quot;, &quot;even&quot;)) %&gt;% group_by(Subject, Split) %&gt;% summarise(Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;Split&quot;, values_from = &quot;Accuracy.mean&quot;) %&gt;% summarise(r = cor(even, odd)) %&gt;% mutate(r = (2 * r) / (1 + r)) data_scores$Score_splithalf &lt;- splithalf$r 11.5.2 Cronbach’s alpha Here is an example if the task score was an aggregate of accuracy. cronbachalpha &lt;- data_raw %&gt;% filter(Subject %in% data_scores$Subject) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;Trial&quot;, values_from = &quot;Accuracy&quot;) %&gt;% alpha() # from the psych package data_scores$Score_cronbachalpha &lt;- cronbachalpha$total$std.alpha "],["single-merged-file.html", "12 Single Merged File 12.1 Set up 12.2 Import 12.3 Select only important variables 12.4 Output", " 12 Single Merged File It is easiest to start an R script from a template. You can download an R script template for creating a merged file workflow::template(mergescript = TRUE) As with all the other script templates we have used there are 4 sections in the merge script: Setup This is where any required packages are loaded and the import/output file directories and names are set Import files This is where the multiple _Scores.csv files are imported and merged Select only important variables and create composite scores This is where we select only the variables (columns) that we want to use for statistical analysis. We can also rename the variables to be shorter and more concise - this can make statistical analysis easier. This is also where we can create composite scores. Output Finally we need to save the single merged data file. 12.1 Set up This is what the Setup section looks like. We need the datawrangling package for the files_join() function. #### Set up #### ## Load packages library(here) library(datawrangling) # for files_join() library(dplyr) ## Set import/output directories import_dir &lt;- &quot;Data Files/Scored Data&quot; output_dir &lt;- &quot;Data Files&quot; output_file &lt;- &quot;name_of_datafile.csv&quot; ################ 12.2 Import The datawrangling::files_join() function becomes very useful here. This function will import multiple files, that contain the string \"Scores\", located in import_dir and merge them into a single data frame all in one line of code. #### Import Files #### data_import &lt;- files_join(here(import_dir), pattern = &quot;Scores&quot;, id = &quot;Subject&quot;) ###################### 12.3 Select only important variables We can simply use the select() function to keep only the variables we need for statistical analysis and also rename variables. #### Select only important variables #### data_merge &lt;- data_import %&gt;% select() ## Create list of final subjects subj.list &lt;- select(data_merge, Subject) ################################################################# Not shown here, but this would also be the place to create composite variables if needed using datawrangling::composite(). This function was explained in detail in Chapter 7. If you want to exclude subjects that have too much missing data across certain tasks this would be the place to do it. Finally, I think it is a good idea to create a data file that only contains one column - a list of subjects that have made it through to this stage of data cleaning and scoring. 12.4 Output The last thing to do is save data_merge and subj.list. #### Output #### write_csv(data_merge, here(output_dir, output_file)) write_csv(subj.list, here(output_dir, &quot;subjlist_final.csv&quot;)) ################ rm(list=ls()) The organizational structure and workflow for data analysis is depcited here: You start with only the task_raw.csv files located in Data Files/Raw Data, copied over from the Data Collection directory. The 1_task_score.R scripts imports a task_raw.csv file and performs data cleaning and scoring to create a task_Scores.csv file located in Data Files/Scored Data. The 2_merge.R script merges all the task_Scores.csv files together into one Merged_Data.csv located in Data Files. This file is ready for statistical analysis, it will have all the variables you are interested in and univariate outliers removed. The 3_Analysis.Rmd is an R Markdown script document for conducting statistical analyses and data visualization on Merged_Data.csv. The output of this script document is an Analysis.html results output file located in Results "],["a-masterscript.html", "13 A Masterscript", " 13 A Masterscript When you have a lot of R scripts it can become tedious to open up and source each script one at a time. Instead, what you can do is have a masterscript file that executes (or sources) each R script file from one place. The masterscript file essentially just contains lines of code using the source() function. For instance a masterscript in a Data Collection repository might look like: ## fluid intelligence source(&quot;R Scripts/fluid intelligence/rapm_raw.R&quot;) source(&quot;R Scripts/fluid intelligence/lettersets_raw.R&quot;) source(&quot;R Scripts/fluid intelligence/numberseries_raw.R&quot;) ## working memory capacity source(&quot;R Scripts/working memory capacity/symspan_raw.R&quot;) source(&quot;R Scripts/working memory capacity/rotspan_raw.R&quot;) source(&quot;R Scripts/working memory capacity/runletter_raw.R&quot;) source(&quot;R Scripts/working memory capacity/rundigit_raw.R&quot;) source(&quot;R Scripts/working memory capacity/mentalcounters_raw.R&quot;) ## attention control source(&quot;R Scripts/attention control/antisaccade_raw.R&quot;) source(&quot;R Scripts/attention control/sact_raw.R&quot;) source(&quot;R Scripts/attention control/visual_flankerDL_raw.R&quot;) source(&quot;R Scripts/attention control/visual_stroopDL_raw.R&quot;) ## other source(&quot;R Scripts/demographics_raw.R&quot;) This script will source all these R scripts without having to open them. For Data Analysis a masterscript can also help to organize your data analysis procedure and the order in which certain R scripts need to be ran. For instance: ################################################# #------ 1. Scored data ------# ################################################# source(&quot;R Scripts/1_baseline_score.R&quot;) source(&quot;R Scripts/1_gf_score.R&quot;) source(&quot;R Scripts/1_wmc_score.R&quot;) source(&quot;R Scripts/1_antisaccade_score.R&quot;) source(&quot;R Scripts/1_sact_score.R&quot;) source(&quot;R Scripts/1_visualarrays_score.R&quot;) source(&quot;R Scripts/1_questionnaires_score.R&quot;) rm(list = ls()) ############################################################# #------ 2. Create Final Merged Data File for Analysis ------# ############################################################# source(&quot;R Scripts/2_merge.R&quot;, echo = TRUE) rm(list = ls()) ################################ #------ 3. Data Analysis ------# ################################ rmarkdown::render(&quot;R Scripts/3_MainAnalyses.Rmd&quot;, output_dir = &quot;Results&quot;, output_file = &quot;MainAnalyses.html&quot;) rmarkdown::render(&quot;R Scripts/3_Reliabilities.Rmd&quot;, output_dir = &quot;Results&quot;, output_file = &quot;Reliabilities.html&quot;) rmarkdown::render(&quot;R Scripts/3_Demographics.Rmd&quot;, output_dir = &quot;Results&quot;, output_file = &quot;Demographics.html&quot;) rm(list = ls()) ################################################# "],["data-visualization-overview.html", "Data Visualization: Overview R Markdown", " Data Visualization: Overview R Markdown Phew! You’ve made it this far, good job. Up until now you have been learning how to do data preparation steps in R. Now for the fun part, statistical analyses and data visualization! This is the third and final step in the data workflow process depicted above. Traditionally you have likely done these analyses in SPSS or EQS and have created figures in Excel or PowerPoint. The rest of the guide will cover how to do these steps in R. Writing scripts to do statistical analyses is an entirely different process than writing scripts for data preparation. Therefore, we should first go over the general process of conducting and outputting statistical analyses in R. In programs like SPSS when you run a statistical analysis, it will be outputted to a viewable .spv document. One downfall of this is that .spv files are propriety format so can only be opened if you have SPSS installed. However, there is the option to export a .spv file as a PDF. One downfall about R is that unlike SPSS, there is not a native way to create output documents from statistical analyses. Fortunately, RStudio has an output document format called R Markdown. What is an R Markdown File? R Markdown is a powerful way to create reports of statistical analyses. Reports can be outputted in a lot of different formats; html, Microsoft Word, PDF, presentation slides, and more. In fact, this guide was created using R Markdown. The easiest format to output as is html. html documents are opened in a web browser and therefore can be opened on any computer and device (phones, tablets, Windows, Mac). Follow this link for a brief Intro to R Markdown First, you need to install the rmarkdown package install.packages(&quot;rmarkdown&quot;) To open an R Markdown document go to File -&gt; New File -&gt; R Markdown… Select HTML and click OK An example R Markdown document will open. Go ahead and read the contents of the document. There are three types of content in an R Markdown document: A YAML header R code chunks Formatted text 13.0.1 YAML header The YAML header contains metadata about how the document should be rendered and the output format. It is located at the very top of the document and is surrounded by lines of three dashe, --- title: &quot;Title of document&quot; output: html_document --- There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html R code chunks Unlike a typical R script file (.R), an R Markdown document (.Rmd) is a mixture of formatted text and R code chunks. Not everything in an R Markdown document is executed in the R console, only the R code chunks. To run chunks of R code you can click on the green “play” button on the top right of the R code chunk. Go ahead and do this for the three R code chunks in the R Markdown document you opened. (cars and pressure are just example data frames that come pre-loaded with R). We have not gone over these functions yet, but you can see that the results of the R code are now displayed in the document. The first R code chunk is just setting some default options of how the output of R code chunks should be displayed. We will cover these options in more detail later. Formatted text The formatted text sections are more than just adding comments to lines of code. You can write up descriptive reports, create bulletted or numbered lists, embed images or web links, create tables, and more. The text is formatted using a language known as Markdown, hence the name R Markdown. Markdown is a convenient and flexible way to format text. When a Markdown document is rendered into some output (such as html or PDF), the text will be formatted as specified by the Markdown syntax. In the R Markdown document you have open you can see some Markdown syntax. The pound signs ## at the beginning of a line are used to format headers. One # is a level one header, two ## is a level two header and so on. Also notice in the second paragraph, the word Knit is surrounded by two asterisks on each side. When this document is rendered, the word Knit will be bold. Go ahead and render the R Markdown document by clicking on the Knit button at the top of the window. Once it is done rendering you will see a new window pop up. This is the outputted html file. You can see how the document has formatted text based on the Markdown syntax. There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. Here is one I reference often: Markdown Cheatsheet "],["fundamentals-of-data-visualization.html", "14 Fundamentals of Data Visualization 14.1 Grammar of Graphics 14.2 Plotting Functions in R", " 14 Fundamentals of Data Visualization Data visualization is an essential skill for anyone working with data. It is a combination of statistical understanding and design principles. In this way, data visualization is about graphical data analysis and communication and perception. Data visualization is often times glossed over in our stats courses. This is unfortunate because it is so important for better understanding our data, for communicating our results to others, and frankly it is too easy to create poorly designed visualizations. As a scientist, there are two purposes for visualizing our data. Data exploration: it is difficult to fully understand our data just by looking at numbers on a screen arranged in rows and columns. Being skilled in data visualization will help you better understand your data. Explain and Communicate: You will also need to explain and communicate your results to colleagues or in scientific publications. The same data visualization principles apply to both purposes, however for communicating your results you may want to place more emphasis on aesthetics and readability. For data exploration your visualizations do not have to be pretty. 14.1 Grammar of Graphics Leland Wilkinson (Grammar of Graphics, 1999) formalized two main principles in his plotting framework: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings The essential grammatical elements to create any visualization are: 14.2 Plotting Functions in R It is possible to create plots in R using the base R function plot(). The neat thing about plot() is that it is really good at knowing what kind of plot you want without you having to specify. However, these are not easy to customize and the output is a static image not an R object that can be modified. To allow for data visualization that is more in line with the principles for a grammar of graphics, Hadley Wickham (pictured below) created the ggplot2 package. This by far the most popular package for data visualization in R. "],["introduction-to-ggplot2.html", "15 Introduction to ggplot2 15.1 Grammar of Graphics 15.2 Data layer 15.3 Aesthetic Layer 15.4 Geometries Layer 15.5 Facets Layer 15.6 Statistics Layer 15.7 Coordinates Layer 15.8 Themes Layer", " 15 Introduction to ggplot2 15.1 Grammar of Graphics We saw from the last chapter that the two main components in a grammar of graphics are: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings We also saw that the three essential elements are the data layer, aesthetics layer, and geometrics layer. In ggplot2 there are a total of 7 layers we can add to a plot 15.2 Data layer The Data Layer specifies the data being plotted. Let’s see what this means more concretely with an example data set. A very popular data set used for teaching data science is the iris data set. In this data set various species of iris were measured on their sepal and petal length and width. This data set actually comes pre-loaded with R, so you can simply view it by typing in your console View(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa We can see that this data is in wide format. What type of graph we can visualize will depend on the format of the data set. On occasion, in order to visualize a certain pattern of the data will require you to change the formatting of the data. Let’s go ahead and start building our graphical elements in ggplot2. Load the ggplot2 library. Then: library(ggplot2) ggplot(data = iris) You can see that we only have a blank square. This is because we have not added any other layers yet, we have only specified the data layer. 15.3 Aesthetic Layer The next grammatical element is the aesthetic layer, or aes for short. This layer specifies how we want to map our data onto the scales of the plot The aesthetic layer maps variables in our data onto scales in our graphical visualization, such as the x and y coordinates. In ggplot2 the aesthetic layer is specified using the aes() function. Let’s create a plot of the relationship between Sepal.Length and Sepal.Width, putting them on the x and y axis respectively. ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) You can see we went from a blank box to a graph with the variable and scales of Sepal.Length mapped onto the x-axis and Sepal.Width on the y-axis. However, there is no data yet :( What are we to do? 15.4 Geometries Layer The next essential element for data visualization is the geometries layer or geom layer for short. Just to demonstrate to you that ggplot2 is creating R graphic objects that you can modify and not just static images, let’s assign the previous graph with data and aesthetics layers only onto an R object called p, for plot. p &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) Now let’s say we want to add the individual raw data points to create a scatterplot. To do this we can use the function geom_point(). This is a geom layer and the type of geom we want to add are points. In ggplot2 there is a special notation that is similar to the pipe operator %&gt;% seen before. Except it is plus sign + p + geom_point() And walla! Now we have a scatterplot of the relationship between Sepal.Length and Sepal.Width. Cool. If we look at the scatterplot it appears that there are at least two groups or clusters of points. These clusters might represent the different species of flowers, represented in the Species column. There are different ways we can visualize or separate this grouping structure. First, we will consider how to plot these species in separate plots within the same visualization. 15.5 Facets Layer The facet layer allows you to create subplots within the same graphic object The previous three layers are the essential layers. The facet layer is not essential, however given your data you may find it helps you to explore or communicate your data. Let’s create facets of our scatterplot by Species ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) 15.6 Statistics Layer The statistics layer allows you plot statistical values calculated from the data So far we have only plotted the raw data values. However, we may be interested in plotting some statistics or calculated values, such as a regression line, means, standard error bars, etc. Let’s add a regression line to the scatterplot. First without the facet layer then with the facet layer ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) 15.7 Coordinates Layer The coordinate layer allows you to adjust the x and y coordinates You can adjust the min and max values, as well as the major ticks. This is more useful when you have separate graphs (non-faceted) and you want to plot them on the same scale for comparison. This is actually a very important design principle in data visualization. If you want to compare two separate graphs, then they need to be on the same scale!!! library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = c(4, 8), ylim = c(2, 5)) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = c(4, 8), ylim = c(2, 5)) ggplot(filter(iris, Species == &quot;virginica&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = c(4, 8), ylim = c(2, 5)) Just a note. I highly suggest not using scale_x_continuous() or scale_y_continuous() functions. The coord_cartesian() function is like zooming in and out of the plot area. The scale_ functions actually change the shape of the data and statistics layers. If a data point falls outside of the scale limits then it will be removed from any statistical analyses (even if the individual data points are not plotted geom_point()) 15.8 Themes Layer The Themes Layer refers to all non-data ink. You can change the labels of x or y axis, add a plot title, modify a legend title, add text anywhere on the plot, change the background color, axis lines, plot lines, etc. There are three types of elements within the Themes Layer; text, line, and rectangle. Together these three elements can control all the non-data ink in the graph. Underneath these three elements are sub-elements and this can be represented in a hierarchy such as: For instance, you can see that you can control the design of the text for the plot title and legend title theme(title = element_text()) or individually with theme(plot.title = element_text(), legend.title = element_text()). Any text element can be modified with element_text() Any line element can be modified with element_line() Any rect element can be modified with element_rect() You can then control different features such as the color, linetype, size, font family, etc. As an example let’s change some theme elements to our facet plot. Let’s change the axis value labels to red font and increase the size ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text = element_text(color = &quot;red&quot;, size = 14)) Now let’s only change the x-axis text and not the y-axis text. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) It is a good idea to have a consistent theme across all your graphs. And so you might want to just create a theme object that you can add to all your graphs. a_theme &lt;- theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14), panel.grid = element_blank(), panel.background = element_rect(fill = &quot;pink&quot;)) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + a_theme 15.8.1 Built-in Themes For the most part you can probably avoid the theme() function by using built-in themes, unless there is a specific element you want to modify. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + theme_linedraw() You can also set a default theme for the rest of your ggplots at the top of your script. That way you do not have to keep on specifying the theme for evey ggplot. theme_set(theme_linedraw()) Now you can create a ggplot with theme_linedraw() without specifying theme_linedraw() every single time. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) You can do a google search to easily find different types of theme templates. I personally like theme_linedraw() "],["scatterplots.html", "16 Scatterplots 16.1 Scatterplots", " 16 Scatterplots You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) The main type of plots we typically want to create in psychological science are: Scatterplots Bar graphs Line graphs Histograms 16.1 Scatterplots We have already spent a good amount of time creating scatterplots using stat_smooth() and/or geom_smooth(). These two functions are essentially identical. In fact, many of the geom_ functions are just wrappers around stat_ functions. The scatterplot we created from last chapter is essentially an interaction plot. The interaction of Species x Sepal.Length on Sepal.Width. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) For modeling an interaction effect in regression it is easier to interpret if the lines extend to all possible values - not just across the values within a group. We can do this by specifying the argument geom_smooth(fullrange = TRUE) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) Now what if the moderator was a continuous variable and not categorical like Species? We would want to set the color aesthetic to be on +/- 1 SD on the mean. How would we go about doing this? The answer is: It would be very difficult to do so. This is where the function plot_model() from the sjPlot package comes in handy. 16.1.1 Adding other geoms There might be other geoms we want to add to a scatterplot. Let’s add some summary statistics to the graph. Specifically, a horizontal dashed line representing the mean on Sepal.Width and a vertical dashed line representing the mean on Sepal.Length. To make it more simple let’s only do this for Species == \"setosa\". library(dplyr) iris_means &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% mutate(Sepal.Width_mean = mean(Sepal.Width, na.rm = TRUE), Sepal.Length_mean = mean(Sepal.Length, na.rm = TRUE)) ggplot(iris_means, aes(Sepal.Length, Sepal.Width)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + geom_hline(aes(yintercept = Sepal.Width_mean), linetype = &quot;dashed&quot;, color = &quot;red4&quot;) + geom_vline(aes(xintercept = Sepal.Length_mean), linetype = &quot;dashed&quot;, color = &quot;green4&quot;) "],["plotting-means.html", "17 Plotting Means 17.1 Bar Graphs 17.2 Alternatives to Bar Graphs 17.3 Two-Way Interaction Plots 17.4 Three-Way Interaction Plots", " 17 Plotting Means You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) 17.1 Bar Graphs Bar graphs are the standard. They are ubiquitous across psychology. Basically everyone uses them. But in all honesty, Bar graphs SUCK!. The worst part about them is that they hide the distribution of the raw data points (even when error bars are included). Even worse, too often you will see bar graphs with NO ERROR BARS! Yikes! A bar graph with no error bars tells you almost NOTHING! To illustrate this let’s use a data set containing information on mammalian sleep patterns from the data set msleep. head(msleep) ## # A tibble: 6 x 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cheetah Acinonyx carni Carnivora lc 12.1 NA NA 11.9 NA 50 ## 2 Owl monkey Aotus omni Primates &lt;NA&gt; 17 1.8 NA 7 0.0155 0.48 ## 3 Mountain beaver Aplodontia herbi Rodentia nt 14.4 2.4 NA 9.6 NA 1.35 ## 4 Greater short-tailed shrew Blarina omni Soricomorpha lc 14.9 2.3 0.133 9.1 0.00029 0.019 ## 5 Cow Bos herbi Artiodactyla domesticated 4 0.7 0.667 20 0.423 600 ## 6 Three-toed sloth Bradypus herbi Pilosa &lt;NA&gt; 14.4 2.2 0.767 9.6 NA 3.85 Let’s plot the relationship between the different eating habits (vore) and total sleep time (sleep_total). msleep1 &lt;- filter(msleep, !is.na(vore)) ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) This only tells us what the means are. We have no idea about the distributions. Well for this reason people usually like to see error bars. Okay well let’s add error bars. ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2) Okay better. But we still cannot see the underlying distribution. 17.2 Alternatives to Bar Graphs Here is a crazy idea. What if we plotted the raw data points. Like we do with scatterplots! Whoa! What a concept ggplot(msleep1, aes(vore, sleep_total)) + geom_point() When plotting raw data points with categorical variables on the x-axis it makes more sense to jitter the points so they are not all just lying on top of each other. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) Wow! Does this give you a completely different picture than the bar graph with error bars? It does to me! Especially look at the insecti and omni eating habits. There is definitely a bi-modal distribution happening there. From the bar graph with error bars, we might be fooled into thinking that the distributions for carni and omnie are pretty similar. But are they? Not at all! THIS IS WHY YOU SHOULD ALWAYS PLOT THE RAW DATA POINTS But means and error bars are also useful information so let’s add those ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) Another aesthetic option that is useful when we are plotting means and error bars on top of raw data is the alpha aesthetic. This can allow us to make the raw data points more transparent, fade into the background a little more. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) + stat_summary(fun.y = mean, na.rm = TRUE, aes(group = 1), geom = &quot;line&quot;, color = &quot;dodgerblue&quot;, size = .75, shape = &quot;diamond&quot;) 17.3 Two-Way Interaction Plots library(tidyr) iris.long &lt;- iris %&gt;% mutate(Flower = row_number()) %&gt;% gather(&quot;Part&quot;, &quot;Inches&quot;, -Flower, -Species) %&gt;% separate(Part, into = c(&quot;Part&quot;, &quot;Measurement&quot;)) %&gt;% arrange(Flower, Species) %&gt;% select(Flower, Species, Part, Measurement, Inches) head(iris.long) ## Flower Species Part Measurement Inches ## 1 1 setosa Sepal Length 5.1 ## 2 1 setosa Sepal Width 3.5 ## 3 1 setosa Petal Length 1.4 ## 4 1 setosa Petal Width 0.2 ## 5 2 setosa Sepal Length 4.9 ## 6 2 setosa Sepal Width 3.0 ggplot(iris.long, aes(Measurement, Inches, group = Species, color = Species)) + geom_point(position = position_jitterdodge(jitter.width = .2, dodge.width = .7), alpha = .1) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, shape = &quot;diamond&quot;, size = 4, color = &quot;black&quot;, position = position_dodge(width = .7)) + stat_summary(fun.data = mean_cl_normal, na.rm = TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;black&quot;, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set1&quot;) 17.4 Three-Way Interaction Plots Just add facet_wrap(~ Part) after the first ggplot() line. ggplot(iris.long, aes(Measurement, Inches, group = Species, color = Species)) + facet_wrap(~ Part) + geom_point(position = position_jitterdodge(jitter.width = .2, dodge.width = .7), alpha = .1) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, shape = &quot;diamond&quot;, size = 4, color = &quot;black&quot;, position = position_dodge(width = .7)) + stat_summary(fun.data = mean_cl_normal, na.rm = TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;black&quot;, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set1&quot;) Therefore, you can see how to plot interactions using group/color and facet_wrap(). "],["univariate-plots.html", "18 Univariate Plots", " 18 Univariate Plots This will be a chapter on univariate plots "],["statistical-analysis-overview.html", "Statistical Analysis: Overview R Markdown", " Statistical Analysis: Overview R Markdown This section will cover what RMarkdown documents are and how to use them. Coming Soon! For a brief intro to R Markdown see https://rmarkdown.rstudio.com/lesson-1.html There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. One guide that I frequently reference is https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet "],["statistics-in-r.html", "19 Statistics in R", " 19 Statistics in R This will be an introduction to statistical analysis in R "],["structural-equation-modelling.html", "20 Structural Equation Modelling 20.1 lavaan 20.2 semoutput", " 20 Structural Equation Modelling By far the most common statistical analyses we do in this lab are confirmatory factor analysis (CFA) and structural equation modeling (SEM). This Chapter will cover how to conduct CFA’s and SEM’s in R using the lavaan package. 20.1 lavaan Install the lavaan package install.packages(&quot;lavaan&quot;) Visit the lavaan website and navigate to the Tutorial tab. This is an excellent resource for you to consult if you forget any syntax or want more details on using lavaan. You should go over the full tutorial yourself, but I will go ahead and cover the basics here. There are only two main steps to run a lavaan model. Build the model object Run the model with cfa() or sem() 20.1.1 Building the model object The model object is where you specify the model equation for the CFA or SEM. It is actually very easy and intuitive to do. Basically you specify the model equation within single quotes and pass it to an object called model. Let’s say we have want to run a model corresponding to this model diagram: We would simply specify: model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; This defines a CFA model with three latent factors; visual, textual, and speed with their respective indicators. The indicators need to correspond to column names in the data frame. There are certain defaults that lavaan uses so that we do not have to specify every single path in the model. For instance, by default it will add correlations between the latent factors in a CFA model. That is why in the model example above, the latent correlations are not explicit, yet they are implicitly part of the model. Model Syntax formula type operator mnemonic latent variable definition =~ is measured by regression ~ is regressed on variance/covariance ~~ is correlated with new parameter := is defined by 20.1.2 Run the model Then the model can be ran using cfa() or sem() functions fit &lt;- cfa(model, data) The first two arguments to pass onto the lavaan functions are model and data, respectively. There are other important arguments that we will cover later. The summary(fit, fit.measures = TRUE, standardized = TRUE) output to a lavaan model looks like ## lavaan 0.6-7 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 301 ## ## Model Test User Model: ## ## Test statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 918.852 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.931 ## Tucker-Lewis Index (TLI) 0.896 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3737.745 ## Loglikelihood unrestricted model (H1) -3695.092 ## ## Akaike (AIC) 7517.490 ## Bayesian (BIC) 7595.339 ## Sample-size adjusted Bayesian (BIC) 7528.739 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.092 ## 90 Percent confidence interval - lower 0.071 ## 90 Percent confidence interval - upper 0.114 ## P-value RMSEA &lt;= 0.05 0.001 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual =~ ## x1 1.000 0.900 0.772 ## x2 0.554 0.100 5.554 0.000 0.498 0.424 ## x3 0.729 0.109 6.685 0.000 0.656 0.581 ## textual =~ ## x4 1.000 0.990 0.852 ## x5 1.113 0.065 17.014 0.000 1.102 0.855 ## x6 0.926 0.055 16.703 0.000 0.917 0.838 ## speed =~ ## x7 1.000 0.619 0.570 ## x8 1.180 0.165 7.152 0.000 0.731 0.723 ## x9 1.082 0.151 7.155 0.000 0.670 0.665 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual ~~ ## textual 0.408 0.074 5.552 0.000 0.459 0.459 ## speed 0.262 0.056 4.660 0.000 0.471 0.471 ## textual ~~ ## speed 0.173 0.049 3.518 0.000 0.283 0.283 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.549 0.114 4.833 0.000 0.549 0.404 ## .x2 1.134 0.102 11.146 0.000 1.134 0.821 ## .x3 0.844 0.091 9.317 0.000 0.844 0.662 ## .x4 0.371 0.048 7.779 0.000 0.371 0.275 ## .x5 0.446 0.058 7.642 0.000 0.446 0.269 ## .x6 0.356 0.043 8.277 0.000 0.356 0.298 ## .x7 0.799 0.081 9.823 0.000 0.799 0.676 ## .x8 0.488 0.074 6.573 0.000 0.488 0.477 ## .x9 0.566 0.071 8.003 0.000 0.566 0.558 ## visual 0.809 0.145 5.564 0.000 1.000 1.000 ## textual 0.979 0.112 8.737 0.000 1.000 1.000 ## speed 0.384 0.086 4.451 0.000 1.000 1.000 Yikes!! You really should learn to understand this output, but nicer looking output would be nice right? This is where my semoutput package comes in handy. 20.2 semoutput If you have not done so already open the RStudio project file for this tutorial. Download the PoliticalDemocracy dataset used in the lavaan tutorial Save it to the folder Data Files of your project directory. Install semoutput devtools::install_github(&quot;dr-JT/semoutput&quot;) Additional packages you will need to have installed install.packages(&quot;sjPlot&quot;) install.packages(&quot;semPlot&quot;) Once you install semoutput you should Restart R by going to: Session -&gt; Restart R You can download an R Markdown template for doing CFA and SEM in lavaan. Go to: File -&gt; New File -&gt; R Markdown…From Template -&gt; CFA/SEM (lavaan) 20.2.1 YAML Header At the top of the document is what is called the YAML header. Here is where you can specify certain parameters that you may want to use as default in your analyses. You also need to specify the location and name of the data file you will be working with params: import.file: &quot;&quot; # Relative file path to data mimic: &quot;lavaan&quot; # Which software program to mimic for estimating models missing: &quot;ML&quot; # How to deal with missing values: &quot;ML&quot; or &quot;listwise&quot; std.lv: TRUE # For CFAs, default setting whether to set latent variances to 1 or not std.ov: FALSE # Standardize all observed varialbes? se: &quot;standard&quot; # How to calcualte standard errors: &quot;standard&quot; or &quot;bootstrap&quot; bootstrap: 1000 # If se = &quot;bootstrap&quot; how many boostrap samples? skipping the efa example for now The first R code chunk Required Packages is where you should load any packages used in the document. The next R code chunk is where the data file is imported. The next two R code chunk’s print out a descriptive and correlational tables. The next section is a template for conducting an exploratory factor analysis with psych::fa(). Let’s skip this for now. The next two sections are templates for CFA and SEM using lavaan. For each CFA and SEM section there are 4 subsections. The “Summary Output” subsection displays nice looking tables summarizing the model results The “Diagram Output” subsection will display a model diagram The “Residual Correlation Matrix” subsection will display the residual correlation matrix The “Full Output” subsection will display the results from summary() along with parameter estimates and modification indices. This way you can still get the full output from a lavaan model as it provides more information than the “Summary Output”. You can also add additional output to this section if you need more info about the model. 20.2.2 CFA Example Ultimately we will run the following SEM model. First let’s conduct a CFA of the model. The following error residuals are correlated: y1 and y5; y2 and y4; y2 and y6; y3 and y7; y4 and y8; y6 and y8 To correlate error residuals you would specify: y1 ~~ y5 Move down to the CFA section. First, you need to create a list of the latent factor labels (this is for the output and not running a lavaan model). factors &lt;- c(&quot;dem60&quot;, &quot;ind60&quot;, &quot;dem65&quot;) Then specify the model equation. The commented lines (e.g. # latent factors) are just optional and can be changed or removed. Remember, the factor correlations are implied. model &lt;- &#39; # latent factors # correlated errors # constraints &#39; model &lt;- &#39; # latent factors dem60 =~ y1 + y2 + y3 + y4 ind60 =~ x1 + x2 + x3 dem65 =~ y5 + y6 + y7 + y8 # correlated errors y1 ~~ y5 y2 ~~ y4 y2 ~~ y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 # constraints &#39; You do not need to change anything for cfa(). Unless you want to change some of the defaults you set in the YAML header. fit &lt;- cfa(model = model, data = data, mimic = params$mimic, missing = params$missing, std.lv = params$std.lv, std.ov = params$std.ov, se = params$se, bootstrap = params$bootstrap) Run this R code chunk by pressing the green arrow button. Then run each R code chunk in each subsection to print the output. sem_sig(fit) Table 20.1: Model Significance Sample.Size Chi.Square df p.value 75 38.125 35 0.329 sem_fitmeasures(fit) Table 20.1: Model Fit Measures CFI RMSEA RMSEA.Lower RMSEA.Upper AIC BIC 0.995 0.035 0 0.092 3179.582 3276.916 sem_factorloadings(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 20.1: Factor Loadings Standardized Latent Factor Indicator Loadings sig p Lower.CI Upper.CI SE z dem60 y1 0.850 *** 0 0.765 0.936 0.044 19.435 dem60 y2 0.717 *** 0 0.592 0.843 0.064 11.207 dem60 y3 0.722 *** 0 0.596 0.849 0.064 11.221 dem60 y4 0.846 *** 0 0.759 0.933 0.044 19.020 ind60 x1 0.920 *** 0 0.874 0.965 0.023 39.658 ind60 x2 0.973 *** 0 0.941 1.005 0.017 58.917 ind60 x3 0.872 *** 0 0.812 0.933 0.031 28.304 dem65 y5 0.808 *** 0 0.713 0.903 0.048 16.698 dem65 y6 0.746 *** 0 0.634 0.858 0.057 13.031 dem65 y7 0.824 *** 0 0.734 0.913 0.046 18.063 dem65 y8 0.828 *** 0 0.738 0.918 0.046 18.030 sem_factorcor(fit, factors = factors) Table 20.1: Latent Factor Correlations Factor 1 Factor 2 r sig p Lower.CI Upper.CI SE dem60 ind60 0.447 *** 0 0.242 0.652 0.105 dem60 dem65 0.967 *** 0 0.909 1.024 0.029 ind60 dem65 0.578 *** 0 0.403 0.753 0.089 sem_factorvar(fit, factors = factors) Table 20.1: Latent Factor Variance/Residual Variance Factor 1 Factor 2 var var.std sig p sem_rsquared(fit) Table 20.1: R-Squared Values Variable R-Squared y1 0.7232242 y2 0.5142640 y3 0.5217883 y4 0.7152245 x1 0.8461294 x2 0.9467924 x3 0.7606256 y5 0.6528920 y6 0.5565270 y7 0.6784378 y8 0.6853215 semPaths(fit, latents = factors, whatLabels = &quot;std&quot;, layout = &quot;tree2&quot;, rotation = 2, style = &quot;lisrel&quot;, optimizeLatRes = TRUE, intercepts = FALSE, residuals = TRUE, curve = 1, curvature = 2, sizeLat = 10, nCharNodes = 8, sizeMan = 11, sizeMan2 = 4, edge.label.cex = 1.2, edge.color = &quot;#000000&quot;) modificationIndices(fit, sort. = TRUE, minimum.value = 3) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 59 ind60 =~ y4 4.796 0.577 0.577 0.174 0.174 ## 60 ind60 =~ y5 4.456 0.559 0.559 0.215 0.215 ## 67 dem65 =~ y4 4.260 2.986 2.986 0.898 0.898 ## 72 y1 ~~ y3 3.771 0.849 0.849 0.274 0.274 ## 81 y2 ~~ x1 3.040 -0.155 -0.155 -0.200 -0.200 20.2.3 SEM Example Now let’s run the actual SEM model. Really the only difference is that we will add some regression paths factors &lt;- c(&quot;dem60&quot;, &quot;ind60&quot;, &quot;dem65&quot;) model &lt;- &#39; # latent factors dem60 =~ y1 + y2 + y3 + y4 ind60 =~ x1 + x2 + x3 dem65 =~ y5 + y6 + y7 + y8 # variances # covariances y1 ~~ y5 y2 ~~ y4 y2 ~~ y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 # regressions dem65 ~ dem60 + ind60 dem60 ~ ind60 &#39; fit &lt;- sem(model = model, data = data, mimic = params$mimic, missing = params$missing, std.lv = FALSE, std.ov = params$std.ov, se = params$se, bootstrap = params$bootstrap) sem_sig(fit) Table 20.2: Model Significance Sample.Size Chi.Square df p.value 75 38.125 35 0.329 sem_fitmeasures(fit) Table 20.2: Model Fit Measures CFI RMSEA RMSEA.Lower RMSEA.Upper AIC BIC 0.995 0.035 0 0.092 3179.582 3276.916 sem_factorloadings(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 20.2: Factor Loadings Standardized Latent Factor Indicator Loadings sig p Lower.CI Upper.CI SE z dem60 y1 0.850 *** 0 0.765 0.936 0.044 19.435 dem60 y2 0.717 *** 0 0.592 0.843 0.064 11.207 dem60 y3 0.722 *** 0 0.596 0.849 0.064 11.221 dem60 y4 0.846 *** 0 0.759 0.933 0.044 19.020 ind60 x1 0.920 *** 0 0.874 0.965 0.023 39.658 ind60 x2 0.973 *** 0 0.941 1.005 0.017 58.917 ind60 x3 0.872 *** 0 0.812 0.933 0.031 28.304 dem65 y5 0.808 *** 0 0.713 0.903 0.048 16.698 dem65 y6 0.746 *** 0 0.634 0.858 0.057 13.031 dem65 y7 0.824 *** 0 0.734 0.913 0.046 18.063 dem65 y8 0.828 *** 0 0.738 0.918 0.046 18.030 sem_paths(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 20.2: Regression Paths Standardized Predictor DV Path Values SE z sig p Lower.CI Upper.CI dem60 dem65 0.885 0.052 17.100 *** 0.000 0.784 0.987 ind60 dem65 0.182 0.073 2.498 0.013 0.039 0.325 ind60 dem60 0.447 0.105 4.267 *** 0.000 0.242 0.652 sem_factorcor(fit, factors = factors) Table 20.2: Latent Factor Correlations Factor 1 Factor 2 r sig p Lower.CI Upper.CI SE sem_factorvar(fit, factors = factors) Table 20.2: Latent Factor Variance/Residual Variance Factor 1 Factor 2 var var.std sig p dem60 dem60 3.956 0.800 *** 0.000 ind60 ind60 0.448 1.000 *** 0.000 dem65 dem65 0.172 0.039 0.434 sem_rsquared(fit) Table 20.2: R-Squared Values Variable R-Squared y1 0.7232243 y2 0.5142639 y3 0.5217879 y4 0.7152243 x1 0.8461294 x2 0.9467924 x3 0.7606255 y5 0.6528920 y6 0.5565263 y7 0.6784382 y8 0.6853214 dem60 0.1995522 dem65 0.9609949 semPaths(fit, latents = factors, whatLabels = &quot;std&quot;, layout = &quot;tree2&quot;, rotation = 2, style = &quot;lisrel&quot;, optimizeLatRes = TRUE, intercepts = FALSE, residuals = TRUE, curve = 1, curvature = 2, sizeLat = 10, nCharNodes = 8, sizeMan = 11, sizeMan2 = 4, edge.label.cex = 1.2, edge.color = &quot;#000000&quot;) "]]
