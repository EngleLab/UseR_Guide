[
["englelab-packages.html", "Chapter 6 EngleLab Packages 6.1 englelab 6.2 datawrangling", " Chapter 6 EngleLab Packages I have created R packages that contain functions to more easily score tasks, transform variables, and clean data. In this chapter you will learn about the functions from these two packages englelab https://github.com/EngleLab/englelab datawrangling https://dr-jt.github.io/datawrangling I am hosting these packages on GitHub and can be downloaded using the devtools package install.packages(&quot;devtools&quot;) devtools::install_github(&quot;EngleLab/englelab&quot;) devtools::install_github(&quot;dr-JT/datawrangling&quot;) 6.1 englelab The functions in the englelab package are to create “tidy” raw data files and scored data files from the complex span and fluid intelligence tasks we frequently use. It also contains a function to calculate scores using the binning method from [insert citation here]. There are also functions to calcualte the reliability measures; cronbach’s alpha and split-half reliability. This package is intended to eventually share with other researchers who download the tasks from our website and use R to do data analysis. 6.1.1 “tidy” raw data functions It is suggested to only use the “tidy” raw functions rather than the scoring functions. The reason for this is that the scoring function does not create a “tidy” raw data file - which is useful for doing internal consitency analyses, and is just a good idea to have a “tidy” raw data file for trial-by-trial performance. Here is the list of “tidy” raw data functions: raw.ospan() raw.symspan() raw.rotspan() raw.rapm() raw.numberseries() raw.lettersets() These functions take as input an imported E-Merged or edat file. For the complex span tasks you need to specify the number of blocks administered with the blocks argument. ex. raw.ospan(data, blocks = 2) The output of the raw functions will also contain columns for the subject’s final score on the task. This is why it is suggested to just use the raw functions, you can still easily grab the final score on the task from the outputed file. 6.1.1.1 Example Here is an example script that uses the raw.ospan() function to import an E-merged data file and output a “tidy” raw data file. ## Set up #### ## Load packages library(readr) library(englelab) ## Set import/output directories import.file &lt;- &quot;data/raw/emerge/ospan.txt&quot; output.file &lt;- &quot;data/raw/ospan_raw.txt&quot; ############## ## Import import &lt;- read_delim(import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ## Clean up raw data file and save data_raw &lt;- raw.ospan(import, blocks = 2) ## Save data file write_delim(data_raw, path = output.file, &quot;\\t&quot;, na = &quot;&quot;) 6.1.2 score data functions Again, it is suggested to use the raw functions instead of the score functions. Here is the list of the score functions: score.ospan() score.symspan() score.rotspan() score.rapm() score.numberseries() score.lettersets() Like the raw functions the score functions take as input a “messy” raw E-Merged or edat file. For the complex span tasks you need to specify the number of blocks adminestered. 6.1.3 Calculating Bin Scores The bin.score() function will calculate bin scores. These are the arguments you will need to specify: x: a dataframe with trial level data. Needs to have RT and Accuracy DVs rt.col: Column name in dataframe that contains the reaction time data accuracy.col: Column name in dataframe that contains the accuracy data condition.col: Column name in dataframe that contains the trial condition values baseline.condition: The values that specify the baseline condition (e.g. “congruent”) id: Column name in dataframe that contains the subject identifiers The default argument values are: bin.score(x, rt.col = &quot;RT&quot;, accuracy.col = &quot;Accuracy&quot;, condition.col = &quot;Condition&quot;, baseline.condition = &quot;congruent&quot;, id = &quot;Subject&quot;) Your data file may already be setup with these default value column names. If so, then you just need to specify the dataframe. 6.1.4 Reliability functions 6.1.4.1 cronbach.alpha() This function takes as input a “tidy” raw trial-level dataframe. x: x a dataframe with trial level data trial.col: The column name that identifies trial number value: The column name that identifies the values to be used id: The column name that identifies the Subject IDs. Cronbach’s alpha is calculated using the alpha() function from the psych package. The difficulty in simply using the alpha() function is getting the dataframe in the correct structure. To use alpha() the values that reliability is going to be assessed over need to be in columns. The dataframe, then is one subject per row with a column for each value. For most of our tasks, the values that will be assessed over are the individual Trial level DV (RT or Accuracy). So there needs to be one column for each Trial. This is an unusual data structure and is really only useful for calculating reliability. cronbach.alpha() will save you time by creating the correct data structure for you based on a more common structure that is contained in your “tidy” raw data files (one row per trial per subject, with RT and Accuracy as columns). You should be able to take your “tidy” raw data as input to cronbach.alpha(). The output of cronbach.alpha() is a single value representing Cronbach’s Alpha. 6.1.4.2 splithalf() This function takes as input a “tidy” raw trial-level dataframe. x: x a dataframe with trial level data trial.col: The column name that identifies trial number value: The column name that identifies the values to be used id: The column name that identifies the Subject IDs. The default values are splithalf(data, trial.col = &quot;Trial&quot;, value = NULL, id = &quot;Subject&quot;) The data is split in half by even and odd trials. You should be able to take your “tidy” raw data as input to splithalf(). The output of splithalf() is a single value representing split-half reliability. 6.2 datawrangling It would take too long to cover each of the functions in this package one-by-one. I will cover just a few that are the most commonly used functions. For a descriptions of each function see https://dr-jt.github.io/datawrangling/reference/index.html 6.2.1 Merging Data Files You might find yourself in a situation where you need to merge multiple text files together. There are two types of merge operations that can be performed. In R, a “join” is merging dataframes together that have at least some rows in common (e.g. Same Subject IDs) and have at least one column that is different. The rows that are common serve as the reference for how to “join” the dataframes together. In R, a “bind” is combining datarames together by staking either the rows or columns. It is unlikely that we you will need to do a column bind so we can skip that. A row “bind” takes dataframes that have the same columns but different rows. This will happen if you have separate data files for each subject from the same task. Each subject data file will have their unique rows (subject by trial level data) but they will all have the same columns. The E-Merge software program is performing a row “bind” of each subject .edat file. In E-Prime 2 we have to go through E-Merge to do this process. However, in E-Prime 3.0 there is the option to output an exported .edat file as a tab-delimited .txt file. Using the files.bind() function from the datawrangling package will allow us to skip the E-Merge step. The datawrangling package contains two functions to merge data files together: files.join() files.bind() They both work in a similar way. The files you want to merge need to be in the same folder on your computer. You specify the location of this folder using the path = argument. You need to specify a pattern that uniquely identifies the files you want to merge (e.g. “.txt”, or “Flanker”) using the pattern = argument. Then specify the directory and filename you want to save the merge file to using the output.file = argument. Here are the arguments that can be specified: path: Folder location of files to be merged pattern: Pattern to identify files to be merged delim: Delimiter used in files. Passed onto readr::read_delim() na: How are missing values defined in files to be merged. Passed to readr::write_delim() output.file: File name and path to be saved to. id: Subject ID column name. Passed onto plyr::join_all(by = id). ONLY for files.join() bind: The type of bind to perform (default = “rows”). ONLY for files.bind() 6.2.2 Transformations There are a set of function is datawrangling to allow you to more easily transform column values into new variables and to do data cleaning. 6.2.2.1 Create Composites The function composite() will create composite scores out of specified columns. Right now you can only create “mean” composite scores. In the future I plan on adding “sum” and “factor score” composite types. Here is a list of the arguments you can specifiy: x: dataframe variables: c() of columns to create the composite from type: What type of composite should be calculated?, i.e. mean or sum. (Default = “mean”). standardize: Logical. Do you want to calculate the composite based on standardized (z-score) values? (Default = TRUE) name: Name of the new composite variable to be created missing.allowed: Criteria for the number of variables that can having missing values and still calculate a composite for that subject Example: library(datawrangling) composite(data, variables = c(&quot;RAPM&quot;, &quot;NumberSeries&quot;, &quot;LetterSets&quot;), type = &quot;mean&quot;, standardize = TRUE, name = &quot;Gf&quot;, missing.allowed = 1) 6.2.2.2 Centering and Z-scoring The function center() will create either unstandardized or standardized (z-scored) centered variables. The list of arguments that can be passed onto the function are: x: dataframe variables: c() of columns to center standardized: Logical. Do you want to calculate zscores? (Default = FALSE) Example: library(datawrangling) center(data, variables = c(&quot;RT&quot;), standardized = TRUE) 6.2.3 Data Cleaning 6.2.3.1 Trimming The function trim() will replace outlier scores that exceed a certain z-score cutoff. There are several options for how to replace the outlier scores. Replace with “NA” (missing value) “cutoff” (the z-score cutoff value, e.g. 3.5 SDs) “mean” “median” The arguments that can be specified are: x: dataframe variables: c() of variables to be trimmed. option to set variables = &quot;all&quot; to trim all variables in a dataframe. But then must specify id = cutoff: z-score cutoff to use for trimming (default: 3.5) replace: What value should the outlier values be replaced with. (default: replace = “NA”) id: Column name that contains subject IDs. **ONLY needs to be used if variables = &quot;all&quot; Example: library(datawrangling) trim(data, variables = &quot;RT&quot;, cutoff = 3.5 replace = &quot;cutoff&quot;) 6.2.3.2 Latent variable criteria The function remove.latent() will remove subjects who have missing data on too many tasks for a construct(s). The factor structure needs to be specified in the factor.list argument. For example, factor.list = list(WMC = c(&quot;OSpan&quot;, &quot;SymSpan&quot;, &quot;RotSpan&quot;), Gf = c(&quot;RAPM&quot;, &quot;LetterSets&quot;, &quot;NumberSeries&quot;)) The arguments that can be specified are: x: dataframe factor.list: list of factors and tasks missing.allowed: Proportion of tasks allowed to be missing id: Subject ID variable output.dir: File directory to save removed subjects to [OPTIONAL] output.file: File name to save removed subjects to [OPTIONAL] Example: library(datawrangling) remove.latent(data, factor.list = list(WMC = c(&quot;OSpan&quot;, &quot;SymSpan&quot;, &quot;RotSpan&quot;), Gf = c(&quot;RAPM&quot;, &quot;LetterSets&quot;, &quot;NumberSeries&quot;)), missing.allowed = .5, id = &quot;Subject&quot;) Now on to scoring the “tidy” raw data file "]
]
