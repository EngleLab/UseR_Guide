[
["index.html", "R Tutorial and User Guide for the EngleLab Preface How to use this guide Workflow Overview", " R Tutorial and User Guide for the EngleLab Jason Tsukahara 2019-05-01 Preface Welcome to the EngleLab R Tutorial and User Guide! R is becoming a popular tool for doing statistical analyses across many scientific disciplines. It has many advantages but there is a considerable learning curve. This guide is meant to help you become proficient in R and quickly learn standard data processing tasks that are common in the EngleLab. There are more detailed tutorials and resources mentioned throughout this guide. If you want to become more proficient in R it is a good idea to go through those as well. There are a lot of online forums where people have asked the same questions as you and someone has most likely provided an answer. I suggest taking advantage of Google to help you learn more about R. How to use this guide There are two general ways you might use this guide: Going through each chapter step-by-step as a tutorial Referencing the guide as you work on your own projects If you need to process your data and get it ready for analysis then you can start at Section III: Example Data Preparation to start working on your project in R as soon as possible. You can always go back and reference Section I and Section II. I would suggest also reading through Section IV: Data Science Practices. You will find the script Templates presented in Chapter 14 particularly useful If you need to start on your statistical analysis and data visualization then you can start at Section V: Data Analysis and go through the Chapters that are only relevant to your specific analysis. Workflow The image below represents the general data processing steps required to go from raw data to visualization and statistical analyses. This R Guide will show you how to create R scripts for each of these stages; allowing you to fully reproduce your analyses from beginning to end. One of the major advantages to this is it allows you to easily go back and change how you process data at any one of these stages. The first step of the data processing workflow is to convert “messy” raw data files to “tidy” raw data files. Most experiment softwares will produce a “messy” raw data file. In this file there are usually more rows and columns than you would ever be interested in, variable or values are named incoherently (i.e. stimSlide2.RT), and/or there may be separate files for each subject. It will be easier to work with a “tidy” raw data file. A “tidy” raw data file has only the rows and columns that are relevant (one row for each trial), variable and values are named coherently (i.e. RT), and there is one file that contains data for all subjects. This is an easy step to skip but it is highly recommended because it will make it easier 1) for your future self to come back and do some additional analyses/re-analyses and 2) for you to share your raw data with other researchers. The next step is to score and clean the data. For most statistical analyses you will want to aggregate the trial-level data into one or more dependent measures for that task. The scored data file will have one row per subject (or possibly one row per subject per experimental condition). This step also involves any data cleaning procedures you might choose to perform, such as removing outliers. Finally you are now ready for the fun part, Data Analysis! Data analysis consists of visualing your data and conducting statistical analyses. At this stage you will be generating output of your results in tables and figures. Overview In Section I: Getting Started in R it is all about learning the fundamentals of using R. From installing R and RStudio to learning about object types, how functions work, and more. Chapter 3 is more optional, however you will find it useful to being more proficient in R. Section II: Working with Data is still about learning the fundamentals but more specifically the fundamentals of working with data like we do in this lab. The tidyverse is a collection of packages that has an intuitive grammar and design philosophy. It makes working with data in R so much easier and also quick to learn. Therefore, you will be learning how to work with data in R the tidyverse way. Section II, Chapter 7, will also cover how to perform more complex but common data manipulations. Such as calculating z-scores, creating composite variables, and trimming data. I have made these more complex transformations easier by with functions in my datawrangling package. After learning these fundamentals, in Section III: Example Data Preparation, you will immediately dive into processing data in R with an example data set from a Flanker task. This will provide you a good overview and experience with how to write R scripts to do data preparation (Stages 1 and 2 in the diagram above). Then before getting into data visualization and statistical analysis, Section IV: Data Science Practices you will learn how to implement good data science practices in R that align with Open Science and Reproducibility principles. And most importantly you these practices will help YOU store, manage, handle, reproduce, and re-analyze your data. Hopefully these practices will empower you to explore and understand your data better. Finally, in Section V: Data Analysis you will get to the fun part! Visualizing and analyzing your data with statistics! First you will learn about the fundamentals of Data Visualization in R using the ggplot2 package (part of the tidyverse). ggplot2 has made R one of the best, if not the best (according to my friend Adhaar who mastered in computer science at Georgia Tech), programming languages for data visualization. Then, various statistical methods will be covered, including regression and structural equation modelling. I plan on adding a final section at some point with a list of useful resources. Before getting into using R, you need to install the required programs "],
["installation.html", "Chapter 1 Installation 1.1 Installing R 1.2 Installing R Studio 1.3 The R Studio Environemnt 1.4 R Studio Settings", " Chapter 1 Installation 1.1 Installing R First you need to download the latest version of R from their website https://www.r-project.org Select CRAN on the left, just under Download Select the first option under 0-Cloud Select the download option depending on your computer Select the base installation (for Windows) or the Latest Release (for Mac) Open and Run the installation file 1.2 Installing R Studio The easiest way to interact with R is through the R Studio environment. To do this you need to install R Studio from https://www.rstudio.com/products/rstudio/download/#download Select the Free version of R Studio Desktop Select the download option depending on your computer 1.3 The R Studio Environemnt Go ahead an open the RStudio application on your computer. When you open a fresh session of RStudio there are 3 window panes open. The Console window, the Environment window, and the Files window. Go ahead and navigate to File -&gt; New File -&gt; R Script. You should now see something similar to the image below There are 4 window panes and each one has it’s own set of tabs associated with it: The Console window (the bottom left window pane) is where code is executed and output is displayed. The Source window (the top left window pane) is where you will write your code to create a script file. When you open a new script file you will see a blank sheet where you can start writing the script. When you execute lines of code from here you will see it being executed in the Console window. The Source window is also where you can view dataframes you have just imported or created. In the image above, notice the different tabs in the Source window. There are two “Untitled” script files open and one dataframe called ‘data’. The Environment window (top right window pane) is where you can see any dataframes, variables, or functions you have created. Go ahead and type the following in your Console window and hit enter. hello &lt;- &quot;hello&quot; You should now see the object hello in the Environment window pane The Files window (the bottom right window pane) is where you can see your computer’s directories, plots you create, manage packages, and see help documentation. 1.4 R Studio Settings There are a few changes to R Studio settings I suggest you make. I will not go into why these are a good idea - so just do what I say! If you want to know you can talk to me about it. Navigate to Tools -&gt; Global Options Change the settings to look like this: Be sure to set ‘Save workspace to .RData on exit’ to Never You can also change the “Editor Theme” if you navigate to the “Appearance” tab in Settings. Dark themes are easier on the eyes. I use Material dark theme. Now you are ready to start writing some R code! "],
["basic-r.html", "Chapter 2 Basic R 2.1 Creating R objects 2.2 If…then Statements 2.3 R Packages 2.4 More R Basic Resources", " Chapter 2 Basic R This chapter will cover the basics of how to assign values to objects, create and extract information from vectors, lists, and dataframes. If you have not done so already, open a new R script file. To create a new R script go to File -&gt; New File -&gt; R Script This should have opened a blank Script window called Untitled. The Script window is your workspace. This is where you will write, edit, delete, re-write, your code. To follow along with the tutorial, you should type the lines of code I display in the tutorial into your script. Go ahead and save your empty script as 2_basics.R 2.1 Creating R objects In R, everything that exists is an object and everything you do to objects are functions. You can define an object using the assignment operator &lt;-. Everything on the left hand side of the &lt;- assignment operator is an object. Everything on the right hand side of &lt;- are functions or values. Go ahead and type the following two lines of code in your script string &lt;- &quot;hello&quot; string ## [1] &quot;hello&quot; You can execute/run a line of code by placing the cursor anywhere on the line and press Ctrl + Enter. Go ahead an run the two lines of code. In this example, the first line creates a new object called string with a value of “hello”. The second line simply prints the output of string to the Console window. In the second line there is no assignment operator. When there is no &lt;- this means you are essentially just printing to the console. You can’t do anything with stuff that is just printed to the console, it is just for viewing purposes. For instance, if I wanted to calculate 1 + 2 I could do this by printing it to the console 1 + 2 ## [1] 3 However, if I wanted to do something else with the result of that calculation then I would not be able to unless I assigned the result to an object using &lt;- result &lt;- 1 + 2 result &lt;- result * 5 result ## [1] 15 The point is, you are almost always going to assign the result of some function or value to an object. Printing to the console is not very useful. Almost every line of code, then, will have an object name on the left hand side of &lt;- and a function or value on the right hand side of &lt;- In the first example above, notice how I included &quot; &quot; around hello. This tells R that hello is a string, not an object. If I were to not include &quot; &quot;, then R would think I am calling an object. And since there is no object with the name hello it will print an error string &lt;- hello ## Error in eval(expr, envir, enclos): object &#39;hello&#39; not found Do not use &quot; &quot; for Numerical values a &lt;- &quot;5&quot; + &quot;1&quot; ## Error in &quot;5&quot; + &quot;1&quot;: non-numeric argument to binary operator You can execute lines of code by: Typing them directly into the Console window Typing them into the Script window and then on that line of code pressing Ctrl+Enter. With Ctrl+Enter you can execute one line of your code at a time. Clicking on Source at the top right of the Script window. This will run ALL the lines of code contained in the script file. It is important to know that EVERYTHING in R is case sensitive. a &lt;- 5 a + 5 ## [1] 10 A + 5 ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found 2.1.1 Classes Classes are types of values that exist in R: character &quot;hello&quot;, &quot;19&quot; numeric (or double) 2, 32.55 integer 5, 99 logical TRUE, FALSE To evaluate the class of an object you can use the typeof() typeof(a) ## [1] &quot;double&quot; To change the class of values in an object you can use the function as.character() , as.numeric() , as.double() , as.integer() , as.logical() functions. as.integer(a) ## [1] 5 as.character(a) ## [1] &quot;5&quot; as.numeric(&quot;hello&quot;) ## Warning: NAs introduced by coercion ## [1] NA 2.1.2 Vectors Okay so now I want to talk about creating more interesting objects than just a &lt;- 5. If you are going to do anything in R it is important that you understand the different data types and data structures you can use in R. I will not cover all of them in this tutorial. For more information on data types and structures you should go to https://ramnathv.github.io/pycon2014-r/learn/structures.html Vectors contain elements of data. The length of a vector is the number of elements in the vector. For instance, the variable a we created earlier is actually a vector of length 1. It contains one element with a value of 5. Now let’s create a vector with more than one element. b &lt;- c(1,3,5) c() is a function. Functions contain arguments that are inputs for the function. Arguments are separated by commas. In this example the c() fucntion concatenates the arguments (1, 3, 5) into a vector. We are passing the result of this function to the object b. What do you think the output of b will look like? b ## [1] 1 3 5 You can see that we now have a vector that contains 3 elements; 1, 3, 5. If you want to reference the value of specific elements of a vector you use brackets [ ]. For instance, b[2] ## [1] 3 The value of the second element in vector b is 3. Let’s say we want to grab only the 2nd and 3rd elements. We can do this at least two differnt ways. b[2:3] ## [1] 3 5 b[-1] ## [1] 3 5 Now, it is important to note that we have not been changing vector b. If we display the output of b, we can see that it still contains the 3 elements. b ## [1] 1 3 5 To change vector b we need to define b as vector b with the first element removed b &lt;- b[-1] b ## [1] 3 5 Vector b no longer contains 3 elements. Now, let’s say we want to add an element to vector b. c(5,b) ## [1] 5 3 5 Here the c() fucntion created a vector with the value 5 as the first element followed by the values in vector b Or we can use the variable a that has a value of 5. Let’s add this to vector b b &lt;- c(a,b) b ## [1] 5 3 5 What if you want to create a long vector with many elements? If there is a pattern to the sequence of elements in the vector then you can create the vector using seq() seq(0, 1000, by = 4) ## [1] 0 4 8 12 16 20 24 28 32 36 40 44 48 52 ## [15] 56 60 64 68 72 76 80 84 88 92 96 100 104 108 ## [29] 112 116 120 124 128 132 136 140 144 148 152 156 160 164 ## [43] 168 172 176 180 184 188 192 196 200 204 208 212 216 220 ## [57] 224 228 232 236 240 244 248 252 256 260 264 268 272 276 ## [71] 280 284 288 292 296 300 304 308 312 316 320 324 328 332 ## [85] 336 340 344 348 352 356 360 364 368 372 376 380 384 388 ## [99] 392 396 400 404 408 412 416 420 424 428 432 436 440 444 ## [113] 448 452 456 460 464 468 472 476 480 484 488 492 496 500 ## [127] 504 508 512 516 520 524 528 532 536 540 544 548 552 556 ## [141] 560 564 568 572 576 580 584 588 592 596 600 604 608 612 ## [155] 616 620 624 628 632 636 640 644 648 652 656 660 664 668 ## [169] 672 676 680 684 688 692 696 700 704 708 712 716 720 724 ## [183] 728 732 736 740 744 748 752 756 760 764 768 772 776 780 ## [197] 784 788 792 796 800 804 808 812 816 820 824 828 832 836 ## [211] 840 844 848 852 856 860 864 868 872 876 880 884 888 892 ## [225] 896 900 904 908 912 916 920 924 928 932 936 940 944 948 ## [239] 952 956 960 964 968 972 976 980 984 988 992 996 1000 Vectors can only contain elements of the same “class”. d &lt;- c(1, &quot;2&quot;, 5, 9) d ## [1] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;9&quot; as.numeric(d) ## [1] 1 2 5 9 2.1.3 Factors Factors are special types of vectors that can represent categorical data. You can change a vector into a factor object using factor() factor(c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;)) ## [1] male female male male female female male ## Levels: female male factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;)) ## [1] high low medium high high low medium medium ## Levels: high low medium f &lt;- factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;), levels = c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;)) f ## [1] high low medium high high low medium medium ## Levels: high medium low 2.1.4 Lists Lists are containers of objects. Unlike Vectors, Lists can hold different classes of objects. list(1, &quot;2&quot;, 2, 4, 9, &quot;hello&quot;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] &quot;hello&quot; You might have noticed that there are not only single brackets, but double brackets [[ ]] This is because Lists can hold not only single elements but can hold vectors, factors, lists, dataframes, and pretty much any kind of object. l &lt;- list(c(1,2,3,4), &quot;2&quot;, &quot;hello&quot;, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) l ## [[1]] ## [1] 1 2 3 4 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] &quot;hello&quot; ## ## [[4]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; You can see that the length of each element in a list does not have to be the same. To reference the elements in a list you need to use the double brackets [[ ]]. l[[1]] ## [1] 1 2 3 4 To reference elements within list elements you use double brackets followed by a single bracket l[[4]][2] ## [1] &quot;b&quot; You can even give names to the list elements person &lt;- list(name = &quot;Jason&quot;, phone = &quot;123-456-7890&quot;, age = 23, favorite_colors = c(&quot;blue&quot;, &quot;red&quot;, &quot;brown&quot;)) person ## $name ## [1] &quot;Jason&quot; ## ## $phone ## [1] &quot;123-456-7890&quot; ## ## $age ## [1] 23 ## ## $favorite_colors ## [1] &quot;blue&quot; &quot;red&quot; &quot;brown&quot; And you can use the names to reference elements in a list person[[&quot;name&quot;]] ## [1] &quot;Jason&quot; person[[&quot;favorite_colors&quot;]][3] ## [1] &quot;brown&quot; 2.1.5 Data Frames You are probably already familiar with this type of data structure. SPSS and Excel uses this type of structure. It is just rows and columns of data. A data table! This is the format that is used to perform statiscital analyses on. So let’s create a data frame so you can see what one looks like in RStudio data &lt;- data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) data ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 You can view the Data Frame by clicking on the object in the Environment window or by executing the command View(data) Notice that it created three columns labeled id, x, and y. Also notice that since we only specified a vector of length 2 for x this column is coerced into 10 rows of repeateding “a” and “b”. All columns in a dataframe must have the same number of rows. You can use the $ notation to reference just one of the columns in the dataframe data$y ## [1] 10 20 30 40 50 60 70 80 90 100 Alternatively you can use data[&quot;y&quot;] ## y ## 1 10 ## 2 20 ## 3 30 ## 4 40 ## 5 50 ## 6 60 ## 7 70 ## 8 80 ## 9 90 ## 10 100 To reference only certain rows within a column data$y[1:5] ## [1] 10 20 30 40 50 data[1:5,&quot;y&quot;] ## [1] 10 20 30 40 50 2.2 If…then Statements If…then statements are useful for when you need to execute code only if a certain statement is TRUE. For instance,… First we need to know how to perform logical operations in R Okay, we have this variable a a &lt;- 5 Now let’s say we want to determine if the value of a is greater than 3 a &gt; 3 ## [1] TRUE You can see that the output of this statement a &gt; 3 is TRUE Here is a list of logical operations in R Now let’s write an if…then statement. If a is greater than 3, then multiply a by 2. if (a&gt;3){ a &lt;- a*2 } a ## [1] 10 The expression that is being tested is contained in parentheses, right after the if statement. If this expression is evaluated as TRUE then it will perform the next line(s) of code. The { is just a way of encasing multiple lines of code within one if statement. The lines of code then need to be closed of with }. In this case, since we only had one line of code b &lt;- a*2 we could have just written it as. a &lt;- 5 if (a&gt;3) a &lt;- a*2 a ## [1] 10 What if we want to do something to a if a is NOT greater than 3? In other words… if a is greater than 3, then multiple a by 2 else set a to missing a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 You can keep on chaining if…then… else… if… then statements together. a &lt;- 5 if (is.na(a)){ print(&quot;Missing Value&quot;) } else if (a&lt;0){ print(&quot;A is less than 0&quot;) } else if (a&gt;3){ print(&quot;A is greater than 3&quot;) } ## [1] &quot;A is greater than 3&quot; 2.3 R Packages R comes with a basic set of functions. All the functions we have used so far are part of the R basic functions. But when you want to start doing more complex operations it would be nice to have more complex functions. This is where R Packages come in… An R Package is simply a collection of functions - that usually have some common theme to them. Now the most wonderful thing about R is that other R users have developed tons of packages with functions they created themselves. For instance, a group of users have developed an R package called lavaan that makes it extremely easy to conduct SEM in R. 2.3.1 Installing and Loading R Packages R packages are easy to install and load. You just need to know the name of the package. install.packages(&quot;name_of_package&quot;) or for multiple packages at once install.packages(c(&quot;package1&quot;, &quot;package2&quot;, &quot;package3&quot;)) Installing the package does not mean you can start using the functions. To be able to use the function you need to then load the package library of functions as such library(name_of_package) When loading packages you do not have to incase the package name in &quot; &quot; 2.4 More R Basic Resources For additional tips in the basics of coding R see: https://ramnathv.github.io/pycon2014-r/visualize/README.html https://www.datacamp.com/courses/free-introduction-to-r/?tap_a=5644-dce66f&amp;tap_s=10907-287229 http://compcogscisydney.org/psyr/ http://r4ds.had.co.nz/workflow-basics.html Something "],
["intermediate-r.html", "Chapter 3 Intermediate R 3.1 For Loops 3.2 Functions", " Chapter 3 Intermediate R This chapter will cover more intermediate R programming, such as for loops, and functions. Save a new R script as 3_intermediate.R 3.1 For Loops For loops allow you iterate the same line of code over multiple instances. Let’s say we have a vector of numerical values c &lt;- c(1,6,3,8,2,9) c ## [1] 1 6 3 8 2 9 and want perform an if…then operation on each of the elements. Let’s use the same if…then statement we used above. If the element is greater than 3, then multiply it by 2 - else set it to missing. Let’s put the results of this if…then statement into a new vector d What we need to do is loop this if…then statement for each element in c We can start out by writing the for loop statement for (i in seq_along(c)){ } This is how it works. The statement inisde of parathenses after for contains two statements separated by in. The first statement is the variable that is going to change it’s value over each iteration of the loop. You can name this whatever you want. In this case I chose the label i. The second statement defines all the values that will be used at each iteration. The second statement will always be a vector. In this case the vector is seq_along(c). seq_along() is a function that creates a vector that contains a sequence of numbers from 1 to the length of the object. In this case the object is the vector c, which has a length of 6 elements. Therefore seq_along(c), creates a vector containing 1, 2, 3, 4, 5, 6. The for loop will start with i defined as 1, then on the next iteration the value of i will be 2 … and so until the last element of seq_along(c), which is 6. We can see how this is working by printing ‘i’ on each iteration. for (i in seq_along(c)){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 You can see how on each iteration it prints the values of seq_along(c) from the first element to the last element. What we will want to do is, on each iteration of the for loop, access the ith element of the vector c. Recall, you can access the element in a vector with [ ], for instance c[1]. Let’s print each ith element of c. for (i in seq_along(c)){ print(c[i]) } ## [1] 1 ## [1] 6 ## [1] 3 ## [1] 8 ## [1] 2 ## [1] 9 Now instead of printing i the for loop is printing each element of vector c. Let’s use the same if…then statement as above a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 But instead we need to replace a with c[i] For now let’s just print() the output of the if… then statement. for (i in seq_along(c)){ if (c[i] &gt; 3){ print(c[i]*2) } else { print(NA) } } ## [1] NA ## [1] 12 ## [1] NA ## [1] 16 ## [1] NA ## [1] 18 Now for each element in c, if it is is greater than 3, then multiply it by 2 - else set as missing value. You can see that on each iteration the output is either the ith element of c multiplied by 2 or NA. But just printing things to the console is useless! Let’s overwright the old values in c with the new values. for (i in seq_along(c)){ if (c[i] &gt; 3){ c[i] &lt;- c[i]*2 } else { c[i] &lt;- NA } } But what if we want to preserve the original vector c? Well we need to put it into a new vector, let’s call it vector d. This get’s a little more complicated but is something you might find yourself doing fairly often so it is good to understand how this works. But if you are goind to do this to a “new” vector that is not yet created you will run into an error. c &lt;- c(1,6,3,8,2,9) for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } You first need to create vector d - in this case we can create an empty vector. d &lt;- c() So the logic of our for loop, if…then statement is such that; on the ith iteration - if c[i] is greater than 3, then set d[i] to c[i]*2 - else set d[i] to NA. c &lt;- c(1,6,3,8,2,9) d &lt;- c() for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } c ## [1] 1 6 3 8 2 9 d ## [1] NA 12 NA 16 NA 18 Yay! Good job. 3.2 Functions Basically anything you do in R is by using functions. In fact, learning R is just learning what functions are available and how to use them. Not much more to it than that. You have only seen a couple of functions at this point. In this chapter, a common function used was c(). This function simply concatenates a series of numerical or string values into a vector. c(1,6,3,7). Functions start with the name of the function followed by parentheses function_name(). Inside the () is where you specify certain arguments separted by commas , . Some argruments are optional and some are required for the function to work. For example, another function you saw last chapter was data.frame(). This function creates a dataframe with the columns specified by arguments. data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 The arguments id, x, and y form the columns in the dataframe. These arguments themselves used functions. For instance y used the function seq(). This function creates a sequence of numbers in a certain range at a given interval. Sometimes arguments are not defined by an =. The first two arguments in in seq() specify the range of 10 to 100. The third argument by specified the interval to be 10. So seq(10, 100, by = 10) creates a sequence of numbers ranging from 10 to 100 in intervals of 10. seq(10, 100, by = 10) ## [1] 10 20 30 40 50 60 70 80 90 100 In the seq() function the by argument is not required. This is because there is a default by value of 1. seq(10, 100) ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## [18] 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 ## [35] 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 ## [52] 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 ## [69] 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 ## [86] 95 96 97 98 99 100 Obviously if you want to specify a different interval, then you will need to specify by =. 3.2.1 Creating Your Own Functions This section is optional. It will go over how to create your own functions. Even if you do not want to get too proficient in R, it can be a good idea to know how to create your own function. It also helps you better understand how functions actaully work. We are going to create a function that calculates an average of values. To define a function you use the function() and assign the output of function() to an object, which becomes the name of the function. For instance, function_name &lt;- function(){ } This is a blank function so it is useless. Before we put stuff inside of a function let’s work out the steps to calculate an average. Let’s say we have an array a that has 10 elements a &lt;- c(1,7,4,3,8,8,7,9,2,4) a ## [1] 1 7 4 3 8 8 7 9 2 4 To calculate an average we want to take the sum of all the values in a and divide it by the number of elements in a. To do this we can use the sum() and length() functions. sum(a) ## [1] 53 length(a) ## [1] 10 sum(a)/length(a) ## [1] 5.3 Easy! So now we can just put this into a function. average &lt;- function(x){ avg &lt;- sum(x)/length(x) return(avg) } When creating a function, you need to specify what input arguments the function is able to take. Here were are specifying the argument x. You can use whatever letter or string of letters you want, but a common notation is to use x for the object that is going to be evaluated by the function. Then, inside the function we use the same letter x to calculate the sum() and length() of x. What this means is that Arguments specified in a function become objects (or variables) passed inside the function You can create new objects inside a function. For instance we are creating an object, avg. However, these objects are created only inside the environment of the function. You cannot use those objects outside the function and they will not appear in your Environment window. To pass the value of an object outside of the function, you need to specify what you want to return() or what is the outpute of the function. In this case it is the object avg that we created inside the function. Let’s see the function in action average(a) ## [1] 5.3 Cool! You created your first function. Becuase the function only takes one argument x it knows that whatever object we specify in average() is the object we want to evaluate. But what if our vector contains missing values? b &lt;- c(1,NA,4,2,7,NA,8,4,9,3) average(b) ## [1] NA Uh oh. Here the vector b contains two missing values and the function average(b) returns NA. This is becuase in our function we use the function sum() without specifiying to ignore missing values. If you type in the console ?sum you will see that there is an argument to specify whether missing values should be removed or not. The default value of this argument is FALSE so if we want to remove the missing values we need to specify na.rm = TRUE. It is a good idea to make your functions as flexible as possible. Allow the user to decide what they want to happen. For instance, it might be the case that the user wants a value of NA returned when a vector contains missing values. So we can add an argument to our average() function that allows the user to decide what they want to happen; ignore missing values or return NA if missing values are present. Let’s label this argument na.ignore. We could label it na.rm like the sum() function but for the sake of this Tutorial I want you to learn that you can label these arguments however you want, it is arbitrary. The label should make sense however. Before we write the function let’s think about what we need to change inside the function. Basically we want our new argument na.ignore to change the value of na.rm in the sum() function. If na.ignore is TRUE then we want na.rm = TRUE. Remember that arguments become objects inside of a function. So we will want to change: avg &lt;- sum(x)/length(x) to avg &lt;- sum(x, na.rm = na.ignore)/length(x) Let’s try this out on our vector b na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b) ## [1] 3.8 We can test if our average function is calculating this correctly by using the actual base R function mean(). mean(b, na.rm = TRUE) ## [1] 4.75 Uh oh. We are getting different values. This is because length() is also not ignoring missing values. The length of b, is 10. The length of b ignoring missing values is 8. Unfortunately, length() does not have an argument to specify we want to ignore missing values. How we can tell length() to ignore missing values is by length(b[!is.na(b)]) ## [1] 8 This is saying, evaluate the length of elements in b that are not missing. Now we can modify our function with na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b[!is.na(b)]) ## [1] 4.75 to get average &lt;- function(x, na.ignore = FALSE){ avg &lt;- sum(x, na.rm = na.ignore)/length(x[!is.na(x)]) return(avg) } average(b, na.ignore = TRUE) ## [1] 4.75 mean(b, na.rm = TRUE) ## [1] 4.75 Walla! You did it. You created a function. Notice that we set the default value of na.ignore to FALSE. If we had set it as TRUE then we would not need to specify average(na.ignore = TRUE) since TRUE would have been the default. When using functions it is important to know what the default values are Both for loops and functions allow you to write more concise and readable code. If you are copying and pasting the same lines of code with only small modification, you can probably write those lines of code in a for loop or a function. Something "],
["the-tidyverse.html", "Chapter 4 The tidyverse", " Chapter 4 The tidyverse In this section you will learn how to work with data in R by using a collection of packages known as the tidyverse The tidyverse is a collection of R packages that share an underlying design philosophy, grammar, and data structures. Hadley Wickham has been the main contributor to developing the tidyverse. Although you will be learning R in this tutorial, it might be more appropriate to say that you are learning the tidyverse. The tidyverse consists of packages that are simple and intuitive to use and will take you from importing data (with readr), to transforming and manipulating data structures (with dplyr and tidyr), and to data vizualisation (with ggplot2). Something "],
["importing-and-outputing-data.html", "Chapter 5 Importing and Outputing Data 5.1 CSV 5.2 Tab-Delimited 5.3 SPSS 5.4 RStudio Import GUI 5.5 Merging Data Files", " Chapter 5 Importing and Outputing Data In the Data Preparation stage, every R script that you write will require you to import a data file and output a new data file. In this Chapter you will learn how to import and output comma-separate value (csv), tab-delimited, and SPSS data files. For most of these data types we can use the readr package. The readr package contains useful functions for importing and outputing data files. Go ahead and install the readr package. In the console type: install.packages(&quot;readr&quot;) We will also use the foreign and haven packages for SPSS data files install.packages(&quot;foreign&quot;) install.packages(&quot;haven&quot;) You do not really need to save an R script file for this Chapter. We will use some example data files for this chapter. Go ahead and download these files. You will have to unzip the file. For now just unzip it in your downloads folder. Inside the unzipped folder you will see a number of data files in different file formats. Download Example Data Files 5.1 CSV csv files are by far the easiest files to import into R and most software programs. For this reason, I suggest any time you want to save/output a data file to your computer, do it in csv format. 5.1.1 Import .csv We can import csv files using read_csv() from the readr package. library(readr) read_csv(&quot;filepath/datafile.csv&quot;) You can see this is very simple. We just need to specify a file path to the data. We will talk more about file paths later but for now we will use absolute file paths. First, figure out the absolute file path to your downloads folder (or wherever the unzipped data folder is located). On Windows the absolute file path will usually start from the C:/ drive. On Macs, it starts from ~/ Import the Flanker_Scores.csv file. You might have something that looks like read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) However, this just printed the output of read_csv() to the console. To actually import this file into R, we need to assign it to an object in our Environment. import_csv &lt;- read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) You can name the object whatever you like. I named it import_csv. To view the dataframe View(import_csv) 5.1.2 Output .csv We can output a csv file using write_csv() from the readr package. write_csv(object, &quot;filepath/filename.csv&quot;) Let’s output the object import_csv to a csv file named: new_Flanker_Scores.csv to the downloads folder write_csv(import_csv, &quot;~/Downloads/new_Flanker_Scores.csv&quot;) Note that whenever writeing (outputing) a file to our computer there is no need to assign the output to an object. 5.2 Tab-Delimited tab-delimited files are a little more tedious to import just because they require specifying more arguments. Which means you have to memorize more to import tab-delimited files. 5.2.1 Import .txt To import a tab-delimited file we can use read_delim() from the readr package. read_delim(&quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) There are three additional arguments we have to specify: delim, escape_double, and trim_ws. The notation for tab-delimted files is &quot;\\t&quot;. Let’s import the Flanker_raw.txt file import_tab &lt;- read_delim(&quot;~/Downloads/Flanker_raw.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) View the import_tab object 5.2.2 Output .txt We can output a tab-delimited file using write_delim() from the readr package. write_delim(object, path = &quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;) Output the import_tab object to a file named: new_Flanker_raw.txt write_delim(import_tab, path = &quot;~/Downloads/Flanker_raw.txt&quot;, delim = &quot;\\t&quot;) 5.3 SPSS As horrible as it might sound, there might be occassions where we need to import an SPSS data file. And worse, we might need to output an SPSS data file! I will suggest to use different packages for importing and outputing spss files. 5.3.1 Import .sav To import an SPSS data file we can use read.spss() from the foreign package. library(foreign) read.spss(&quot;filepath/filename.sav&quot;, to.data.frame = TRUE, use.value.labels = TRUE) The use.value.labels argument allows us to import the value labels from an SPSS file. Import and View the sav file CH9 Salary Ex04.sav import_sav &lt;- read.spss(&quot;~/Downloads/CH9 Salary Ex04.sav&quot;) 5.3.2 Output .sav To output an SPSS data file we can use write_sav() from the haven packge. library(haven) write_sav(object, &quot;filepath/filename.sav&quot;) Go ahead and output the import_sav object to a file: new_CH9 Salary Ex04.sav write_sav(import_sav, &quot;~/Downloads/new_CH9 Salary Ex04.sav&quot;) 5.4 RStudio Import GUI The nice thing about R Studio is that there is also a GUI for importing data files. When you are having difficulty importing a file correctly or unsure of the file format you can use the RStudio Import GUI. In the Environment window click on “Import Dataset”. You will see several options available, these options all rely on different packages. Select whatever data type you want to import You will see a data import window open up that looks like this Select “Browse” on the top right and select the data file you want to import. The “Data Preview” window will let you see if it is importing it in the right format. You can change the import options below this. You might want to change the “Name” but you can always do this later in the R Script. Make sure all the settings are correct by assessing the “Data Preview” window. Does the dataframe look as you would expect it to? Finally, copy and paste the code you need in the “Code Preview” box at the bottom right. You might not always need the library(readr) or View(data) lines. Rather than selecting “Import” I suggest just closing out of the window and pasting the code into your R script. csv files have a nice feauture in that RStudio knows that these are file types we might want to import. So instead of navigating through the Import Dataset GUI we can just click on the file in the Files window pane. 5.5 Merging Data Files You might find yourself in a situation where you need to merge multiple files together. There are two types of merge operations that can be performed. In R, a “join” is merging dataframes together that have at least some rows in common (e.g. Same Subject IDs) and have at least one column that is different. The rows that are common serve as the reference for how to “join” the dataframes together. In R, a “bind” is combining datarames together by staking either the rows or columns. It is unlikely that we you will need to do a column bind so we can skip that. A row “bind” takes dataframes that have the same columns but different rows. This will happen if you have separate data files for each subject from the same task. Each subject data file will have their unique rows (subject by trial level data) but they will all have the same columns. The E-Merge software program is performing a row “bind” of each subject .edat file. For E-Prime data we have to go through the E-Merge software program to bind individual subject files. However, you might have individual subject data files not from E-Prime that you need to merge. Or you may want to merge data files from multiple tasks into one big merged file. My datawrangling package contains two functions to merge data files together: files_join() files_bind() They both work in a similar way. The files you want to merge need to be in the same folder on your computer. You specify the location of this folder using the path = argument. You need to specify a pattern that uniquely identifies the files you want to merge (e.g. “.txt”, or “Flanker”) using the pattern = argument. Then specify the directory and filename you want to save the merge file to using the output.file = argument. Here are the arguments that can be specified: path: Folder location of files to be merged pattern: Pattern to identify files to be merged delim: Delimiter used in files. output.delim: Delimiter to be used in output file. Default is , (csv) na: How are missing values defined in files to be merged. Default is NA output.file: File name and path to be saved to. id: Subject ID column name. ONLY for files_join() For example: library(datawrangling) files_bind(&quot;filepath/data/subj&quot;, pattern = &quot;Flanker&quot;, delim = &quot;\\t&quot;, output.file = &quot;filepath/data/filename_merged.csv&quot;) This will bind any files in the directory filepath/data/subj that contain the string &quot;Flanker&quot; and output the mergeed data to a file called filename_merged.csv to the directory filepath/data. Something "],
["data-manipulation-using-dplyr.html", "Chapter 6 Data Manipulation using dplyr 6.1 Setup 6.2 Import 6.3 rename() 6.4 filter() 6.5 select() 6.6 mutate() 6.7 group_by() 6.8 summarise() 6.9 spread() 6.10 Pipe Operator %&gt;%", " Chapter 6 Data Manipulation using dplyr In this Chapter you will learn the fundamentals of data manipulation in R. In the Getting Started in R section you learned about the various types of objects in R. The most important object you will be using is the dataframe. Last Chapter you learned how to import data files into R as dataframes. Now you will learn how to do stuff to that dataframe using the dplyr package (which is of course part of the tidyverse) dplyr is one of the most amazing packages in R. It uses a Grammar of Data Manipulation that is intuitive and easy to learn. The language of dplyr will be the underlying framework for how you will think about manipulating a dataframe. dplyr uses intuitive langauge that you are already familiar with. As with any R function, you can think of functions in the dplyr package as verbs - that refer to performing a particular action on a dataframe. The core dplyr functions are: rename() renames columns filter() filters rows based on their values in specified columns select() selects (or removes) columns mutate() creates new columns based on transformation from other columns, edits values within existing columns group_by() splits dataframe into separate groups based on specified columns summarise() aggregates across rows to create a summary statistic (means, standard deviations, etc.) For more information on these functions Visit the dplyr webpage If you have not done so already, install the dplyr package install.packages(&quot;dplyr&quot;) You will also need the tidyr package (a tidyverse package) for this Chapter install.packages(&quot;tidyr&quot;) Save a new R script file as 6_dplyr.R For this Chapter we will use an example data set from the Flanker task. This data set is a tidy raw data file for over 100 subjects on the Flanker task. There is one row per Trial per Subject and there is RT and Accuracy data on each Trial. Each Trial is either congruent or incongruent. What we will want to do is calculate a FlankerEffect for each Subject so that we end up with one score for each Subject. Go ahead and download the example data set and save it wherever you wish. We will talk about how to organize your data and R scripts in section III. Workflow. Download Example Tidyverse Data 6.1 Setup At the top of your script load the three packages you will need for this Chapter ## Setup library(readr) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidyr) Notice how I added a commented line at the top. Adding comments to your scripts is highly advisable, as it will help you understand your scripts when you come back to them after not working on them for a while. You only need to add a single # to create a commented line. You will also notice that it printed out some warning messages. Sometimes different packages have the same function names. So when you load a package it may override or mask functions from other packages that are already loaded. 6.2 Import Import the data file you downloaded. For now we will just use absolute file paths. Which actually is a BIG No-No. But we will learn more about using relative file paths in section III. Workflow import &lt;- read_csv(&quot;/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial/Data Files/tidyverse_example.csv&quot;) ## Parsed with column specification: ## cols( ## Subject = col_double(), ## TrialProc = col_character(), ## Trial = col_double(), ## Condition = col_character(), ## RT = col_double(), ## ACC = col_double(), ## Response = col_character(), ## TargetArrowDirection = col_character(), ## SessionDate = col_character(), ## SessionTime = col_time(format = &quot;&quot;) ## ) It is always a good idea to get to know your dataframe before you start messing with it. What are the column names? What kind of values are stored in each column? How many observations are there? How many Subjects? How many Trials? etc. To take a quick look at the first few rows of a dataframe use head(). I usually just type this in the console, it is not required to be saved in your Script file. head(import) ## # A tibble: 6 x 10 ## Subject TrialProc Trial Condition RT ACC Response TargetArrowDire… ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 14000 practice 1 incongru… 1086 1 left left ## 2 14000 practice 2 incongru… 863 1 left left ## 3 14000 practice 3 congruent 488 1 right right ## 4 14000 practice 4 incongru… 588 1 right right ## 5 14000 practice 5 congruent 581 1 right right ## 6 14000 practice 6 incongru… 544 1 right right ## # … with 2 more variables: SessionDate &lt;chr&gt;, SessionTime &lt;time&gt; This gives you a good idea of what column names you will be working with and what kind of values they contain. To evaluate what are all the unique values in a column you can use unique(). You can also use this in combination with length() to evaluate how many unique values are in a column. unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; unique(import$Trial) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 ## [103] 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 ## [120] 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 ## [137] 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 ## [154] 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 ## [171] 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 ## [188] 188 189 190 191 192 max(import$Trial) ## [1] 192 length(unique(import$Subject)) ## [1] 410 unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$ACC) ## [1] 1 0 Okay let’s take a look at how to use the dplyr functions to score this data. 6.3 rename() We do not really need to, but let’s go ahead and rename() a column. How about instead of ACC let’s label it as Accuracy. Pretty simple data &lt;- rename(import, Accuracy = ACC) rename() is really only useful if you are not also using select() or mutate(). In select() you can also rename columns as you select them to keep. This will be illustrated this later 6.4 filter() filter() is an inclusive filter and requires the use of logical statements. In Chapter 2: Basic R I talked a little bit about logical statements. Here is a list of logical operators in R: In addition to these logical operators, these functions can be used infilter(): is.na() - include if missing !is.na() - include if not missing between() - values that are between a certain range of numbers near() - values that are near a certain value We do not want to include practice trials when calculating the mean on RTs. We also do not care about neurtral trials. We will use filter() to remove these rows. First let’s evaluate the values in these columns unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; We can specify our filter() in a couple of different ways data &lt;- filter(data, TrialProc != &quot;practice&quot;, Condition != &quot;neutral&quot;) or data &lt;- filter(import, TrialProc == &quot;real&quot;, Condition == &quot;congruent&quot; | Condition == &quot;incongruent&quot;) Specifying multiple arguments separated by a comma , in filter() is equivalent to an &amp; (and) statement. In the second option, since there are two types of rows on Condition that we want to keep we need to specify Condition == twice, separated by | (or). We want to keep rows where Condition == &quot;congruent&quot; or Condition == &quot;incongruent&quot; Notice that I passed the output of this function to a new object data. I like to keep the object import as the original imported file and any changes will be passed onto a new dataframe, such as data. This makes it easy to go back and see what the original data is. Because if we were to overwrite import then we would have to execute the read_csv() import function again to be able to see the original data file, just a little more tedious. Go ahead and view data. Did it properly remove practice trials? How about neutral trials? unique(data$TrialProc) ## [1] &quot;real&quot; unique(data$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; There is a lot of consistency of how you specify arguments in the dplyr package. You always first specify the dataframe that the function is being performed on, followed by the arguments for that function. Column names can be called just like regular R objects, that is without putting the column name in &quot; &quot; like you do with strings. If all you know is dplyr, then this might not seem like anything special but it is. Most non-tidyverse functions will require you to put &quot; &quot; around column names. 6.5 select() select() allows you to select which columns to keep and/or remove. Let’s keep Subject, Condition, RT, Trial, and ACC and remove Response, TrialProc, TargetArrowDirection, SessionDate, and SessionTime. select() is actually quite versatile - you can remove columns by specifying certain patterns. I will only cover a couple here, but to learn more Visit the select() webpage We could just simply select all the columns we want to keep data &lt;- select(data, Subject, Condition, RT, Trial, Accuracy) alternatively we can specify which columns we want to remove by placing a - in front of the columns data &lt;- select(data, -Response, -TrialProc, -TargetArrowDirection, -SessionDate, -SessionTime) or we can remove (or keep) columns based on a pattern. For instance SessionDate and SessionTime both start with Session data &lt;- select(data, -Response, -TrialProc, -TargetArrowDirection, -starts_with(&quot;Session&quot;)) You might start realizing that there is always more than one way to perform the same operation. It is good to be aware of all the ways you can use a function because there might be certain scenarios where it is better or even required to use one method over another. In this example, you only need to know the most straightfoward method of simply selecting which columns to keep. You can also rename variables as you select() them… let’s change Accuracy back to ACC… just beacuse we are crazy! data &lt;- select(data, Subject, Condition, RT, Trial, ACC = Accuracy) We are keeping Subject, Condition, RT, Trial, and renaming ACC to Accuracy. 6.6 mutate() mutate() is a very powerful function. It basically allows you to do any computation or transformation on the values in the dataframe. You can change the values in already existing columns create new columns based on transformation of other columns 6.6.1 Changing values in an existing column Reaction times that are less than 200 milliseconds most likely do not reflect actual processing of the task. Therefore, it would be a good idea to not include these when calculating means. What we are going to do is is set any RTs that are less than 200 milliseconds to missing, NA. First let’s make sure we even have trials that are less than 200 milliseconds. Two ways to do this. 1) View the dataframe and click on the RT column to sort by RT. You can see there are RTs that are as small as 1 millisecond! Oh my, that is definitely not a real reaction time. 2) you can just evaluate the minimum value in the RT column: min(data$RT) ## [1] 0 Now lets mutate() data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT)) Since we are replacing values in an already existing column we can just specify that column name, RT = followed by the transformation. Here we need to specify an if…then… else statment. To do so within the mutate() function we use the function called ifelse(). ifelse() evaluates a logical statement specified in the first argument, RT &lt; 200. mutate() works on a row-by-row basis. So for each row it will evaluate whether RT is less than 200. If this logical statement is TRUE then it will perform the next agrument, in this case sets RT = NA. If the logical statement is FALSE then it will perform the last argument, in this case sets RT = RT (leaves the value unchanged). 6.6.2 Creating a new column Let’s say for whatever reason we want to calculate the difference between the RT on a trial minus the overall grand mean RT (for now, accross all subjects and all trials). This is not necessary for what we want in the end but what the heck, let’s be a little crazy. (I just need a good example to illustrate what mutate() can do.) So first we will want to calculate a “grand” mean RT. We can use the mean() function to calculate a mean. mean(data$RT, na.rm = TRUE) ## [1] 529.1414 Since we replaced some of the RT values with NA we need to make sure we specify in the mean() function to remove NAs by setting na.rm = TRUE. We can use the mean() function inside of a mutate() function. Let’s put this “grand” mean in a column labeled grandRT. First take note of how many columns there are in data ncol(data) ## [1] 5 So after calculating the grandRT we should expect there to be one additional column for a total of 6 columns data &lt;- mutate(data, grandRT = mean(RT, na.rm=TRUE)) Cool! Now let’s calculate another column that is the difference between RT and grandRT. data &lt;- mutate(data, RTdiff = RT - grandRT) We can put all these mutate()s into one mutate() data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) Notice how I put each one on a seperate line. This is just for ease of reading and so the line doesn’t extend too far off the page. Just make sure the commas are still there at the end of each line. 6.7 group_by() This function is very handy if we want to perform functions seperately on different groups or splits of the dataframe. For instance, maybe instead of calculating an overall “grand” mean we want to calculate a “grand” mean for each Subject seperately. Instead of manually breaking the dataframe up by Subject, the group_by() function does this automatically in the background. Like this… data &lt;- group_by(data, Subject) data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) You will now notice that each subject has a different grandRT, simply because we specified group_by(data, Subject). Let’s say we want to do it not just grouped by Subject, but also Condition. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) group_by() does not only work on mutate() - it will work on any other functions you specify after group_by(). I suggest exercising caution when using group_by() because the grouping will be maintained until you specify a different group_by() or until you ungroup it using ungroup(). So I always like to ungroup() immediately after I am done with it. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) data &lt;- ungroup(data) 6.8 summarise() The summarise() function will reduce a data frame by summarising values in one or multiple columns. The values will be summarised on some statistical value, such as a mean, median, or standard deviation. Remember that in order to calculate the FlankerEffect for each subject, we first need to calculate each subject’s mean RT on incongruent trials and their mean RT on congruent trials We’ve done our filtering, selecting, mutating, now let’s aggergate RTs accross Condition to calculate mean RT. We will use a combo of group_by() and summarise(). summarise() is almost always used in conjunction with group_by(). data &lt;- group_by(data, Subject, Condition) data &lt;- summarise(data, RT.mean = mean(RT, na.rm = TRUE)) %&gt;% ungroup() To summarise() you need to create new column names that will contain the aggregate values. RT.mean seems to make sense to me. What does the resulting data frame look like? There should be three rows per subject, one for incongruent trials, one for congruent trials, and one for neutral trials. You can see that we now have mean RTs on all conditions for each subject. Also, notice how non-group_by columns got removed: Trial, and ACC. 6.9 spread() Our data frame now looks like head(data) ## # A tibble: 6 x 3 ## Subject Condition RT.mean ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 14000 congruent 394. ## 2 14000 incongruent 487. ## 3 14001 congruent 389. ## 4 14001 incongruent 406. ## 5 14002 congruent 452. ## 6 14002 incongruent 508. Ultimately, we want to have one row per subject and to calculate the difference in mean RT between incongruent and congruent conditions. It is easier to calculate the difference between two values when they are in the same row. Currently, the mean RT for each condition is on a different row. What we need to do is reshape the dataframe. To do so we will use the spread() function from the tidyr package. The tidyr package, like readr and dplyr, is from the tidyverse set of packages. The spread() function will convert a long data frame to a wide dataframe. In other words, it will spread values on different rows across different columns. In our example, what we want to do is spread() the mean RT values for the two conditions across different columns. So we will end up with is one row per subject and one column for each condition. Rather than incongruent, and congruent trials being represented down rows we are spreading them across columns (widening the data frame). The two main arguments to specify in spread() are key: The column name that contains the variables to create new columns by (e.g. “Condition”) value: The colunn name that contains the values (e.g. “RT”) data &lt;- spread(data, key = Condition, value = RT.mean) Now our dataframe looks like head(data) ## # A tibble: 6 x 3 ## Subject congruent incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 394. 487. ## 2 14001 389. 406. ## 3 14002 452. 508. ## 4 14003 563. 659. ## 5 14004 520. 643. ## 6 14005 469. 540. From here it is pretty easy, we just need to create a new column that is the difference between incongruent and congruent columns. We can use the mutate() function to do this data &lt;- mutate(data, FlankerEffect = incongruent - congruent) Perfect! Using the readr, dplyr, and tidyr packages we have gone from a “tidy” raw data file to a dataframe with one row per subject and a column of FlankerEffect scores. 6.10 Pipe Operator %&gt;% One last thing about the dplyr package. dplyr allows for passing the output from one function to another using what is called a pipe operatior. The pipe operator is: %&gt;% This makes code more concise, easier to read, and easier to edit. When you pass the output of one function to another with %&gt;% you do not need to specify the dataframe (input) on the next function. %&gt;% implies that the input is the output from the previous funciton, so this is made implicit. We can pipe all the functions in the chapter together as such ## Setup library(readr) library(dplyr) library(tidyr) ## Import import &lt;- read_csv(&quot;/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial/Data Files/tidyverse_example.csv&quot;) ## Score data &lt;- import %&gt;% rename(Accuracy = ACC) %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% select(Subject, Condition, RT, Trial, ACC = Accuracy) %&gt;% group_by(Subject, Condition) %&gt;% mutate(RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE)) %&gt;% ungroup() %&gt;% spread(key = Condition, value = RT.mean) %&gt;% mutate(FlankerEffect = incongruent - congruent) Virtually all the R scripts you write will require the dplyr package. The more you know what it can do, the easier it will be for you to write R Scripts. I highly suggest checking out these introductions to dplyr. https://dplyr.tidyverse.org https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html Something "],
["common-data-manipulations.html", "Chapter 7 Common Data Manipulations 7.1 Descriptive Statistics 7.2 Centering and Standardizing Variables 7.3 Trimming 7.4 Composites 7.5 Scale Transformations 7.6 Custom Transformations", " Chapter 7 Common Data Manipulations In R, the term data wrangling is often times used to refer to performing data manipulation and transformations. The functions you will learn about in this Chapter come from the datawrangling package I developed. There are certain data transformations we use on a regular basis that would require several steps and lines of code to do. datawrangling allows you to perform these transformation in a single line of code. Hopefully, datawrangling will get you to start using R more easily for data analysi. I am hosting the datawrangling package on GitHub. To download packages on GitHub you first need to download the devtools package. install.packages(&quot;devtools&quot;) Now install the datawrangling package: devtools::install_github(&quot;dr-JT/datawrangling&quot;) Save a new R script file as 7_transform.R For this Chapter, let’s create a dataframe to use as an example for common data manipulations using datawrangling. Don’t worry about what this code means for now, just copy it into your script and run it. import &lt;- data.frame(ID = c(1:100), Score1 = rnorm(100, mean = 2, sd = .8), Score2 = rnorm(100, mean = 7, sd = 1.1), Score3 = rnorm(100, mean = 10, sd = 1.8), Score4 = rnorm(100, mean = 20, sd = 2.3)) head(import) ## ID Score1 Score2 Score3 Score4 ## 1 1 2.2900663 8.695905 9.542858 20.94829 ## 2 2 2.3145157 5.830390 9.826708 23.19553 ## 3 3 0.5003731 7.991509 12.893764 21.41399 ## 4 4 1.8286620 6.503109 11.141769 16.15341 ## 5 5 1.7467929 6.693397 8.324463 20.75273 ## 6 6 1.4164575 5.959133 10.432110 16.03588 7.1 Descriptive Statistics First you should know how to compute some basic descriptive statistics. Basic descriptive statistics include mean, median, standard deviation, max, min, skew, kurtosis, etc… The functions to calculate these are pretty straightforward: Base R maximum: max() minimum: min() count:n() mean: mean() median: median() standard deviation: sd() variance: var() quantiles (percentiles): quantile() specify the percentiles with the argument probs = (default is c(0, .25, .5, .75, 1)) e1071 package skewness: skewness(variable, na.rm = TRUE, type = 2) kurtosis: kurtosis(variable, na.rm = TRUE, type = 2) For all of these you need to specify na.rm = TRUE if the variable column has missing data. It is best to just always set na.rm = TRUE. For example, mean(variable, na.rm = TRUE) To calculate the overall mean on Score1 would look like library(dplyr) data &lt;- import %&gt;% mutate(Score1.mean = mean(Score1, na.rm = TRUE)) 7.2 Centering and Standardizing Variables The function center() will create either unstandardized or standardized (z-scored) centered variables. The list of arguments that can be passed onto the function are: x: dataframe variables: c() of columns to center standardize: Logical. Do you want to calculate zscores? (Default = FALSE) Example: library(datawrangling) data &lt;- center(import, variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), standardize = TRUE) View the dataframe data. You will notice that there are now 4 additional columns: Score1_z, Score2_z, Score3_z, and Score4_z. If you choose to to calculate centered (unstandardized) scores, then standardize = FALSE. And it will create variables with the suffix _c. 7.3 Trimming The function trim() will replace outlier scores that exceed a certain z-score cutoff. There are several options for how to replace the outlier scores. Replace with “NA” (missing value) “cutoff” (the z-score cutoff value, e.g. 3.5 SDs) “mean” “median” The arguments that can be specified are: x: dataframe variables: c() of variables to be trimmed. option to set variables = &quot;all&quot; to trim all variables in a dataframe. But then must specify id = cutoff: z-score cutoff to use for trimming (default: 3.5) replace: What value should the outlier values be replaced with. (default: replace = “NA”) id: Column name that contains subject IDs. **ONLY needs to be used if variables = &quot;all&quot; Example: data &lt;- import %&gt;% trim(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), cutoff = 3.5, replace = &quot;NA&quot;, id = &quot;ID&quot;) ## Warning in if (variables == &quot;all&quot;) {: the condition has length &gt; 1 and only ## the first element will be used Notice how you don’t even need to center() the variables first. The centering is being done inside of trim(). You can evaluate outliers and replace with different values (replace =) all in one function and one line of code. 7.4 Composites The composite() function allows you to easily create a composite score from multiple variables and also specifiy a certain criteria for how many missing values are allowed. data &lt;- import %&gt;% composite(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;), type = &quot;mean&quot;, standardize = TRUE, name = &quot;Score_comp&quot;, missing.allowed = 1) The function composite() will create composite scores out of specified columns. Right now you can only create “mean” composite scores. In the future I plan on adding “sum” and “factor score” composite types. Here is a list of the arguments you can specifiy: x: dataframe variables: c() of columns to create the composite from type: What type of composite should be calculated?, i.e. mean or sum. (Default = “mean”). standardize: Logical. Do you want to calculate the composite based on standardized (z-score) values? (Default = TRUE) name: Name of the new composite variable to be created missing.allowed: Criteria for the number of variables that can having missing values and still calculate a composite for that subject The remaining functions do not come from the datawrangling package but you may find them useful nonetheless. 7.5 Scale Transformations 7.5.1 log [insert base off of Field] 7.5.2 polynomial You can create orthogonal polynomials of variables using the poly() function and specify the degree of polynomial to go up to with degree = poly(import$Score1, degree = 3) You can see it creates up to three degrees of polynomials on the Score1 variable. The first degree is a linear, second is a quadratic, and third is cubic. Let’s say we want to create three new columns with each of these three polynomials. To do so we need to individually access each vector such as poly(import$Score1, degree = 3)[,1] library(dplyr) data &lt;- import %&gt;% mutate(Score1.linear = poly(Score1, degree = 3)[ , 1], Score1.quadratic = poly(Score1, degree = 3)[ , 2], Score1.cubic = poly(Score1, degree = 3)[ , 3]) Here is plot to show you visually what happened 7.6 Custom Transformations In general, with mutate() you can specify any custom transformation you want to do on a variable. For instance, if you want to subtract each score by 5, and divide by 10 then you can do it! I don’t know why you would ever want to do that, but you can. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 - 5)/10) Or take the sum of Score1 and Score2 and divide by the difference between Score3 and Score4. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 + Score2)/(Score3 - Score4)) Something "],
["example-data-set.html", "Chapter 8 Example Data Set 8.1 Initial Setup 8.2 Example Data: Flanker Task", " Chapter 8 Example Data Set In the next couple of Chapters you will go over an example of how to write and organize scripts for Data Preparation. Data Preparation involves Stages 1 and 2 in the data procesing workflow diagram. Go ahead and download the example data set Download Example Flanker Data 8.1 Initial Setup In Section IV - Data Science Practices, you will learn more about the importance of organization and good practices to let you take full advantage of data analysis in R. To go ahead and get you started using R I will have you implement these data science practices without talking too much about them yet. 8.1.1 RStudio Project RStudio has a feature called Projects. RStudio Projects allow you to keep your R environment for one project separated from another project. Visit this page for more details on R Projects. If you have not done so already, go ahead and create an R Project for this tutorial. Navigate to File -&gt; New Project… -&gt; Choose Existing Directory if you already created an “R Tutorial” folder on your computer or New Directory -&gt; New Project if you have not. Create the R Project in the root folder of your R Tutorial. You should now see a file called R Tutorial.Rproj, or whatever you named your project/root directory. Open your RStudio Project. There are a couple of ways you can open an RStudio Project. One way is to just simply open the .Rproj file. This will open a new R Session (and RStudio window). If you already have an RStudio window open you can navigate to the very top-right of the application window and browse different projects you have recently worked on. This is where you can also see which Project you currently have open. 8.1.2 Directory Organization Now let’s setup your directory organization. In the directory where your .Rproj file is located, you should create the following folders: R Scripts - A folder to put all your scripts in one place Data Files - A folder where any data files will be stored Results - A folder where any outputed results and figures will be stored When conducting a study you might have other directores such as Tasks where the task files are located, or Documents where any documents such as Methods or other materials are located. The Results directory will become relevant later when we get to Stage 3: Data Analysis. Within the Data Files directory you should create the following folders: Raw Data - A folder containing raw data files. Scored Data - A folder containig scored data files. Within the Raw Data folder you should create the following directory E-Merge - A folder containing E-Merged and exported .txt merged files. This structure helps to keep clear where we are importing and outputing data files to in the data processing workflow stages. The messy raw data files are located in the E-Merge folder. The tidy raw data files are located in the Raw Data folder. The scored task files are located in the Scored Data folder. With your R Scripts and data files in the messy raw data folder, you will be able to create, re-create, and modify the tidy and scored data files as well as outputed analyses 100% by simply running your R Scripts. With a click of a button! Go ahead and unzip the Example data set you downloaded. There are some files that are already Scored, go ahead and put these in Data Files/Scored Data. Put the other files/folders in Data Files/Raw Data/E-Merge. One of the raw files is an E-Merged file, the other is the E-Merged file exported to a tab-delimited .txt file. There is also a folder labled subj. We will come back to what these files are later. 8.1.3 R Scripts For any given project, you will have multiple R Scripts at all three stages of the data processing workflow. How are you to organize them and have a convenient way to run your scripts from Stage 1: messy to tidy raw, to Stage 2: tidy raw to scored, to Stage 3: data analysis? I very very highly recommend you just put all your R Scripts into ONE and ONLY ONE folder. That way you don’t have to go searching all over the place for different scripts. Then, I also suggest that your name them using a convention that makes sense and will organize your scripts according the the three stages of the data processing workflow. 8.1.4 Masterscript Having a masterscript is a very convenient way to run and re-run all or just part of your analyses. Rather than opening each individual script and sourcing (executing all lines of code) each one, you can source() each script individually or all together in the masterscript. Therefore, the masterscript will only really contain lines of code that sources other scripts. It is the control center for your data analysis. Save an R Script file in your project’s root directory (where the .Rproj file is located) Save a new R script file as RTutorial_masterscript.R For now, just copy and paste the following code into your masterscript. I will go over the purpose of all this in Chapter 14: Templates. However, you can see that it is basically setting up a directory structure we can use to import and output files, and corresponds to the data processing workflow stages. ## Setup #### ## Load Packages library(here) library(rmarkdown) ## Specify the directory tree directories &lt;- list(scripts = &quot;R Scripts&quot;, data = &quot;Data Files&quot;, raw = &quot;Data Files/Raw Data&quot;, messy = &quot;Data Files/Raw Data/E-Merge&quot;, scored = &quot;Data Files/Scored Data&quot;, results = &quot;Results&quot;) saveRDS(directories, here(&quot;directories.rds&quot;)) ############# ############################################# #------ 1. &quot;messy&quot; to &quot;tidy&quot; raw data ------# ############################################# ################################################# #------ 2. &quot;tidy&quot; raw data to Scored data ------# ################################################# ############################################################# #------ 3. Create Final Merged Data File for Analysis ------# ############################################################# ############################### #------ 4. Data Analysis ------# ############################### rm(list=ls()) 8.2 Example Data: Flanker Task Why the Flanker task? Because we love the Flanker task! Not really, but it is a good task to illustrate most of the functions you will want to use for other tasks. Basically, on the Flanker task the subject responds to the direction of a centrally presented arrow. There are two types of trials, congruent and incongruent. On congruent trials, the arrows surrounding (flanking) the central arrow are in the same direction as the center arrow. In the incongruent condition, the arrows are facing the opposite direction. Reaction time is slower on incongruent trials compared to congruent trials. The difference in Reaction Time is the dependent measure for this task, and we can call this score a FlankerEffect. Our goal for the next couple of Chapters is to get a data file with each subjects score on the FlankerEffect starting from a messy raw data file. Something "],
["merging-individual-subject-files.html", "Chapter 9 Merging Individual Subject Files 9.1 Setup 9.2 files_bind()", " Chapter 9 Merging Individual Subject Files This Chapter is not Required For the most part, you will merge individual subject files through the E-Merge program because the individual files are in the proprietary .edat2 file type that cannot be opened outside of E-Prime programs. However, there may be cases where you have individual subject files not in .edat2 but in .txt, .csv, or .xlsx file types. These can easily be merged into one data file using R. While in the middle of data collection you may want to look at your data or run some analysis. In that case, each time you want to do so you will need to merge individual subject data files. Again, usually we do this through the manual and tedious process of E-Merge. Once you are finished with data collection you will only have to merge individual subject data files one last time. This step is sort of a precursor to data processing and in R we can write a script for this step separately, if needed. Save a new R script file as ._study_finish.R in the R Scripts folder I have created a function to easily merge individual subject files together. The files_bind() function comes from my datawrangling package. To install datawrangling you first need to install devtools. If you have not done so already, install devtools by typing in your console window install.package(&quot;devtools&quot;) Now install datawrangling by typing in your console window devtools::install_github(&quot;dr-JT/datawrangling&quot;) 9.1 Setup Source the masterscript After sourcing the masterscript you should see a new file in your project’s root directory called directiories.rds. For now don’t do anything with it but just be aware that it is there. First, at the top of your script load the required packages and import the directories.rds file library(here) ## here() starts at /Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial library(datawrangling) directories &lt;- readRDS(here(&quot;directories.rds&quot;)) For now you do not need to know how the here package works. I will talk about it more in the next Chapter and explain why we use it more in Chapter 14. 9.2 files_bind() See Chapter 5: Importing and Outputing Data for more detailed information on file_bind(). The arguments you need to specify in files_bind() are: path: The file path where the data are located delim: The delimiter or filetype of the data files. (i.e. `&quot; for tab-delimited) output.file: The filepath and filename to save the merged data file as By default the merged file will be saved as a .csv file. You can change this by setting output.delim. In your script enter the following line of code datawrangling::files_bind(here(directories$messy, &quot;subj&quot;), delim = &quot;\\t&quot;, output.file = here(directories$raw, &quot;Flanker.csv&quot;)) The first argument here(directories$messy, &quot;subj&quot;) is just specifiying the file directory path. Go ahead and type this in your console window to see how this is the case. here(directories$messy, &quot;subj&quot;) ## [1] &quot;/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial/Data Files/Raw Data/E-Merge/subj&quot; Again, the here() function will be talked more about in later Chapters. If you have more tasks in which you need to merge individual subject data files you would then just include a similar line of code specific for that task (different file path and different filename). Source the script You should now see a data file Flanker.csv. This is the merged messy raw data file. The Flanker.txt file is the merged messy raw data file that was exported from the E-Merged file. Therefore, these two dat files are equivalent except that one is tab-delimited and one is a comma-separated value. In thi next Chapter you will create a tidy raw data file from this messy raw data file. I will leave it up to you which version of the messy raw data file you want to use. The only thing that will differ is how you import the file. Something "],
["messy-to-tidy.html", "Chapter 10 Messy to Tidy 10.1 Setup 10.2 Import 10.3 Tidy raw data 10.4 Output 10.5 Copy and Paste 10.6 Masterscript", " Chapter 10 Messy to Tidy Stage 1 in the data processing workflow is converting messy raw data files to tidy raw data files. In this Chapter you will go over an example of how to write an R Script for this stage of data processing. This part is not always fun and it can be very tempting to skip and go straight to creating a scored data file that is ready for data analysis. Because data analysis is the fun part! However, I strongly advise against that. There are at least a few good reasons why: Sometimes you actually NEED the raw trial level data. For instance, to do reliability or internal consistency analyses. Visualizing and analyzing trial-level data can help you better understand your data. You or some other researcher might want to go back and run analyses starting from the trial-level data. Maybe you/they want to score the data slightly differently than you did before. Data storage. Storing and sharing your data in a tidy raw data format makes SO MUCH more sense than storing your messy raw data. If working with your messy raw data gives you a headache now imagine how much worse that is when you have not thought about that data for a long time. And so many other reasons In the previous Chapter you should have downloaded the example Flanker data set, setup your directory organization, and created an RStudio Project. Go ahead and open the RStudio Project if you have not done so already. Save a new R script file as 1_flanker_raw.R in the R Scripts folder Source the masterscript After sourcing the masterscript you should see a new file in your project’s root directory called directiories.rds. For now don’t do anything with it but just be aware that it is there. There are 4 sections to create in the script: Setup Import Tidy Output 10.1 Setup In the Setup section we 1) load any packages, using library(), that are required for the script, 2) set the import/output filepaths and filenames, and 3) set any other parameters for the script. Let’s go over how to create the setup section one piece at a time. 10.1.1 Load packages here: You will use here to reference import and output file paths readr: You will use readr to import and export files dplyr: You will use dplyr to change the messy raw data file into a tidy raw data file datawrangling: You will use a function in this package to remove duplicate subject numbers in the messy raw data file. At the top of your script write the following code (with commented sections) ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) 10.1.2 Set Import/Output Directories It is good practice to set the import and output directories at the top of your script. First, you need to import the file directories.rds. This is not standard, it is a method that is likely unique to me. I came up with it on my own. But I have found it useful to use directories.rds to manage file paths across different scripts within the same project. And it is pretty simple to use. Let’s see how this works. Import directories.rds directories &lt;- readRDS(here(&quot;directories.rds&quot;)) Now print the directories object to your console directories You can see that directories is a list object that contains different filepaths. To access those file paths: directories$raw Or directories$messy These are relative file paths from the projects root directory. We can set the import and output file directories as such ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$messy output.dir &lt;- directories$raw 10.1.3 Set Import/Output File Names Now set the import and output file names. Let’s output the tidy raw data files as Flanker_raw.csv. ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$messy output.dir &lt;- directories$raw ## Set Import/Output Filenames task &lt;- &quot;Flanker&quot; import.file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ############# That is all we need to include in the Setup block. Notice how I added some #### on the top line after Setup and then included more ############# at the very bottom of the Setup block, after the Set Import/Output Filenames section. This helps to organize your script by blocks. For instance, you can collapse all the code in the Setup block by clicking on the downward arrow/carrot to the right of Setup. 10.2 Import The next block of code is for importing the data file. After the Setup block add a section for the Import block ## Import #### ############## You can import the Flanker.txt tab-delimited file using read_delim() from the readr package. To specificy the file path we can use the here() function from the here package. When you loaded the here package using library(here) it searched for a .Rproj file. Now when you use the function here() it will create a filepath to the directory where the .Rproj file is located, your projects root directory. Go ahead and type here() into the console. library(here) here() To specify a filepath and filename in our project directory, we can use here() here(&quot;filepath&quot;, &quot;filename&quot;) Using the import.dir and import.file objects we created in the Setup block, this makes it really easy to specify the filepath and filename. here(import.dir, import.file) ## [1] &quot;/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial/Data Files/Raw Data/E-Merge/Flanker.txt&quot; ## Import #### import &lt;- read_delim(here(import.dir, import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ############## Now it happens on occasion that the wrong subject number is entered in when an RA is starting up a task. This can result in duplicate Subject numbers in the E-Merge file. Luckily I have created a function to remove the duplicate subjects, and put their information (with session date and time) into a specific file. This file will be created in a new folder called “duplicates”. The function is duplicates_remove() from the datawrangling package on my GitHub. It can be difficult to remember what arguments you need to include in a function. To see helpful documentation about a function you can type in the console ?duplicates_remove ## Import #### import &lt;- read_delim(here(import.dir, import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) %&gt;% duplicates_remove(taskname = task, output.folder = here(output.dir, &quot;duplicates&quot;)) ############## View the dataframe View(import) It is a mess, right? Here are some things you need to know about the messy raw data file. These are the columns and what type of values they contain: Subject: Subject number Procedure[Trial]: Procedure type (keep: TrialProc and PracTrialProc) PracTrialList.Sample: Trial number for practice trials TrialList.Sample: Trial number for real trials FlankerType: condition for real and practice trials (Values are: congruent, incongruent, and neutral) PracSlideTarget.RT: Reaction time for practice trials PracSlideTarget.ACC: Accuracy for practice trials PracSlideTarget.RESP: Response for practice trials ({LEFTARROW} = left and {RIGHTARROW} = right) SlideTarget.RT: Reaction time for real trials SlideTarget.ACC: Accuracy for real trials SlideTarget.RESP: Response for real trials ({LEFTARROW} = left and {RIGHTARROW} = right) TargerDirection: direction of the target arrow for practice trials TargetDirection: direction of the target arrow for real trials SessionDate: Date of session SessionTime: Time of session 10.3 Tidy raw data The next block is where we actually tidy the messy raw data. This basically involves filtering rows, renaming columns, changing values in columns, and selecting columns. And accordingly you will use these dplyr functions to do so: filter() rename() mutate() select() This is why dplyr is so intuitive. The language of dplyr directly corresponds to how we think about working with data. Create a block for tidying the raw data after the Import block ## Tidy raw data #### ##################### 10.3.1 Filter Filter only relevant rows. We want to keep only the rows that contain trials from the practice and real trials. ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) ##################### The column names are contained in single quotes because the names contain the special characters [ ]. There are certain characters R does not like to use as variable names and one of them is square brackets. 10.3.2 Rename Columns Now the column specifying real vs practice trials is a little tedious to keep typing out since it requires the single quotes and brackets. To rename columns we can use rename() function in dplyr. ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) ##################### 10.3.3 Change Values Chabge the values in TrialProc. unique(data_raw$TrialProc) ## [1] &quot;PracTrialProc&quot; &quot;TrialProc&quot; Right now real trials have the value of TrialProc. The same name as the column, not good! And the “practice” trials have the value of PractTrialProc. Let’s simply change these values to real and practice, respectively. ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;)) ##################### Evaluate that this worked unique(data_raw$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; Okay now let’s move on to figuring out what other columns we want to keep and if we need to do any more computations on them. We want to keep the columns that specify the following information Subject number TrialProc (real vs practice) Trial number Condition (congruent vs incongruent) Reaction time Accuracy Response Target arrow direction (left or right) Session Date Session Time This gets a little more tricky here because the information for some of these variables are in one column for practice trials and a different column for real trials. That means we need to merge the information from these two columns into one. For instance the RT data for practice trials is contained in the column PracSlideTarget.RT and for real trials RT data is in SlideTarget.RT. We can create a new column labeled RT that on real trials, values are taken from the SlideTarget.RT column and on practice trials, values are taken from the PracSlideTarget.RT column. ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT)) ##################### We can do the same thing for trial, accuracy, response, and target arrow direction. Combining them all into one mutate() function ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT), Trial = case_when(TrialProc == &quot;real&quot; ~ TrialList.Sample, TrialProc == &quot;practice&quot; ~ PracTrialList.Sample), Accuracy = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.ACC, TrialProc == &quot;practice&quot; ~ PracSlideTarget.ACC), Response = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RESP, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RESP), TargetArrowDirection = case_when(TrialProc == &quot;real&quot; ~ TargetDirection, TrialProc == &quot;practice&quot; ~ TargerDirection)) ##################### Notice how I sometimes put case_when() on a different line. This is just to make sure the code does not run too far off to the right so we do not have to horizontally scroll too much. You might want to change the values in the Response and CorrectResponse columns to be more clear (left and right). ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT), Trial = case_when(TrialProc == &quot;real&quot; ~ TrialList.Sample, TrialProc == &quot;practice&quot; ~ PracTrialList.Sample), Accuracy = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.ACC, TrialProc == &quot;practice&quot; ~ PracSlideTarget.ACC), TargetArrowDirection = case_when(TrialProc == &quot;real&quot; ~ TargetDirection, TrialProc == &quot;practice&quot; ~ TargerDirection), Response = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RESP, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RESP), Response = case_when(Response == &quot;z&quot; ~ &quot;left&quot;, Response == &quot;{/}&quot; ~ &quot;right&quot;)) ##################### 10.3.4 Select Columns The last thing to do is select only the columns we want to keep. Remember we want to only select columns with the following information Subject number Trial number Condition Reaction time Accuracy Response Correct Response Target arrow direction Session Date Session Time ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT), Trial = case_when(TrialProc == &quot;real&quot; ~ TrialList.Sample, TrialProc == &quot;practice&quot; ~ PracTrialList.Sample), Accuracy = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.ACC, TrialProc == &quot;practice&quot; ~ PracSlideTarget.ACC), TargetArrowDirection = case_when(TrialProc == &quot;real&quot; ~ TargetDirection, TrialProc == &quot;practice&quot; ~ TargerDirection), Response = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RESP, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RESP), Response = case_when(Response == &quot;z&quot; ~ &quot;left&quot;, Response == &quot;{/}&quot; ~ &quot;right&quot;)) %&gt;% select(Subject, TrialProc, Trial, Condition = FlankerType, RT, Accuracy, Response, TargetArrowDirection, SessionDate, SessionTime) ##################### And that is it for the Tidy raw data block! Almost done 10.4 Output The final block is to Output or Save the data file to your computer. This step is too easy ## Output #### write_csv(data_raw, here(output.dir, output.file)) ############## That’s it! One last thing that is good to put at the end of every script. rm(list=ls()) This will remove all the objects from the current Environment. Then if we were to put it all together, using the template from the previous chapter: ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$messy output.dir &lt;- directories$raw ## Set Import/Output Filename task &lt;- &quot;Flanker&quot; import.file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ############# ## Import #### import &lt;- read_delim(here(import.dir, import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) %&gt;% duplicates_remove(taskname = task, output.folder = here(output.dir, &quot;duplicates&quot;)) ############## ## Tidy raw data #### data_raw &lt;- import %&gt;% filter(`Procedure[Trial]`==&quot;TrialProc&quot; | `Procedure[Trial]`==&quot;PracTrialProc&quot;) %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;TrialProc&quot; ~ &quot;real&quot;, TrialProc == &quot;PracTrialProc&quot; ~ &quot;practice&quot;), RT = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RT, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RT), Trial = case_when(TrialProc == &quot;real&quot; ~ TrialList.Sample, TrialProc == &quot;practice&quot; ~ PracTrialList.Sample), Accuracy = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.ACC, TrialProc == &quot;practice&quot; ~ PracSlideTarget.ACC), TargetArrowDirection = case_when(TrialProc == &quot;real&quot; ~ TargetDirection, TrialProc == &quot;practice&quot; ~ TargerDirection), Response = case_when(TrialProc == &quot;real&quot; ~ SlideTarget.RESP, TrialProc == &quot;practice&quot; ~ PracSlideTarget.RESP), Response = case_when(Response == &quot;z&quot; ~ &quot;left&quot;, Response == &quot;{/}&quot; ~ &quot;right&quot;)) %&gt;% select(Subject, TrialProc, Trial, Condition = FlankerType, RT, Accuracy, Response, TargetArrowDirection, SessionDate, SessionTime) ##################### ## Output #### write_csv(data_raw, here(output.dir, output.file)) ############## rm(list=ls()) Great! You have written an R script for the first step in Data Preparation, converting a messy raw data file to a tidy raw data file. 10.5 Copy and Paste Notice how pretty much everything except the Tidy block can be used from one task script to another. Besides the Tidy block, you basically would just have to change the task, import.file, and output.file objects. The Import and Output blocks can literally be copy and pasted without changing anything at all. The only thing that changes is what happens inside of filter(), rename(), mutate(), and select(). This means you can spend less time thinking about how you need to organize your script, and importing and outputing files. Instead, you can focus on the meat of the script, what happens in the Tidy block. 10.6 Masterscript Finally, you should add a line of code to the masterscript that sources the file 1_flanker_raw.R. source(here(&quot;R Scripts&quot;, &quot;1_flanker_raw.R&quot;), echo = TRUE) Something "],
["data-cleaning-and-scoring.html", "Chapter 11 Data Cleaning and Scoring 11.1 Setup 11.2 Import 11.3 Data Cleaning and Scoring 11.4 Trimming 11.5 Calculate FlankerEffect 11.6 Remove Subjects 11.7 Calculate Binned Scores 11.8 Merge 11.9 Save data file 11.10 Masterscript", " Chapter 11 Data Cleaning and Scoring Stage 2 in the data processing workflow is data cleaning and scoring. In this Chapter you will go over an example of how to write an R Script for this stage of data processing. In the previous Chapter you created a tidy raw data file that had one row per trial (including practice trials). In this Chapter you will use the tidy raw data file to calculate the FlankerEffect for each Subject. You will end up with a data file that has only one row per Subject. The FlankerEffect is the difference of mean RTs on incongruent trials minus congruent trials. Save a new R script file as 2_flanker_score.R in the R Scripts folder There are 4 blocks of code to create in the script: Setup Import Clean and Score Output 11.1 Setup The Setup block for this script will be very similar to the Setup for the raw script you created in the previous Chapter. However, we will also add some parameters for data cleaning criteria. You can copy and paste the code from the raw script for the Setup block and then modify it. The import directory should be the raw data folder (the output of the raw script created last Chapter). The output directory should be the scored data folder. The import filename is Flanker_raw.csv (the outputed file from the raw script). The output filename is Flanker_Scores.csv. ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) library(englelab) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set Import/Output Filenames task &lt;- &quot;Flanker&quot; import.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output.file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ############# 11.1.1 Data Cleaning Parameters In the example we are about to go through we will implement the following data cleaning procedures when calculating the FlankerEffect. They will be implemented in the following order rt.min: Set RTs less than 200ms to missing (NA) and Accuracy to incorrect (0). rt.trim: Trim RTs. Replace Outlier RTs that are above or below 3.5 SDs of the mean, with values exactly at 3.5 SDs above or below the mean. This is evaluated for each Subject by each condition seprately. acc.criteria: Finally remove subjects that performed less than 3.5 standard deviations below the mean accuracy on any Condition (congruent or incongruent). Let’s add these data cleaning parameters to the Setup block ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) library(englelab) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set Import/Output Filenames task &lt;- &quot;Flanker&quot; import.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output.file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ## Set Data Cleaning Params rt.min &lt;- 200 rt.trim &lt;- 3.5 acc.criteria &lt;- -3.5 ############# You will now be able to use these objects rt.min, rt.trim, and acc.criteria when writing the code to actually do the data cleaning. This is useful because if you want to change these parameters you can do it right here in the Setup block rather than searching through all your code and figure out where you need to change the values. 11.2 Import The import block is very simple, especially since we are just importing a .csv file. ## Import #### import &lt;- read_csv(here(import.dir, import.file)) ############## 11.3 Data Cleaning and Scoring The next block is where we do the actual data cleaning and task scoring. This step is more complicated and often times requires some forethought. But we don’t always have the best forethought so you will likely re-write previous lines of code. One thing you must think about before writing the script for this stage is the statistical analyses you eventually plan on conducting. The type of statistical analyses you plan on conducting will determine the final dataframe you want to end up at in this stage of data preparation. What are the final dependent variables (or task scores) you want to calaculate? In the Flanker task there are several task scores we might want to calculate (in a regression context). FlankerEffect on RT: Mean reaction time difference between incongruent and congruent trials FlankerEffect on Accuracy: Mean accuracy difference between incongruent and congruent trials Flanker Binned Scores: A scoring method to combine accuracy and reaction time (an alternative to difference scores) 11.4 Trimming First we need to get rid of practice trials / keep only real trials. ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) ################################# Then, set RTs less than 200ms to missing (NA) and Accuracy to 0 using mutate() and ifelse() ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) ################################# And, Trim RTs, grouped by Subject and Condition using the trim() function from my datawrangling package. trim() is an easy way to trim values on a variable using a certain z-score cutoff. The main arguments to pass onto trim() are: variables: The column name that contains the values you want to trim cutoff: What z-score cutoff value you want to use replace: How you want to replace the outlier values. Options are, &quot;mean&quot;, &quot;cutoff&quot;, or &quot;NA You can use group_by() with trim() to trim independently for each Condition (congruent, incongruent, neutral). Always ungroup() afterwards. ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = rt.trim, replace = &quot;cutoff&quot;) %&gt;% ungroup() ################################# We will implement the third data cleaning procedure later. 11.5 Calculate FlankerEffect What we want to do is calculate both the FlankerEffect and FlankerBinned Scores. These are separate scoring procedures. The general approach we will take is to create two separate dataframes for each procedure, based off the data_trim. Then we will merge the two dataframes back into one. First calculate the FlankerEffect. We want to calcualte the FlankerEffect on RT using only Accurate trials. Since we also want to calculate the FlankerEffect on Accuracy we cannot just use a filter(). Instead we should mutate() the values in RT to be NA when Accuracy is 0. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) Next step is to calculate mean RT and mean Accuracy separately for congruent, incongruent, and neutral trials. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() Because we used group_by(Subject, Condition), summarise() will calculate the mean RT and mean Accuracy separately for each Subject and each Condition. Always be sure to ungroup() afterwards. View the data frame. In the console type View(data_flanker) Now rather than having one row per trial, group_by() and summarise() has aggregated the data down to Subject x Condition. What we want to do is calculate the difference between incongruent and congruent conditions on RT.mean and Accuracy.mean. However, congruent, incongruent, and neutral conditions are on separate rows. What we need to do is reshape the data so that there is a column for each Condition on RT.mean and Accuracy.mean. Our columns should be congruent_RT.mean incongruent_RT.mean neutral_RT.mean congruent_Accuracy.mean incongruent_Accuracy.mean neutral_Accuracy.mean Typically, to reshape a data frame we would use the gather() and spread() functions from the tidyr package. However, these do not allow reshaping on more than one value column. We have two value columns, RT.mean and Accuracy.mean. Luckily I have created a function that can allow us to do this, reshape_spread() from my datawrangling package. The main arguments you need to specify are: variables: The column name that contains the key variables to spread on values: The column name(s) that hold the values to be used id: Which columns should be preserved (i.e. Subject) So we can add something like this ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), ACC.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;ACC.mean&quot;)) Now View() the data frame. There should now be only ONE row per Subject. We can now just use mutate() to calculate the difference between these columns to get the FlankerEffect. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;)) %&gt;% mutate(FlankerEffect_RT = incongruent_RT.mean - congruent_RT.mean, FlankerEffect_ACC = incongruent_Accuracy.mean - congruent_Accuracy.mean) 11.6 Remove Subjects Next, we should implement the third data cleaning procedure listed above. acc.criteria: Finally remove subjects that performed less than 3.5 standard deviations below the mean accuracy on any Condition (congruent, incongruent, or neutral). It is convenient to do it now because we have a column with mean Accuracy for congruent and incongruent trials. We also want to do this before applying the binning procedure. The approach I like to take with entirely removing subjects is to keep a record of those subjects in a data file somewhere. To do this we will 1) create a new data frame of subjects that will be removed and then 2) use a function I created, remove_save() from the datawrangling package. This function is a short hand way of doing two things at once. Removing the subjects from the full data file Saving the removed subjects to a specified directory. The criteria we are removing subjects based on are those who performed 3.5 SDs below the mean. So we first need to calculate a column of z-scores (on SD units), then filter those who are below 3.5 z-scores. ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) Then use remove_save(). The main arguments to specify are: x: the data frame that contains ALL subjects remove: the data frame that contains subjects to be removed output.dir: directory to output file with removed subjects to output.file: name of file with removed subjects I put the removed subjects in a folder called “removed” and name the file something like “Flanker_removed.csv”. ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) data_flanker &lt;- remove_save(data_flanker, data_remove, output.dir = here(output.dir, &quot;removed&quot;), output.file = paste(task, &quot;removed.csv&quot;, sep = &quot;_&quot;)) If any subjects were removed you should now see a folder called removed in the Scored Data folder with a file called “Flanker_removed.csv”. 11.7 Calculate Binned Scores Great! We have now calculated FlankerEffects scores and performed the data cleaning procedures. Now we need to calculate Binned scores. The data_flanker data frame is no longer in a format that we can calculate bin scores. We need to use the trimmed data frame that has trial level data. We should remove the poor performing subjects and Missing RTs. This step is actually really important for the binning procedure because bin scores are relative to other subjects in the data. ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), !(Subject %in% data_remove$Subject)) This is stating, keep only Trials without missing values on RT AND Subjects that are NOT (!) in data_remove. We also need to remove neutral trials to calculate bin scores. Bin scores are based on comparing one condition to a baseline condition. In this case we want to compare the incongruent condition to the baseline congruent condition. So we need to get rid of neutral conditions. ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) And finally calculate bin scores using bin_score() from the englelab package. The main arguments to specify are: x: The data frame rt.col: Column name that contains the reaction time data. Default = “RT” accuracy.col: Column name that contains the accuracy data. Default = “Accuracy” condition.col: Column name that contains the trial condition type. Default = “Condition” baseline.condition: The values that specify the baseline condition type: How should Bin trials be aggregated, “sum” or “mean”. Default = “mean” id: Column name that contains subject identifiers. Default = “Subject” ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) %&gt;% bin_score(baseline.condition = &quot;congruent&quot;, type = &quot;mean&quot;,) %&gt;% rename(FlankerBin = &quot;BinScore&quot;) Awesome! Now we have two data frames, one, data_flanker, with FlankerEffect scores and another, data_binned with FlankerBin scores. They both have one row per subject. 11.8 Merge Now we can merge these two data frames together using the merge() function from base R. ## Merge data_flanker &lt;- merge(data_flanker, data_binned, by = &quot;Subject&quot;, all = TRUE) Now view the data_flanker. It should be one row per subject and have columns for FlanekrEffect_RT, FlankerEffect_ACC, and FlankerBin. 11.9 Save data file And finally save the data file ## Output #### write_csv(data_flanker, here(output.dir, output.file)) ############## If we put it all together your R script should look something like: ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) library(englelab) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set Import/Output Filenames task &lt;- &quot;Flanker&quot; import.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output.file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ## Set Data Cleaning Params rt.min &lt;- 200 rt.trim &lt;- 3.5 acc.criteria &lt;- -3.5 ############# ## Import #### import &lt;- read_csv(here(import.dir, import.file)) ############## ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = rt.trim, replace = &quot;cutoff&quot;) %&gt;% ungroup() ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;), id = &quot;Subject&quot;) %&gt;% mutate(FlankerEffect_RT = incongruent_RT.mean - congruent_RT.mean, FlankerEffect_ACC = incongruent_Accuracy.mean - congruent_Accuracy.mean) ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) data_flanker &lt;- remove_save(data_flanker, data_remove, output.dir = here(output.dir, &quot;removed&quot;), output.file = paste(task, &quot;removed.csv&quot;, sep = &quot;_&quot;)) ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) %&gt;% bin_score(baseline.condition = &quot;congruent&quot;, type = &quot;mean&quot;,) %&gt;% rename(FlankerBin = &quot;BinScore&quot;) ## Merge data_flanker &lt;- merge(data_flanker, data_binned, by = &quot;Subject&quot;, all = TRUE) ################################# ## Output #### write_csv(data_flanker, here(output.dir, output.file)) ############## rm(list=ls()) 11.10 Masterscript Now you can add lines of code in the manuscript to execute or source() the script “2_flanker_score.R”. Something "],
["merge-create-a-final-data-file.html", "Chapter 12 Merge: Create A Final Data File 12.1 Setup 12.2 Merge 12.3 Select and Trim 12.4 Output 12.5 Masterscript", " Chapter 12 Merge: Create A Final Data File In our lab we run large-scale studies in which we might have more than 30 tasks to score. This means you will have lots of scripts for creating tidy raw data and scored data files. In the end, you don’t just want a bunch of separate scored data files. You want a single data file with all the variables and subjects together. Save a new R script file as 3_merge.R in the R Scripts folder In this Chapter you will learn how to write a script for merging multiple scored data files and doing some more data cleaning by removing outlier scores. There are four blocks of code in this script Setup Merge Select and Trim Output 12.1 Setup The Setup block is very similar to what you did in the previous Chapters ## Setup #### ## Load packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$scored output.dir &lt;- directories$data ############# 12.2 Merge In this block we can import/merge all the Scored data files. You only wrote a script to score the Flanker task, but at the beginning of this section you downloaded other Scored data files. Normally you would have written tidy raw and Scored scripts for these tasks but we can skip that to speed things along. Look at the files in Data Files/Scored Data. You can view your files in RStudio by navigating in the Files window pane. We will import/merge all these files in one step using files_join() from my datawrangling package. The main argunents for this function are: path: Folder location of files to be merged pattern: String pattern to uniquely identify files to be merged id: Subject ID variable name. ## Merge #### import &lt;- files_join(here(import.dir), pattern = &quot;Scores&quot;, id = &quot;Subject&quot;) ############# View import. Notice that it contains A LOT of columns, most of which we are not really intersted in for Data Analysis. 12.3 Select and Trim The next step is really straight forward. The Scored data files will each contain way more columns than we are actually interested in. Therefore, we should only select those columns that contain the variables we want to analyze in the Data Analysis stage. We can also remove Scores that are univariate outliers, using trim() from datawrangling. ## Select and Trim #### data &lt;- import %&gt;% select(Subject, OSpan = OSpan.Partial, SymSpan = SymSpan.Partial, RotSpan = RotSpan.Partial, FlankerEffect = FlankerEffect_RT, StroopEffect = StroopEffect_RT) %&gt;% trim(variables = &quot;all&quot;, cutoff = 3.5, replace = &quot;NA&quot;, id = &quot;Subject&quot;) ######################## Notice how I am renaming some of the variable names to be more concise. Now view data. This is a much more manageable data file compared to import. 12.4 Output Besides outputing the merged data file, I like to also output a file that simply has a list of all Subjects that made it through all the data cleaning procedures. This will be a list of subjects that go into Data Analysis. ## Output ## subj.list &lt;- select(data, Subject) write_csv(data, here(output.dir, &quot;Data.csv&quot;)) write_csv(subj.list, here(output.dir, &quot;subjlist_final.csv&quot;)) ############ Putting it all together: ## Setup #### ## Load packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$scored output.dir &lt;- directories$data ############# ## Merge #### import &lt;- files_join(here(import.dir), pattern = &quot;Scores&quot;, id = &quot;Subject&quot;) ############# ## Select and Clean #### data &lt;- import %&gt;% select(Subject, RAPM, NumberSeries, LetterSets, OSpan = OSpan.Partial, SymSpan = SymSpan.Partial, RotSpan = RotSpan.Partial, Antisaccade = Antisaccade_ACC.mean, FlankerEffect = FlankerEffect_RT, FlankerBin, StroopEffect = StroopEffect_RT, StroopBin) %&gt;% trim(variables = &quot;all&quot;, cutoff = 3.5, replace = &quot;NA&quot;, id = &quot;Subject&quot;) ######################## ## Output ## subj.list &lt;- select(data, Subject) write_csv(data, here(output.dir, &quot;Data.csv&quot;)) write_csv(subj.list, here(output.dir, &quot;subjlist_final.csv&quot;)) ############ rm(list = ls()) 12.5 Masterscript Now put a source() line in the masterscript to source this merge script. Wow! Now you have completed your Data Preparation scripts to go from messy raw data to tidy raw data to scored data and finally a single merged data file that is all ready for Data Analysis! And you can control all this by running the code in the masterscript, or just sourcing the entire masterscript. And the great thing is that this can serve as a general template for you. Truly, most of the code you have written here can just be directly copy and pasted from one task or even one study to the next. Not much changes. What is happening in between import and output will change but the rest will stay more or less the same. Copying and Pasting from a template allows you to focus more getting your data ready for analysis and less on writing code. Something "],
["reproducibility.html", "Chapter 13 Reproducibility 13.1 What is real? 13.2 Where does your analysis live? 13.3 Environment 13.4 Folder Organization 13.5 Naming R Scripts", " Chapter 13 Reproducibility As you start becoming more proficient in R you will be able manage, process, and analyze your data at all stages of analysis through R scripts alone. Goodbye Excel, goodbye SPSS, and good ridance to EQS! Although we deal with data all the time as scientists, we have never really been educated on what good data science practices look like. In psychology, I believe this is partly due to our reliance on programs like SPSS or Excel for data analysis and visualization. While these programs offer a nice user interface, they do not offer any tools to help us manage and process our data at all stages of analysis. Open Science and Reproducibility practices are very quickly becoming the norm in all fields of science. Programming languages like R are perfectly suited to help us implement these practices. Reliance on programs like SPSS and Excel will hinder your ability to join the Open Science and Reproducibility movement. However, simply learning the R syntax alone is not enough. You will need to start thinking more about what are good data science practices that allow me to manage, process, and analyze my data in a way that is consistent with Open Science and Reproducibility? In the previous section I already had you implementing these good data science practices. In the next few Chapters I will explain these in more detail. Reproducibility is not just about allowing others to reproduce your analyses. More importantly it allows you to reproduce your own analyses. Or perhaps go back and modify some processing step and re-run the modified analysis. Therefore, learning R is about learning how to manage and handle your data processing workflow in a way that empowers your ability to analyze and explore your data. If you treat R as just an alternative to SPSS, then it is all too easy to create poorly written scripts and disorganized projects that completely undermine reproducibility. Honestly, I am not sure if it is worth taking the time and effort to learn R as simply an alternative to SPSS. Parts of this next section are taken directly from the excellent book on Data Science in R R for Data Science One day you will need to quit R, go do something else and return to your analysis the next day. One day you will be working on multiple analyses simultaneously that all use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 13.1 What is real? As a beginner R user, it is tempting to consider whatever is in our Environment (i.e. data we have imported) as “real”. However, we should consider our R Scripts and Data Files saved on our computer as real. With your R scripts (and your data files), you can recreate the Environment It’s much harder to recreate R scripts from your Environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your R history. Therefore, the Environment is more of a temporary workspace. This workspace will get cleared out every time you Restart or Quit out of R and RStudio. If you treat your Environment as real this can have disasterous consequences and you can lose a lot of productivity and undermine Reproducibility. Your R Scripts and Data Files are real Even if you need to Quit R, come back to an analysis the next day, or want to run your analysis on a different computer than you started you should be able to reproduce your analyses. You should be able to Reproduce all your analyses from your saved R scripts and original data files. 13.2 Where does your analysis live? “One day you will be working on multiple analyses simultaneously that all use R and you want to keep them separate” There are two levels at which your analysis will live Project level Individual R Script level 13.2.1 Project Level The Project Level refers to where you are storing all your files associated with the project (i.e. R scripts, data files, figures, results) as well as to the organization of your folders and files. If you are working on multiple projects at one time, then it is vital to: Keep your analysis from different projects separated You do not want objects created in your Environment from one project to get mixed up with objects (perhaps with the same object names) in a different project. We have not talked about the concept of Working Diectories yet, but you also need to ensure your working directory is correctly set in order to import and output files. If you are working on multiple projects at one time, not keeping them separated will create issues Reproducing your analysis because the working directory might not be set correctly. The three most important elements to an environment are: The working directory Loaded packages Objects The best way to keep project Environments separated is to work on them in separate R Sessions. You can have multiple Sessions of R open at one time. The three elements of an Environment in one R Session will be different and independent from those in another R Session. RStudio has an excellent way of managing separate projects with a feature called RStudio Projects. Use RStudio Projects to create separate Environments for your projects See below for more details on RStudio Projects. 13.2.2 Script Level Within a Project, you will have multiple R Scripts that are performing a different analysis. It is also important to keep the Environment of an R Script independent of the Environment from other R Scripts, even within the same project. Obviously your R Scripts will be dependent on one another in the sense that one R Script might be creating data files that other R Scripts will later use. For example, you might have an R Script (or multiple scripts) that prepare the data for statistical analysis by creating a scored merged data file. Then another R Script will actually run and output the statistical analyses. The statistical analysis R Script is dependent on a data file created by the first R Script. However, the Environment of the R Scripts are independent from one another. R Scripts are linked in a data processing workflow through the data files they create not by the objects in the Environment Therefore, it is important that the Environment of each R Script within a Project are independent from one another. This means that an R Script should not depend on objects created or data imported in other R Scripts. Any packages required for an R Script should be loaded in THAT script and not depend on them being loaded in other R Scripts. As long as the data files required for an R Script are already created, you should be able to open a completely new and fresh session of R and succesfully execute all the lines of code in that one Script (without having ran any other scripts). This is why in the scripts you created for the previous Section they all had a similar structure: Setup Import Do Stuff Output In the Setup section you are making sure that all the packages for that ONE script are loaded, any import or output directory objects are set correctly, and any parameters for data cleaning are set. Every R script in the Data Preparation stage Imports a file AND Outputs a file. This makes sure the scripts are linked in the data processing workflow by the files they create (output) and not by the Objects in the Environment they create. Then at the end of each R Script I had you include the line of code rm(list = ls()) This removes all Objects in your Environment. That way when you run the next R Script it is starting from a blank Environment Some people in the R community say this is not a good practice, but in reality it is not harmful to add this line of code. It is not perfect because it does not unload any packages that have been loaded. Given that I had you implementing these practices in the previous Section it might not seem like that big of a deal to you. Good! But sometimes when I look at other researcher’s R Scripts sometimes I am dismayed at how poorly their projects and scripts are organized. You can start by implementing these good data science practices from the beginning without thinking too much of it. 13.3 Environment Because we should treat our Environment as a temporary workspace we need to make sure that out Environments are as independent as possible from one another, at both the Project and Scripts Levels. I mentioned above that the three most important elements to an environment are: The working directory Loaded packages Objects 13.3.1 Working Directory You could use what are refered to as absolute file paths to import and export files but this is not good practice. The reason is that the absolute file path is specific to a particular computer. No one computer is going to have the same absolute file path. An absolute file path starts from the root directory on your computer and may look something like: Mac: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial Windows: C:\\Users\\jasontsukahara\\Dropbox (GaTech)\\My Work\\Coding Projects\\R\\R-Tutorial You do not want to write scripts that can only work on a specific computer! One of the great advantages to programming for data processing is reproducibility. You and your future self (and other researchers) can reproduce your exact same data processing steps. If you use absolute file paths you are undermining the reproducibility of your scripts. It is good practice to use relative file paths instead. Relative file paths start from a working directory. Let’s say you have a working directory set to the following location: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study And you want to import files from a Raw Data directory within Cool Study. The absolute path to raw data files in Cool Study might look like: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study/Data Files/Raw Data Whereas a elative file path from the working directory would be: Data Files/Raw Data You can see that with relative file paths, only the internal organization of the project directory matters. This allows your script to be ran on different computeres, systems, and environments! You working directory can be specified in various ways. By default, if you open up RStudio directly it will set the working directory to some default location such as your User root directory. This is not good. If you open up RStudio by directly opening an R script file then it will set the working directory to the location of that file. This is better but still not ideal. Because the working directory changes depending on how you open RStudio a lot of R Users will set a working directory at the top of their script using setwd() However, this is not good because it requires the use of an absolute filepath. Ideally, we should not even have to think about the working directory since it can change depending on how R is opened. Thankfully there is a solution to make our dreams come true! 13.3.1.1 RStudio Projects and here::here() Visit this page for more details on R Projects. Using RStudio Projects, helps keep your scripts and Environments from one project to the next separate from each other. RStudio Projects allow you to open a fresh session of R that automatically sets the working directory to the location where the R Project is saved. R Projects have the file extension .Rproj. Save .Rproj to your project’s root directory There are a couple of ways you can open an RStudio Project. One way is to just simply open the .Rproj file. This will open a new R Session (and RStudio window). If you already have an RStudio window open you can navigate to the very top-right of the application window and browse different projects you have recently worked on. This is where you can also see which Project you currently have open. The here package in combination with RStudio Projects allows you to not even have to think about working directories. For a passionate ode to the here package see: https://github.com/jennybc/here_here First go ahead and install the here package, install.packages(&quot;here&quot;). Basically, when the here package is loaded, library(here), it will search for a .Rproj file it will set a starting file path for the function here(). library(here) You can then use here() to set a relative file path here(&quot;Data Files/Raw Data/flanker_raw.csv&quot;) This is equivalent to here(&quot;Data Files&quot;, &quot;Raw Data&quot;, &quot;flanker_raw.csv&quot;) I typically like to set the first argument as the relative file path and the second argument as the filename here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;) This visually separates the file path and the filename, making your script easier to read. Now in import and output functions you can use here() read_csv(here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;)) write_csv(data, here(&quot;Data Files/Scored Data&quot;, &quot;flanker_scored.csv&quot;)) And you know that everytime you use here() that the file path will start at where you have your .Rproj file saved (in the root directory of your Project). Avoid using setwd() by using RStudio Projects and here::here() 13.3.2 Loaded Packages To make sure that the packages used in one R Script do not depend on the packages loaded in other R Scripts: Load all required packages for that one R Script at the top of the script This allows you to easily evaluate which packages are required for that script. 13.3.3 Objects To make sure that the objects and functions created in one R Script do not depend on the objects and functions created in other R Scripts: Include the following line of code at the bottome of the script rm(list=ls()) This will remove any objects and functions in the current environment. That way when the next R Script is ran, there will be no leftover objects or functions from previous R Scripts. 13.4 Folder Organization I suggest adopting a consistent folder organization for all your research projects. Again, this is just about allowing you to focus on the meat of your R scripts. This is my organization: Working Directory Data Files Raw Data E-Merge Scored Data R Scripts Results In the Data Files/Raw Data/E-Merge folder are the “messy” raw data files In the Data Files/Raw Data folder are the “tidy” raw data files In the Data Files/Scored Data folder are the scored data files In the Results folder are the outpued results In the R Scripts folder are ALL the R Scripts You can see how this organization corresponds to the data processing workflow I introduced earlier 13.5 Naming R Scripts If you have a lot of R Scripts for a project it can make it easier to use a certain naming convention to organize the scripts. First of all, I definitely reccomend putting all your scripts into one folder. There is nothing more annoying then having to search all of your computer for the script you are looking for. I personally like to name my R scripts with a number prefix folowed by an underscore and end it with the name of the data processing step it belongs to (i.e. 1_taskname_raw.R). The numbered prefix denotes what step in the data processing procedure given the organization of the masterscript. This makes it SO MUCH easier to search for the script you need to work on. For instance, my R Script directory might look like Where all the scripts that create “tidy” raw data files have the prefix 1 and the suffix _raw whereas the scripts for scoring data files has the prefix 2 and the suffix _score. The prefix number will order the scripts by their data processing workflow step. Something "],
["r-script-templates.html", "Chapter 14 R Script Templates 14.1 Building a Template Workflow 14.2 EngleLab GitHub 14.3 Masterscript Template 14.4 Messy to Tidy Template 14.5 Tidy to Scored Template", " Chapter 14 R Script Templates 14.1 Building a Template Workflow Steps 1 and 2 are all about transforming data files to create a final output file that is ready for statistical analysis. Every R script you create for steps 1 and 2 will - import a file -&gt; do stuff to the dataframe -&gt; output a saved file, no more, and no less. The general workflow in every script will look like Setup the script by loading required packages using library() Import data file using read_delim() or read_csv() from the readr package Do stuff to the imported dataframe using dplyr functions, such as filter(), select(), group_by(), mutate(), and summarise() Output transformed data to a file using write_csv() from readr Honestly there is not much more to it then that. And because your R scripts for steps 1 and 2 have the same workflow process this makes it very easy to implement a standard organization in your scripts. 14.2 EngleLab GitHub I am hosting a lot of template scripts on an EngleLab organization account I created on GitHub. There are general scripts, like the ones you will see here. And there are scripts for more specific tasks we commonly use; such as fluid intelligence, working memory capacity, and attention control tasks. EngleLab GitHub R-Templates 14.3 Masterscript Template ## Setup #### ## Load Packages library(here) library(rmarkdown) ## Specify the directory tree directories &lt;- list(scripts = &quot;R Scripts&quot;, data = &quot;Data Files&quot;, raw = &quot;Data Files/Raw Data&quot;, messy = &quot;Data Files/Raw Data/E-Merge&quot;, scored = &quot;Data Files/Scored Data&quot;, results = &quot;Results&quot;, removed = &quot;Data Files/Scored Data/removed&quot;) saveRDS(directories, here(&quot;directories.rds&quot;)) ############# ############################################# #------ 1. &quot;messy&quot; to &quot;tidy&quot; raw data ------# ############################################# source(here(&quot;R Scripts&quot;, &quot;1_task_raw.R&quot;), echo=TRUE) ################################################# #------ 2. &quot;tidy&quot; raw data to Scored data ------# ################################################# source(here(&quot;R Scripts&quot;, &quot;2_task_score.R&quot;), echo=TRUE) ############################################################# #------ 3. Create Final Merged Data File for Analysis ------# ############################################################# source(here(&quot;R Scripts&quot;, &quot;3_merge.R&quot;), echo=TRUE) ############################### #------ 4. Data Analysis ------# ############################### render(here(&quot;R Scripts&quot;, &quot;4_MainAnalyses.Rmd&quot;), output_dir = here(&quot;Results&quot;), output_file = &quot;MainAnalyses.html&quot;, params = list(data = here(&quot;Data Files&quot;, &quot;Name_of_datafile.csv&quot;))) rm(list=ls()) 14.3.1 directories This is my unique setup that I like to use. By specifying a directory list in the masterscript and saving it as an R Object file with the extension .rds, you can import this direcotry list and use it in all your scripts. To save an R Object to a file saveRDS(directories, here(&quot;directories.rds&quot;)) here(&quot;directories.rds&quot;) will save the directories object as “directories.rds” to the Projects Root Directory. This means you only need to worry about the ACTUAL file path names in ONE SINGLE place, the masterscipt. Then when setting the import and output file paths in your scripts you can simply reference the elements in directories using the $ notation. For instance: directories$messy directories$raw directories$scored 14.3.2 source() The source() function is a way to execute all the lines of code in a script file. Rather than having to manually open each script file and sourcing it from there you can control your entire data processing workflow from the masterscript using source(). echo = TRUE will print the results of the script to the console that way you can still see what the script is doing. 14.3.3 render() The render() function is how to knit an RMarkdown document (which we will use for the Data Analysis stage). We will see later that this creates a really flexible way to knit RMarkdown documents because you can specify the output filename and location, as well as certain parameters for what data set to import or analysis parameters to set. render() comes from the package rmarkdown which is why it is loaded at the top of the masterscript. 14.4 Messy to Tidy Template Here is a template script for the first stage of data processing, converting messy raw data files to tidy raw data files ## Set up #### ## Load packages library(readr) library(dplyr) library(here) library(datawrangling) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$messy output.dir &lt;- directories$raw ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import.file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ############## ## Import #### import &lt;- read_delim(here(import.dir, import.file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) %&gt;% duplicates_remove(taskname = task, output.folder = here(output.dir, &quot;duplicates&quot;)) ############## ## Tidy raw data #### data_raw &lt;- import %&gt;% filter() %&gt;% rename() %&gt;% mutate() %&gt;% select() ##################### ## Output #### write_csv(data_raw, here(output.dir, output.file)) ############## rm(list=ls()) To me this template is beautiful. Literally the only thing you need to change is task &lt;- &quot;taskname&quot; to the name of the task used in the filename Fill in what happens in the Tidy raw data block. The rest can LITERALLY stay the same. Just copy and paste. How easy! 14.5 Tidy to Scored Template Here is a template script for the first stage of data processing, transforming tidy raw data files to scored data files Notice that to import the “directories.rds” file I use readRDS(here(&quot;directories.rds&quot;)) and then set the import and output file directories using directories$mess and directories$raw. ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import.file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output.file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ## Set Data Cleaning Params ############# ## Import #### import &lt;- read_csv(here(import.dir, import.file)) ############## ## Data Cleaning and Scoring #### data &lt;- import %&gt;% filter() %&gt;% group_by() %&gt;% summarise() ################################# ## Output #### write_csv(data, here(output.dir, output.file)) ############## rm(list=ls()) Again, beautiful! Here there are four things you may need to change: What packages are loaded at the top, library() task &lt;- &quot;taskname&quot; to the name of the task used in the filename Set Data Cleaning Params. This is optional if your task requires certain data cleaning parameters to be set. In Section III: Example Data Preparation we saw an example of this. What happens in the Data Cleaning and Scoring block. This might require more extensive coding than what is in the template, or it may not. In Section III: Example Data Preparation we saw an example of a more complicated script to score the Flanker task. But those are the only four parts of the script that would need to be changed. Notice that to import the “directories.rds” file I use readRDS(here(&quot;directories.rds&quot;)) and then set the import and output file directories using directories$raw and directories$scored. rm(list=ls()) Something "],
["workflow-package.html", "Chapter 15 workflow Package 15.1 Start a new research study 15.2 Download ready-made templates", " Chapter 15 workflow Package I have made it WAY TOO EASY for you to Set up an organized directory and R Project for a new Research Study Download ready-made R-Script templates To be able to do this, you will need to download my workflow package. devtools::install_github(&quot;dr-JT/workflow&quot;) 15.1 Start a new research study Using the workflow package you can easily steup a new research study (BEFORE DATA COLLECTION) with an organized directory and ready-made R Script files. What this package will do is create a root study direcotry with an .Rproj file and the following default folders: /R Scripts : For R script files /Data Files : For Data files /Data Files/Raw Data : For “tidy” raw data files /Data Files/Raw Data/E-Merge : For “messy” raw data files /Data Files/Scored Data : For scored data files /Results : For outputed .html results files /Tasks : For task files (E-Run and E-studio) that will be used to run participants /Documents : For any related document files (e.g. Methods documents) Navigate to __File -&gt; New Project… -&gt; New Directory And browse until you see the option: Create a Research Study Click on that and you will some options you can specify: 15.1.1 Directory Directory Name: This will be the name of the folder for the study Create project as subdirecotry of: Select Browse and choose where the folder (Directory Name) should be located. 15.1.2 Additional Options Type of Study: You can choose either standard or sem. The sem option will download some extra ready-made analysis template scripts (.Rmd) that are useful for conducting SEM in R. # of Sessions: How many sessions will the study have? This will create folders in the Tasks directory for each session. For instance, if there will be 4 sessions it will create the the folders “Session 1”, “Session 2”, “Session 3”, and “Session 4”. Additional Directories: Do you want to add the additional directories? /Results/Figures : For any image files related to analyses /Manuscript : For drafts and final versions of the manuscript related to the study /Presentations : For any presentations you give related to the study Download Scripts: Do you want to download the following scripts? Generic Scripts: Generic masterscript.R, _raw.R, and _score.R script templates Gf Scripts: gf_raw.R and gf_score.R scripts WMC Scripts: wmc_raw.R and wmc_score.R scripts Attention Control Scripts: _raw.R and _score.R scripts for the antisaccade, flankerDL, stroopDL, va4, and sact tasks. The Gf, WMC, and Attention Control scripts should be good-to-go. You likely will not even need to modify these. The Generic scripts are good if you have additional tasks you will want to create _raw.R and _score.R data files for. 15.2 Download ready-made templates If you have already started data collection or have created your study directory, but want to download some R script templates you can use the workflow::template() function. 15.2.1 Generic Templates To download ALL generic templates workflow::template(all = TRUE) To download specific generic templates specify TRUE for whichever templates you want to download. workflow::template(masterscript = TRUE, rawscript = TRUE, scorescript = TRUE, mergescript = TRUE, demographics = TRUE, sem = TRUE) 15.2.2 Gf, WMC, and Attention Control Scripts To download ready-to-go Gf, WMC, and Attention control scripts, just specify TRUE for whichever script you want to dowload. workflow::template(gf = TRUE, wmc = TRUE, ac = TRUE) Something "],
["introduction-to-data-analysis.html", "Chapter 16 Introduction to Data Analysis 16.1 R Markdown", " Chapter 16 Introduction to Data Analysis WARNING: THIS CHAPTER NEEDS TO BE UPDATED! 16.1 R Markdown Phew! You’ve made it this far, good job. Up until now you have been learning how to do data preparation steps in R. Now for the fun part, statistical analyses and data visualization! This is the third and final step in the data workflow process depicted above. Traditionally you have likely done these analyses in SPSS or EQS and have created figures in Excel or PowerPoint. The rest of the guide will cover how to do these steps in R. Writing scripts to do statistical analyses is an entirely different process than writing scripts for data preparation. Therefore, we should first go over the general process of conducting and outputing statistical analyses in R. In programs like SPSS when you run a statistical analysis, it will be outputed to a viewable .spv document. One dowfall of this is that .spv files are proprietry format so can only be opened if you have SPSS installed. However, there is the option to export a .spv file as a PDF. One downfall about R is that unlike SPSS, there is not a native way to create output documents from statistical analyses. Fortunately, RStudio has an output document format called R Markdown. 16.1.1 What is an R Markdown File? R Markdown is a powerful way to create reports of statistical analyses. Reports can be outputed in a lot of different formats; html, Microsoft Word, PDF, presentation slides, and more. In fact, this guide was created using R Markdown. The easiest format to output as is html. html documents are opened in a web browser and therefore can be opened on any computer and device (phones, tablets, Windows, Mac). For a brief intro to R Markdown see https://rmarkdown.rstudio.com/lesson-1.html First, you need to install the rmarkdown package install.packages(&quot;rmarkdown&quot;) To open an R Markdown document go to File -&gt; New File -&gt; R Markdown… Select HTML and click OK An example R Markdown document will open. Go ahead and read the contents of the document. There are three types of content in an R Markdown document: A YAML header R code chunks Formatted text 16.1.2 YAML header The YAML header contains metadata about how the document should be rendered and the output format. It is located at the very top of the document and is surrounded by lines of three dashe, --- title: &quot;Title of document&quot; output: html_document --- There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html 16.1.3 R code chunks Unlike a typical R script file (.R), an R Markdown document (.Rmd) is a mixture of formated text and R code chunks. Not everything in an R Markdown document is executed in the R console, only the R code chunks. To run chunks of R code you can click on the green “play” button on the top right of the R code chunk. Go ahead and do this for the three R code chunks in the R Markdown document you opened. (cars and pressure are just example dataframes that come pre-loaded with R). We have not gone over these functions yet, but you can see that the results of the R code are now displayed in the document. The first R code chunk is just setting some default options of how the output of R code chunks should be displayed. We will cover these options in more detail later. 16.1.4 Formatted text The formatted text sections are more than just adding comments to lines of code. You can write up descriptive reports, create bulleted or numbered lists, embed images or web links, create tables, and more. The text is formatted using a language known as Markdown, hence the name R Markdown. Markdown is a convenient and flexible way to format text. When a Markdown document is rendered into some output (such as html or PDF), the text will be formatted as specified by the Markdown syntax. In the R Markdown document you have open you can see some Markdown syntax. The pound signs ## at the beggining of a line are used to format headers. One # is a level one header, two ## is a level two header and so on. Also notice in the second paragraph, the word Knit is surrounded by two asterisks on each side. When this document is redered, thw word Knit will be bolded. Go ahead and render the R Markdown document by clicking on the Knit button at the top of the window. Once it is done rendering you will see a new window pop up. This is the outputed html file. You can see how the document has formated text based on the Markdown syntax. There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. One guide that I frequently reference is https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Something "],
["fundamentals-of-data-visualization.html", "Chapter 17 Fundamentals of Data Visualization 17.1 Grammar of Graphics 17.2 Plotting Functions in R", " Chapter 17 Fundamentals of Data Visualization Data visualization is an essential skill for anyone working with data. It is a combination of statistical understanding and design principles. In this way, data visualization is about graphical data analysis and communication and perception. Data visualization is often times glossed over in our stats courses. This is unfortunate because it is so important for better understanding our data, for communicating our results to others, and frankly it is too easy to create poorly designed visualizations. As a scientist, there are two purposes for visualizing our data. Data exploration: it is difficult to fully understand our data just by looking at numbers on a screen arranged in rows and columns. Being skilled in data visualization will help you better understand your data. Explain and Communicate: You will also need to explain and communicate your results to collegues or in scientific publications. The same data visualization principles apply to both purposes, however for communicating your results you may want to place more emphasis on aesthetics and readability. For data exploration your visualizations do not have to be pretty. 17.1 Grammar of Graphics Leland Wilkinson (Grammar of Graphics, 1999) formulized two main principles in his plotting framework: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings The essential grammatical elements to create any visualization are: 17.2 Plotting Functions in R It is possible to create plots in R using the base R function plot(). The neat thing about plot() is that it is really good at knowing what kind of plot you want without you having to specify. However, these are not easy to customize and the output is a static image not an R object that can be modified. To allow for data visualization that is more in line with the principles for a grammar of graphics, Hadley Wickham (pictured below) created the ggplot2 package. This by far the most popular package for data visualization in R. Let’s learn you some ggplot2! "],
["introduction-to-ggplot2.html", "Chapter 18 Introduction to ggplot2 18.1 Grammar of Graphics 18.2 Data layer 18.3 Aesthetic Layer 18.4 Geometries Layer 18.5 Facets Layer 18.6 Statistics Layer 18.7 Coordinates Layer 18.8 Themes Layer", " Chapter 18 Introduction to ggplot2 18.1 Grammar of Graphics We saw from the last chapter that the two main components in a grammar of graphics are: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings We also saw that the three essential elements are the data layer, aesthetics layer, and geometrics layer. In ggplot2 there are a total of 7 layers we can add to a plot 18.2 Data layer The Data Layer specifies the data being plotted. Let’s see what this means more concretely with an example data set. A very popular data set used for teaching data science is the iris data set. In this data set various species of iris were measured on their sepal and petal length and width. This data set actually comes preloaded with R, so you can simply view it by typing in your console View(iris) head(iris) We can see that this data is in wide format. What type of graph we can visualize will depend on the format of the data set. On occassion, in order to visualize a certain pattern of the data will require you to change the formating of the data. Let’s go ahead and start building our graphical elements in ggplot2. Load the ggplot2 library. Then: library(ggplot2) ggplot(data = iris) You can see that we only have a blank square. This is becuase we have not added any other layers yet, we have only specified the data layer. 18.3 Aesthetic Layer The next grammatical element is the aesthetic layer, or aes for short. This layer specifies how we want to map our data onto the scales of the plot The aesthetic layer maps variables in our data onto scales in our graphical visualization, such as the x and y coordinates. In ggplot2 the aesthetic layer is specified using the aes() function. Let’s create a plot of the relationship between Sepal.Length and Sepal.Width, putting them on the x and y axis respectively. ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) You can see we went from a blank box to a graph with the variable and scales of Sepal.Length mapped onto the x-axis and Sepal.Width on the y-axis. However, there is no data yet :( What are we to do? 18.4 Geometries Layer The next essential element for data visualization is the geometries layer or geom layer for short. Just to demonstrate to you that ggplot2 is creating R graphic objects that you can modify and not just static images, let’s assign the previous graph with data and aesthetics layers only onto an R object called p, for plot. p &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) Now let’s say we want to add the individual raw data points to create a scatterplot. To do this we can use the function geom_point(). This is a geom layer and the type of geom we want to add are points. In ggplot2 there is a special notation that is similar to the pipe operator %&gt;% seen before. Except it is plus sign + p + geom_point() And walla! Now we have a scatterplot of the relationship between Sepal.Length and Sepal.Width. Cool. If we look at the scatterplot it appears that there are at least two groups or clusters of points. These clusters might represent the different species of flowers, represented in the Species column. There are different ways we can visualize or separate this grouping structure. First, we will consider how to plot these species in separate plots within the same visualization. 18.5 Facets Layer The facet layer allows you to create subplots within the same graphic object The previous three layers are the essential layers. The facet layer is not essential, however given your data you may find it helps you to explore or communicate your data. Let’s create facets of our scatterplot by Species ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) 18.6 Statistics Layer The statistics layer allows you plot statistical values calculated from the data So far we have only plotted the raw data values. However, we may be interested in plotting some statistics or calculated values, such as a regression line, means, standard error bars, etc. Let’s add a regression line to the scatterplot. First without the facet layer then with the facet layer ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) 18.7 Coordinates Layer The coordinate layer allows you to adjust the x and y coordinates You can adjust the min and max values, as well as the major ticks. This is more useful when you have separate graphs (non-faceted) and you want to plot them on the same scale for comparison. This is actually a very important deisgn principle in data visualization. If you want to compare two separate graphs, then they need to be on the same scale!!! library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = seq(4, 8, by = 1), ylim = seq(2, 5, 1)) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = seq(4, 8, by = 1), ylim = seq(2, 5, 1)) ggplot(filter(iris, Species == &quot;virginica&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = seq(4, 8, by = 1), ylim = seq(2, 5, 1)) Just a note. I highly suggest not using scale_x_continuous() or scale_y_continuous() functions. The coord_cartesian() function is like zooming in and out of the plot area. The scale_ functions actually change the shape of the data and statistics layers. If a data point falls outside of the scale limits then it will be removed from any statistical analyses (even if the individual data points are not plotted geom_point()) 18.8 Themes Layer The Themes Layer refers to all non-data ink. You can change the labels of x or y axis, add a plot title, modify a legend title, add text anywhere on the plot, change the background color, axis lines, plot lines, etc. There are three types of elements within the Themes Layer; text, line, and rectangle. Together these three elements can control all the non-data ink in the graph. Underneath these three elements are sub-elements and this can be represented in a hierarchy such as: For instance, you can see that you can control the design of the text for the plot title and legend title theme(title = element_text()) or individually with theme(plot.title = element_text(), legend.title = element_text()). Any text element can be modified with element_text() Any line element can be modified with element_line() Any rect element can be modified with element_rect() You can then control different features such as the color, linetype, size, font family, etc. As an example let’s change some theme elements to our facet plot. Let’s change the axis value labels to red font and increase the size ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text = element_text(color = &quot;red&quot;, size = 14)) Now let’s only change the x-axis text and not the y-axis text. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) It is a good idea to have a consistent theme across all your graphs. And so you might want to just create a theme object that you can add to all your graphs. a_theme &lt;- theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14), panel.grid = element_blank(), panel.background = element_rect(fill = &quot;pink&quot;)) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + a_theme 18.8.1 Built-in Themes For the most part you can probably avoid the theme() function by using built-in themes, unless there is a specific element you want to modify. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + theme_linedraw() You can also set a defeault theme for the rest of your ggplots at the top of your script. That way you do not have to keep on specifying the theme for evey ggplot. theme_set(theme_linedraw()) Now you can create a ggplot with theme_linedraw() without specifying theme_linedraw() every single time. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) You can do a google search to easily find different types of theme templates. I personally like theme_linedraw() Something "],
["scatterplots.html", "Chapter 19 Scatterplots 19.1 Scatterplots", " Chapter 19 Scatterplots You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) The main type of plots we typically want to create in psychological science are: Scatterplots Bar graphs Line graphs Histograms 19.1 Scatterplots We have already spent a good amount of time creating scatterplots using stat_smooth() and/or geom_smooth(). These two functions are essntially identical. In fact, many of the geom_ functions are just wrappers around stat_ functions. The scatterplot we created from last chapter is essentially an interaction plot. The interaction of Species x Sepal.Length on Sepal.Width. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) For modelling an interaction effect in regression it is easier to interpret if the lines extend to all possible values - not just across the values within a group. We can do this by specifying the argument geom_smooth(fullrange = TRUE) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) Now what if the moderator was a continuous variable and not categorical like Species? We would want to set the color aesthetic to be on +/- 1 SD on the mean. How would we go about doing this? The answer is: It would be very difficult to do so. This is where the function plot_model() from the sjPlot package comes in handy. 19.1.1 Adding other geoms There might be other geoms we want to add to a scatterplot. Let’s add some summary statistics to the graph. Specifically, a horizontal dashed line representing the mean on Sepal.Width and a vertical dashed line representing the mean on Sepal.Length. To make it more simple let’s only do this for Species == &quot;setosa&quot;. library(dplyr) iris_means &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% mutate(Sepal.Width_mean = mean(Sepal.Width, na.rm = TRUE), Sepal.Length_mean = mean(Sepal.Length, na.rm = TRUE)) ggplot(iris_means, aes(Sepal.Length, Sepal.Width)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + geom_hline(aes(yintercept = Sepal.Width_mean), linetype = &quot;dashed&quot;, color = &quot;red4&quot;) + geom_vline(aes(xintercept = Sepal.Length_mean), linetype = &quot;dashed&quot;, color = &quot;green4&quot;) Something "],
["plotting-means.html", "Chapter 20 Plotting Means 20.1 Bar Graphs", " Chapter 20 Plotting Means You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) 20.1 Bar Graphs Bar graphs are the standard. They are ubiquitous across psychology. Basically everyone uses them. But in all honesty, Bar graphs SUCK!. The worst part about them is that they hide the distribution of the raw data points (even when error bars are included). Even worse, too often you will see bar graphs with NO ERROR BARS! Yikes! A bar graph with no error bars tells you almost NOTHING! To illustrate this let’s use a data set containing information on mammalian sleep patterns from the data set msleep. head(msleep) Let’s plot the relationship between the different eating habits (vore) and total sleep time (sleep_total). msleep1 &lt;- filter(msleep, !is.na(vore)) ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) This only tells us what the means are. We have no idea about the distributions. Well for this reason people usually like to see error bars. Okay well let’s add error bars. ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2) Okay better. But we still cannot see the underlying distribution. Here is a crazy idea. What if we plotted the raw data points. Like we do with scatterplots! Whoa! What a concept ggplot(msleep1, aes(vore, sleep_total)) + geom_point() When plotting raw data points with categorical variables on the x-axis it makes more sense to jitter the points so they are not all just lying on top of each other. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) Wow! Does this give you a completely different picture than the bar graph with error bars? It does to me! Especially look at the insecti and omni eating habits. There is definitely a bi-modal distribution happening there. From the bar graph with error bars, we might be fooled into thinking that the distributions for carni and omnie are pretty similar. But are they? Not at all! THIS IS WHY YOU SHOULD ALWAYS PLOT THE RAW DATA POINTS But means and error bars are also useful information so let’s add those ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) Another aesthetic option that is useful when we are plotting means and error bars ontop of raw data is the alpha aesthetic. This can allow us to make the raw data points more transparent, fade into the background a little more. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) + stat_summary(fun.y = mean, na.rm = TRUE, aes(group = 1), geom = &quot;line&quot;, color = &quot;dodgerblue&quot;, size = .75, shape = &quot;diamond&quot;) 20.1.1 Multidimensional Bar Plot library(tidyr) iris.long &lt;- iris %&gt;% mutate(Flower = row_number()) %&gt;% gather(&quot;Part&quot;, &quot;Inches&quot;, -Flower, -Species) %&gt;% separate(Part, into = c(&quot;Part&quot;, &quot;Measurement&quot;)) %&gt;% arrange(Flower, Species) %&gt;% select(Flower, Species, Part, Measurement, Inches) head(iris.long) ggplot(iris.long, aes(Measurement, Inches, group = Species, color = Species)) + geom_point(position = position_jitterdodge(jitter.width = .2, dodge.width = .7), alpha = .1) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, shape = &quot;diamond&quot;, size = 4, color = &quot;black&quot;, position = position_dodge(width = .7)) + stat_summary(fun.data = mean_cl_normal, na.rm = TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;black&quot;, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set1&quot;) Something "],
["univariate-plots.html", "Chapter 21 Univariate Plots", " Chapter 21 Univariate Plots This will be a chapter on univariate plots Something "],
["sjplot.html", "Chapter 22 sjPlot", " Chapter 22 sjPlot This will be a chapter on sjPlot Something "],
["custom-tables.html", "Chapter 23 Custom Tables", " Chapter 23 Custom Tables This will be a chapter on creating nice looking Tables in R Something "],
["introduction-to-statistical-analysis-in-r.html", "Chapter 24 Introduction to Statistical Analysis in R", " Chapter 24 Introduction to Statistical Analysis in R This will be an introduction to statistical analysis in R Something "]
]
