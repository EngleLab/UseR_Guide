[
["index.html", "EngleLab: useRguide Welcome to the EngleLab useRguide Why R? Reproducibility and Open Access But I need to analyze my data NOW! What does this book cover? Reseach Project Workflow", " EngleLab: useRguide Jason Tsukahara 2020-09-21 Welcome to the EngleLab useRguide R is a free software environment for statistical computing and graphics. It is quickly becoming the go-to software for psychologists to manage, process, and analyze data. While there are many advantages to using R over other software, such as SPSS, there is a considerable learning curve because there is no GUI - it is code based. Therefore, I decided to write this useRguide to help you become proficient in using R in our lab more quickly. Why R? The way I see it, the main advantage to learning R is not about replacing other statistical software you might use - though you may choose to do so - but it is about developing the skills and expertise in how to manage and process your data. You just cannot learn those skills using SPSS and Excel only. R is a great tool to help you implement reproducible and accessible data science practices. I would not even argue that you should stop using other programs. They have their own advantages. In my experience it is not so much of R vs. SPSS but rather R offers a functionality that SPSS just sucks at. And that is working with the data, transforming variables, merging data, filtering, grouping, aggregating scores, etc. Everything you need to do with your data before running the ANOVA, correlation, regression, or latent analysis. R is excellent at this. So if you still want to use SPSS or EQS for statistical analysis you can do so. But do everything prior to that in R. Use R to work with your data and use SPSS or EQS to conduct statistical analyses. R is also amazing at data visualization. I would take advantage of that as well. What about Python? Python is an extremely powerful programming language and is a more general purpose language than R. A lot of people think - instead of having to learn one language to create experimental tasks and another to do data processing and analysis why not just do it all in Python - why bother with R? I see no problem with that. However, one advantage to R over Python for psychological and behavioral sciences is that R was created for statisticians by statisticians. There will be more package options to do the type of analysis you want to do and there will be a larger and more specific community to get help from. Also the tidyverse set of packages is a huge advantage to R. Ultimately, you can always learn both R and Python. Reproducibility and Open Access As Open Science principles such as Open Access to Data becomes the standard in your field - learning to manage and process your data in a Reproducible way that is accessible to other researchers (and your future self!) is going to be vital to your career in science. The problem is that most of us have not been trained in how to properly manage, store, and process data in a way that is both reproducible and accessible. While using a coding/script based language like R allows us to create reproducible steps as we analyze our data it does not necessarily mean that it will be accessible! Accessibility means that both your code and data files are easy to read and understand by a wide audience (or your future self) that does not know the details of how your data were created. This means you need to have intuitive column labels, variables names, and a workflow that is easy to follow. The reason I emphasize this so much is that I have seen too many data files and scripts posted on open access repositories but they were nearly impossible to understand. Is that really Open Access? I don’t think so. Open access does not simply mean being able to download some files to your computer from a shared server - it also means that you can understand the data and script files. Obviously if they use a different programming language than what you know then you may not understand the scripts, but there should be a workflow structure of the scripts that you can generally understand. There should also be documentation like a Code Book describing in plain English all the variables, data cleaning procedures, and workflow that you used. But I need to analyze my data NOW! It is true that, at first, it may take you longer to analyze data using R than your typical route of Excel, SPSS, or EQS. Because of this, it can be tempting to use those other programs to quickly analyze some data. You may even tell yourself that you will go back and do the analysis in R later when you have more time. But, be honest with yourself, will you really? Also, the daunting uncertainty of how to do anything in R may also make you reluctant to start using R with data you have now. All this is just prioritizing short-term gains over long-term gains. This is probably the biggest barrier that you will have to learning R. STOP IT! This attitude will only keep you behind the times not just on using statistical software but other areas of your research career. It is 100% okay to take longer to analyze some data if you are also acquiring skills that will greatly benefit you in the long-term. It is okay to take longer analyzing data in a way that is consistent with reproducibility and open access principles. Your advisor is okay with this, you need to be too. What does this book cover? In Section I I will introduce you to the basics of using R, from installing R and various packages, basic R skills, and more intermediate skills such as creating your own functions or using for loops. In Section II I will introduce you to the fundamentals of working with data in R, from importing and outputting data to performing manipulations on data. These are basic skills that anyone working with data in R needs to know. This section relies heavily on the tidyverse set of packages. The first two sections are meant to provide you the fundamentals of working with data in R. Later sections will build off these skills and provide an overall workflow of how we process and analyze data in the EngleLab. In Section III I will take a step back and discuss one of the major strengths of using R. There is a lot of excitement around using R in the psychological community. Much of this excitement is coming from the excitement around the Open Science movement. Using code to process and analyze data allows a lot more transparency, reproducibility, and sharing of data and scripts. Open Science practices are quickly becoming the norm. However, one thing I am very adamant about is that simply using R does not mean your are effectively practicing Open Science principles. I have seen countless R scripts and data files that are a complete mess. This is why people say we should “comment” our code. But honestly, this is not enough. We need to figure out a workflow and consistency in our R scripts and data files that make it VERY easy for other researchers to interpret exactly how you are processing and analyzing your data. Otherwise, we will undermine the main strengths of using R; that is, transparency, reproducibility, and sharing of data and scripts. Besides, finding a workflow and consistency in R scripts will also make it easier for you to not only start learning R but to continue using it very efficiently. Eventually, you may find that you can start doing data processing and analysis in R much faster than GUI based systems. Therefore, in Section III, I introduce a reproducible workflow and discuss how we can effectively practice Open Science principles in R. I also provide some R script templates for each stage of data processing. In Section IV I cover in more detail each line of code in the R script templates and provide an example of how one would use them. In order to be more concise, this section will assume you have the knowledge and skills from the previous sections. Sections I - IV will provide you everything you need to know to create a data file that is ready for statistical analysis. In my opinion this is where R excels over other GUI programs, and it may be that this is where your R journey ends. This is okay, you can then take the data file that is ready for analysis and perform statistical analysis in other programs like SPSS, EQS, Jamovi, JASP, or any other software. However, if you want to continue using R to perform statistical analysis and create data visualizations then you may want to continue your R journey in Sections V and VI. In Section V I introduce the fundamentals of data visualization in R using ggplot2. I walk through how to build commonly used graphs. In Section VI I cover how to perform statistical analyses that I frequently use in my own research. In addition, I will show you how to create “nice looking” output of your results and create graphs and tables of your statistical models. [STILL WORKING ON THIS SECTION] Reseach Project Workflow The image below represents the general data processing stages required to go from raw data to data visualization and statistical analyses. For our lab it is important to take note that Data Preparation happens in the Data Collection repository whereas Data Analysis happens in the Data Analysis repository. Section III will cover the reason for this in more detail. The Data Preparation stage is only required because the data files created from the E-Prime or other software program are usually not in a format that is easy to use or understand. I am referring to this format as a messy raw data file. Also, there are typically other preparation steps one needs to take before they can start looking at the data. These might include merging individual subject data files and exporting the data to a non-proprietary format so we can import the data into R. The purpose of the data preparation stage is simply to create tidy raw data files from the messy raw data files. Tidy raw data files are easy to use and understand. There will be one row per trial, column labels that are easy to understand (e.g. Trial, Condition, RT, Accuracy, etc.), and values in columns that make sense. If values in a column are categorical, then the category names will be used rather than numerical values. Ideally, someone not involved in the research project should be able to look at a tidy raw data file and understand what each column represents, and what the values in the column refer to. Often times the data preparation stage is treated as temporary status of our data and we may not save or store tidy data files at this time. This is a big mistake and will cause more headaches down the road. I believe this tendency is there because we want to quickly move on to the more interesting phase - Data Analysis. As scientists, we should learn to take our time to work with our data. There are many advantages to saving and storing tidy raw data files - this will be explored more throughout this guide. What we actually care about is the Data Analysis phase. There are three main stages to data analysis; 1) scoring, cleaning, and merging data 2) data visualization, and 3) statistical analyses. Data analysis tends to be more cyclic and iterative therefore you may end up going back and forth between these stages. The first stage takes the tidy raw data files from the data preparation stage and converts them into a scored data file, usually by aggregating performance across trials. Data cleaning procedures (such as removing outliers) also occurs during this stage. The format of the scored data file will depend on the type of statistical analysis one plans on performing. The data analysis stages usually occurs in tandem with one another. Visualizing our data, running statistical analyses, visualizing our statistical models, etc. If we have more exploratory data, then based on these visualizations and analyses we may decide that we want to use different scoring or cleaning procedures. Or we want to explore our data to further understand our findings. We may then go back to the scoring and cleaning stage, and on and on. The final phase in a research project is to Write up a Manuscript to share your study and findings with the scientific community. It is also a good idea to Share your Data, R scripts, and results. The Open Science Framework is a good place to openly share your projects with other researchers. "],
["installation.html", "1 Installation 1.1 Installing R 1.2 Installing R Studio 1.3 The R Studio Environemnt 1.4 R Studio Settings", " 1 Installation 1.1 Installing R First you need to download the latest version of R from their website https://www.r-project.org Select CRAN on the left, just under Download Select the first option under 0-Cloud Select the download option depending on your computer Select the base installation (for Windows) or the Latest Release (for Mac) Open and Run the installation file 1.2 Installing R Studio The easiest way to interact with R is through the R Studio environment. To do this you need to install R Studio Select the Free version of R Studio Desktop Select the download option depending on your computer 1.3 The R Studio Environemnt Go ahead an open the RStudio application on your computer. When you open a fresh session of RStudio there are 3 window panes open. The Console window, the Environment window, and the Files window. Go ahead and navigate to File -&gt; New File -&gt; R Script. You should now see something similar to the image below There are 4 window panes and each one has it’s own set of tabs associated with it: The Console window (the bottom left window pane) is where code is executed and output is displayed. The Source window (the top left window pane) is where you will write your code to create a script file. When you open a new script file you will see a blank sheet where you can start writing the script. When you execute lines of code from here you will see it being executed in the Console window. The Source window is also where you can view data frames you have just imported or created. In the image above, notice the different tabs in the Source window. There are two “Untitled” script files open and one data frame called ‘data’. The Environment window (top right window pane) is where you can see any data frames, variables, or functions you have created. Go ahead and type the following in your Console window and hit enter. hello &lt;- &quot;hello&quot; You should now see the object hello in the Environment window pane The Files window (the bottom right window pane) is where you can see your computer’s directories, plots you create, manage packages, and see help documentation. 1.4 R Studio Settings There are a few changes to R Studio settings I suggest you make. I will not go into why these are a good idea - so just do what I say! If you want to know you can talk to me about it. Navigate to Tools -&gt; Global Options Change the settings to look like this: Be sure to set ‘Save workspace to .RData on exit’ to Never You can also change the “Editor Theme” if you navigate to the “Appearance” tab in Settings. Dark themes are easier on the eyes. I use Material dark theme. "],
["basic-r.html", "2 Basic R 2.1 Creating R objects 2.2 If…then Statements 2.3 R Packages 2.4 More R Basic Resources", " 2 Basic R This chapter will cover the basics of how to assign values to objects, create and extract information from vectors, lists, and data frames. If you have not done so already, open a new R script file. To create a new R script go to File -&gt; New File -&gt; R Script This should have opened a blank Script window called Untitled. The Script window is your workspace. This is where you will write, edit, delete, re-write, your code. To follow along with the tutorial, you should type the lines of code I display in the tutorial into your script. Go ahead and save your empty script as 2_basics.R 2.1 Creating R objects In R, everything that exists is an object and everything you do to objects are functions. You can define an object using the assignment operator &lt;-. Everything on the left hand side of the &lt;- assignment operator is an object. Everything on the right hand side of &lt;- are functions or values. Go ahead and type the following two lines of code in your script string &lt;- &quot;hello&quot; string ## [1] &quot;hello&quot; You can execute/run a line of code by placing the cursor anywhere on the line and press Ctrl + Enter. Go ahead an run the two lines of code. In this example, the first line creates a new object called string with a value of “hello”. The second line simply prints the output of string to the Console window. In the second line there is no assignment operator. When there is no &lt;- this means you are essentially just printing to the console. You can’t do anything with stuff that is just printed to the console, it is just for viewing purposes. For instance, if I wanted to calculate 1 + 2 I could do this by printing it to the console 1 + 2 ## [1] 3 However, if I wanted to do something else with the result of that calculation then I would not be able to unless I assigned the result to an object using &lt;- result &lt;- 1 + 2 result &lt;- result * 5 result ## [1] 15 The point is, you are almost always going to assign the result of some function or value to an object. Printing to the console is not very useful. Almost every line of code, then, will have an object name on the left hand side of &lt;- and a function or value on the right hand side of &lt;- In the first example above, notice how I included \" \" around hello. This tells R that hello is a string, not an object. If I were to not include \" \", then R would think I am calling an object. And since there is no object with the name hello it will print an error string &lt;- hello ## Error in eval(expr, envir, enclos): object &#39;hello&#39; not found Do not use \" \" for Numerical values a &lt;- &quot;5&quot; + &quot;1&quot; ## Error in &quot;5&quot; + &quot;1&quot;: non-numeric argument to binary operator You can execute lines of code by: Typing them directly into the Console window Typing them into the Script window and then on that line of code pressing Ctrl+Enter. With Ctrl+Enter you can execute one line of your code at a time. Clicking on Source at the top right of the Script window. This will run ALL the lines of code contained in the script file. It is important to know that EVERYTHING in R is case sensitive. a &lt;- 5 a + 5 ## [1] 10 A + 5 ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found 2.1.1 Classes Classes are types of values that exist in R: character \"hello\", \"19\" numeric (or double) 2, 32.55 integer 5, 99 logical TRUE, FALSE To evaluate the class of an object you can use the typeof() typeof(a) ## [1] &quot;double&quot; To change the class of values in an object you can use the function as.character() , as.numeric() , as.double() , as.integer() , as.logical() functions. as.integer(a) ## [1] 5 as.character(a) ## [1] &quot;5&quot; as.numeric(&quot;hello&quot;) ## Warning: NAs introduced by coercion ## [1] NA 2.1.2 Vectors Okay so now I want to talk about creating more interesting objects than just a &lt;- 5. If you are going to do anything in R it is important that you understand the different data types and data structures you can use in R. I will not cover all of them in this tutorial. For more information on data types and structures see this nice Introduction to R Vectors contain elements of data. The length of a vector is the number of elements in the vector. For instance, the variable a we created earlier is actually a vector of length 1. It contains one element with a value of 5. Now let’s create a vector with more than one element. b &lt;- c(1,3,5) c() is a function. Functions contain arguments that are inputs for the function. Arguments are separated by commas. In this example the c() function concatenates the arguments (1, 3, 5) into a vector. We are passing the result of this function to the object b. What do you think the output of b will look like? b ## [1] 1 3 5 You can see that we now have a vector that contains 3 elements; 1, 3, 5. If you want to reference the value of specific elements of a vector you use brackets [ ]. For instance, b[2] ## [1] 3 The value of the second element in vector b is 3. Let’s say we want to grab only the 2nd and 3rd elements. We can do this at least two different ways. b[2:3] ## [1] 3 5 b[-1] ## [1] 3 5 Now, it is important to note that we have not been changing vector b. If we display the output of b, we can see that it still contains the 3 elements. b ## [1] 1 3 5 To change vector b we need to define b as vector b with the first element removed b &lt;- b[-1] b ## [1] 3 5 Vector b no longer contains 3 elements. Now, let’s say we want to add an element to vector b. c(5,b) ## [1] 5 3 5 Here the c() function created a vector with the value 5 as the first element followed by the values in vector b Or we can use the variable a that has a value of 5. Let’s add this to vector b b &lt;- c(a,b) b ## [1] 5 3 5 What if you want to create a long vector with many elements? If there is a pattern to the sequence of elements in the vector then you can create the vector using seq() seq(0, 1000, by = 4) ## [1] 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 ## [19] 72 76 80 84 88 92 96 100 104 108 112 116 120 124 128 132 136 140 ## [37] 144 148 152 156 160 164 168 172 176 180 184 188 192 196 200 204 208 212 ## [55] 216 220 224 228 232 236 240 244 248 252 256 260 264 268 272 276 280 284 ## [73] 288 292 296 300 304 308 312 316 320 324 328 332 336 340 344 348 352 356 ## [91] 360 364 368 372 376 380 384 388 392 396 400 404 408 412 416 420 424 428 ## [109] 432 436 440 444 448 452 456 460 464 468 472 476 480 484 488 492 496 500 ## [127] 504 508 512 516 520 524 528 532 536 540 544 548 552 556 560 564 568 572 ## [145] 576 580 584 588 592 596 600 604 608 612 616 620 624 628 632 636 640 644 ## [163] 648 652 656 660 664 668 672 676 680 684 688 692 696 700 704 708 712 716 ## [181] 720 724 728 732 736 740 744 748 752 756 760 764 768 772 776 780 784 788 ## [199] 792 796 800 804 808 812 816 820 824 828 832 836 840 844 848 852 856 860 ## [217] 864 868 872 876 880 884 888 892 896 900 904 908 912 916 920 924 928 932 ## [235] 936 940 944 948 952 956 960 964 968 972 976 980 984 988 992 996 1000 Vectors can only contain elements of the same “class”. d &lt;- c(1, &quot;2&quot;, 5, 9) d ## [1] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;9&quot; as.numeric(d) ## [1] 1 2 5 9 2.1.3 Factors Factors are special types of vectors that can represent categorical data. You can change a vector into a factor object using factor() factor(c(&quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;)) ## [1] male female male male female female male ## Levels: female male factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;)) ## [1] high low medium high high low medium medium ## Levels: high low medium f &lt;- factor(c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;medium&quot;), levels = c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;)) f ## [1] high low medium high high low medium medium ## Levels: high medium low 2.1.4 Lists Lists are containers of objects. Unlike Vectors, Lists can hold different classes of objects. list(1, &quot;2&quot;, 2, 4, 9, &quot;hello&quot;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] &quot;hello&quot; You might have noticed that there are not only single brackets, but double brackets [[ ]] This is because Lists can hold not only single elements but can hold vectors, factors, lists, data frames, and pretty much any kind of object. l &lt;- list(c(1,2,3,4), &quot;2&quot;, &quot;hello&quot;, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) l ## [[1]] ## [1] 1 2 3 4 ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] &quot;hello&quot; ## ## [[4]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; You can see that the length of each element in a list does not have to be the same. To reference the elements in a list you need to use the double brackets [[ ]]. l[[1]] ## [1] 1 2 3 4 To reference elements within list elements you use double brackets followed by a single bracket l[[4]][2] ## [1] &quot;b&quot; You can even give names to the list elements person &lt;- list(name = &quot;Jason&quot;, phone = &quot;123-456-7890&quot;, age = 23, favorite_colors = c(&quot;blue&quot;, &quot;red&quot;, &quot;brown&quot;)) person ## $name ## [1] &quot;Jason&quot; ## ## $phone ## [1] &quot;123-456-7890&quot; ## ## $age ## [1] 23 ## ## $favorite_colors ## [1] &quot;blue&quot; &quot;red&quot; &quot;brown&quot; And you can use the names to reference elements in a list person[[&quot;name&quot;]] ## [1] &quot;Jason&quot; person[[&quot;favorite_colors&quot;]][3] ## [1] &quot;brown&quot; 2.1.5 Data Frames You are probably already familiar with this type of data structure. SPSS and Excel uses this type of structure. It is just rows and columns of data. A data table! This is the format that is used to perform statistical analyses on. So let’s create a data frame so you can see what one looks like in RStudio data &lt;- data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) data ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 You can view the Data Frame by clicking on the object in the Environment window or by executing the command View(data) Notice that it created three columns labeled id, x, and y. Also notice that since we only specified a vector of length 2 for x this column is coerced into 10 rows of repeating “a” and “b”. All columns in a data frame must have the same number of rows. You can use the $ notation to reference just one of the columns in the data frame data$y ## [1] 10 20 30 40 50 60 70 80 90 100 Alternatively you can use data[&quot;y&quot;] ## y ## 1 10 ## 2 20 ## 3 30 ## 4 40 ## 5 50 ## 6 60 ## 7 70 ## 8 80 ## 9 90 ## 10 100 To reference only certain rows within a column data$y[1:5] ## [1] 10 20 30 40 50 data[1:5,&quot;y&quot;] ## [1] 10 20 30 40 50 2.2 If…then Statements If…then statements are useful for when you need to execute code only if a certain statement is TRUE. For instance,… First we need to know how to perform logical operations in R Okay, we have this variable a a &lt;- 5 Now let’s say we want to determine if the value of a is greater than 3 a &gt; 3 ## [1] TRUE You can see that the output of this statement a &gt; 3 is TRUE Here is a list of logical operations in R Now let’s write an if…then statement. If a is greater than 3, then multiply a by 2. if (a&gt;3){ a &lt;- a*2 } a ## [1] 10 The expression that is being tested is contained in parentheses, right after the if statement. If this expression is evaluated as TRUE then it will perform the next line(s) of code. The { is just a way of encasing multiple lines of code within one if statement. The lines of code then need to be closed of with }. In this case, since we only had one line of code b &lt;- a*2 we could have just written it as. a &lt;- 5 if (a&gt;3) a &lt;- a*2 a ## [1] 10 What if we want to do something to a if a is NOT greater than 3? In other words… if a is greater than 3, then multiple a by 2 else set a to missing a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 You can keep on chaining if…then… else… if… then statements together. a &lt;- 5 if (is.na(a)){ print(&quot;Missing Value&quot;) } else if (a&lt;0){ print(&quot;A is less than 0&quot;) } else if (a&gt;3){ print(&quot;A is greater than 3&quot;) } ## [1] &quot;A is greater than 3&quot; 2.3 R Packages R comes with a basic set of functions. All the functions we have used so far are part of the R basic functions. But when you want to start doing more complex operations it would be nice to have more complex functions. This is where R Packages come in… An R Package is simply a collection of functions - that usually have some common theme to them. Now the most wonderful thing about R is that other R users have developed tons of packages with functions they created themselves. For instance, a group of users have developed an R package called lavaan that makes it extremely easy to conduct SEM in R. 2.3.1 Installing and Loading R Packages R packages are easy to install and load. You just need to know the name of the package. install.packages(&quot;name_of_package&quot;) or for multiple packages at once install.packages(c(&quot;package1&quot;, &quot;package2&quot;, &quot;package3&quot;)) Installing the package does not mean you can start using the functions. To be able to use the function you need to then load the package library of functions as such library(name_of_package) When loading packages you do not have to in case the package name in \" \" 2.4 More R Basic Resources For additional tips in the basics of coding R see: https://ramnathv.github.io/pycon2014-r/visualize/README.html https://www.datacamp.com/courses/free-introduction-to-r/?tap_a=5644-dce66f&amp;tap_s=10907-287229 http://compcogscisydney.org/psyr/ http://r4ds.had.co.nz/workflow-basics.html "],
["intermediate-r.html", "3 Intermediate R 3.1 For Loops 3.2 Functions", " 3 Intermediate R This chapter will cover more intermediate R programming, such as for loops, and functions. Save a new R script as 3_intermediate.R 3.1 For Loops For loops allow you iterate the same line of code over multiple instances. Let’s say we have a vector of numerical values c &lt;- c(1,6,3,8,2,9) c ## [1] 1 6 3 8 2 9 and want perform an if…then operation on each of the elements. Let’s use the same if…then statement we used above. If the element is greater than 3, then multiply it by 2 - else set it to missing. Let’s put the results of this if…then statement into a new vector d What we need to do is loop this if…then statement for each element in c We can start out by writing the for loop statement for (i in seq_along(c)){ } This is how it works. The statement inside of parentheses after for contains two statements separated by in. The first statement is the variable that is going to change it’s value over each iteration of the loop. You can name this whatever you want. In this case I chose the label i. The second statement defines all the values that will be used at each iteration. The second statement will always be a vector. In this case the vector is seq_along(c). seq_along() is a function that creates a vector that contains a sequence of numbers from 1 to the length of the object. In this case the object is the vector c, which has a length of 6 elements. Therefore seq_along(c), creates a vector containing 1, 2, 3, 4, 5, 6. The for loop will start with i defined as 1, then on the next iteration the value of i will be 2 … and so until the last element of seq_along(c), which is 6. We can see how this is working by printing ‘i’ on each iteration. for (i in seq_along(c)){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 You can see how on each iteration it prints the values of seq_along(c) from the first element to the last element. What we will want to do is, on each iteration of the for loop, access the ith element of the vector c. Recall, you can access the element in a vector with [ ], for instance c[1]. Let’s print each ith element of c. for (i in seq_along(c)){ print(c[i]) } ## [1] 1 ## [1] 6 ## [1] 3 ## [1] 8 ## [1] 2 ## [1] 9 Now instead of printing i the for loop is printing each element of vector c. Let’s use the same if…then statement as above a &lt;- 5 if (a&gt;3){ a &lt;- a*2 } else { a &lt;- NA } a ## [1] 10 But instead we need to replace a with c[i] For now let’s just print() the output of the if… then statement. for (i in seq_along(c)){ if (c[i] &gt; 3){ print(c[i]*2) } else { print(NA) } } ## [1] NA ## [1] 12 ## [1] NA ## [1] 16 ## [1] NA ## [1] 18 Now for each element in c, if it is is greater than 3, then multiply it by 2 - else set as missing value. You can see that on each iteration the output is either the ith element of c multiplied by 2 or NA. But just printing things to the console is useless! Let’s overwright the old values in c with the new values. for (i in seq_along(c)){ if (c[i] &gt; 3){ c[i] &lt;- c[i]*2 } else { c[i] &lt;- NA } } But what if we want to preserve the original vector c? Well we need to put it into a new vector, let’s call it vector d. This get’s a little more complicated but is something you might find yourself doing fairly often so it is good to understand how this works. But if you are goind to do this to a “new” vector that is not yet created you will run into an error. c &lt;- c(1,6,3,8,2,9) for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } You first need to create vector d - in this case we can create an empty vector. d &lt;- c() So the logic of our for loop, if…then statement is such that; on the ith iteration - if c[i] is greater than 3, then set d[i] to c[i]*2 - else set d[i] to NA. c &lt;- c(1,6,3,8,2,9) d &lt;- c() for (i in seq_along(c)){ if (c[i] &gt; 3){ d[i] &lt;- c[i]*2 } else { d[i] &lt;- NA } } c ## [1] 1 6 3 8 2 9 d ## [1] NA 12 NA 16 NA 18 Yay! Good job. 3.2 Functions Basically anything you do in R is by using functions. In fact, learning R is just learning what functions are available and how to use them. Not much more to it than that. You have only seen a couple of functions at this point. In this chapter, a common function used was c(). This function simply concatenates a series of numerical or string values into a vector. c(1,6,3,7). Functions start with the name of the function followed by parentheses function_name(). Inside the () is where you specify certain arguments separated by commas , . Some arguments are optional and some are required for the function to work. For example, another function you saw last chapter was data.frame(). This function creates a data frame with the columns specified by arguments. data.frame(id = 1:10, x = c(&quot;a&quot;, &quot;b&quot;), y = seq(10,100, by = 10)) ## id x y ## 1 1 a 10 ## 2 2 b 20 ## 3 3 a 30 ## 4 4 b 40 ## 5 5 a 50 ## 6 6 b 60 ## 7 7 a 70 ## 8 8 b 80 ## 9 9 a 90 ## 10 10 b 100 The arguments id, x, and y form the columns in the data frame. These arguments themselves used functions. For instance y used the function seq(). This function creates a sequence of numbers in a certain range at a given interval. Sometimes arguments are not defined by an =. The first two arguments in in seq() specify the range of 10 to 100. The third argument by specified the interval to be 10. So seq(10, 100, by = 10) creates a sequence of numbers ranging from 10 to 100 in intervals of 10. seq(10, 100, by = 10) ## [1] 10 20 30 40 50 60 70 80 90 100 In the seq() function the by argument is not required. This is because there is a default by value of 1. seq(10, 100) ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ## [23] 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 ## [45] 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 ## [67] 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 ## [89] 98 99 100 Obviously if you want to specify a different interval, then you will need to specify by =. 3.2.1 Creating Your Own Functions This section is optional. It will go over how to create your own functions. Even if you do not want to get too proficient in R, it can be a good idea to know how to create your own function. It also helps you better understand how functions actually work. We are going to create a function that calculates an average of values. To define a function you use the function() and assign the output of function() to an object, which becomes the name of the function. For instance, function_name &lt;- function(){ } This is a blank function so it is useless. Before we put stuff inside of a function let’s work out the steps to calculate an average. Let’s say we have an array a that has 10 elements a &lt;- c(1,7,4,3,8,8,7,9,2,4) a ## [1] 1 7 4 3 8 8 7 9 2 4 To calculate an average we want to take the sum of all the values in a and divide it by the number of elements in a. To do this we can use the sum() and length() functions. sum(a) ## [1] 53 length(a) ## [1] 10 sum(a)/length(a) ## [1] 5.3 Easy! So now we can just put this into a function. average &lt;- function(x){ avg &lt;- sum(x)/length(x) return(avg) } When creating a function, you need to specify what input arguments the function is able to take. Here were are specifying the argument x. You can use whatever letter or string of letters you want, but a common notation is to use x for the object that is going to be evaluated by the function. Then, inside the function we use the same letter x to calculate the sum() and length() of x. What this means is that Arguments specified in a function become objects (or variables) passed inside the function You can create new objects inside a function. For instance we are creating an object, avg. However, these objects are created only inside the environment of the function. You cannot use those objects outside the function and they will not appear in your Environment window. To pass the value of an object outside of the function, you need to specify what you want to return() or what is the outpute of the function. In this case it is the object avg that we created inside the function. Let’s see the function in action average(a) ## [1] 5.3 Cool! You created your first function. Becuase the function only takes one argument x it knows that whatever object we specify in average() is the object we want to evaluate. But what if our vector contains missing values? b &lt;- c(1,NA,4,2,7,NA,8,4,9,3) average(b) ## [1] NA Uh oh. Here the vector b contains two missing values and the function average(b) returns NA. This is because in our function we use the function sum() without specifying to ignore missing values. If you type in the console ?sum you will see that there is an argument to specify whether missing values should be removed or not. The default value of this argument is FALSE so if we want to remove the missing values we need to specify na.rm = TRUE. It is a good idea to make your functions as flexible as possible. Allow the user to decide what they want to happen. For instance, it might be the case that the user wants a value of NA returned when a vector contains missing values. So we can add an argument to our average() function that allows the user to decide what they want to happen; ignore missing values or return NA if missing values are present. Let’s label this argument na.ignore. We could label it na.rm like the sum() function but for the sake of this Tutorial I want you to learn that you can label these arguments however you want, it is arbitrary. The label should make sense however. Before we write the function let’s think about what we need to change inside the function. Basically we want our new argument na.ignore to change the value of na.rm in the sum() function. If na.ignore is TRUE then we want na.rm = TRUE. Remember that arguments become objects inside of a function. So we will want to change: avg &lt;- sum(x)/length(x) to avg &lt;- sum(x, na.rm = na.ignore)/length(x) Let’s try this out on our vector b na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b) ## [1] 3.8 We can test if our average function is calculating this correctly by using the actual base R function mean(). mean(b, na.rm = TRUE) ## [1] 4.75 Uh oh. We are getting different values. This is because length() is also not ignoring missing values. The length of b, is 10. The length of b ignoring missing values is 8. Unfortunately, length() does not have an argument to specify we want to ignore missing values. How we can tell length() to ignore missing values is by length(b[!is.na(b)]) ## [1] 8 This is saying, evaluate the length of elements in b that are not missing. Now we can modify our function with na.ignore &lt;- TRUE sum(b, na.rm = na.ignore)/length(b[!is.na(b)]) ## [1] 4.75 to get average &lt;- function(x, na.ignore = FALSE){ avg &lt;- sum(x, na.rm = na.ignore)/length(x[!is.na(x)]) return(avg) } average(b, na.ignore = TRUE) ## [1] 4.75 mean(b, na.rm = TRUE) ## [1] 4.75 Walla! You did it. You created a function. Notice that we set the default value of na.ignore to FALSE. If we had set it as TRUE then we would not need to specify average(na.ignore = TRUE) since TRUE would have been the default. When using functions it is important to know what the default values are Both for loops and functions allow you to write more concise and readable code. If you are copying and pasting the same lines of code with only small modification, you can probably write those lines of code in a for loop or a function. "],
["working-with-data-overview.html", "Working with Data: Overview", " Working with Data: Overview In this section you will learn how to work with data in R by using a collection of packages known as the tidyverse The tidyverse is a collection of R packages that share an underlying design philosophy, grammar, and data structures. Hadley Wickham has been the main contributor to developing the tidyverse. Although you will be learning R in this tutorial, it might be more appropriate to say that you are learning the tidyverse. The tidyverse consists of packages that are simple and intuitive to use and will take you from importing data (with readr), to transforming and manipulating data structures (with dplyr and tidyr), and to data visualization (with ggplot2). "],
["importing-and-outputing-data.html", "4 Importing and Outputing Data 4.1 CSV 4.2 Tab-Delimited 4.3 SPSS 4.4 RStudio Import GUI 4.5 Import and Merge Multiple Data Files", " 4 Importing and Outputing Data Every R script that you write will require you to import a data file and output a new data file. In this Chapter you will learn various functions to import and output comma-separate value (csv), tab-delimited, and SPSS data files. For most of these data types we can use the readr package The readr package contains useful functions for importing and outputting data files. Go ahead and install the readr package. In the console type: install.packages(&quot;readr&quot;) We will also use the foreign and haven packages for SPSS data files install.packages(&quot;foreign&quot;) install.packages(&quot;haven&quot;) You do not really need to save an R script file for this Chapter. We will use some example data files for this chapter. Go ahead and download these files. You will have to unzip the file. For now just unzip it in your downloads folder. Inside the unzipped folder you will see a number of data files in different file formats. Download Example Import Data Files 4.1 CSV csv files are by far the easiest files to import into R and most software programs. For this reason, I suggest any time you want to save/output a data file to your computer, do it in csv format. 4.1.1 Import .csv We can import csv files using read_csv() from the readr package. library(readr) read_csv(&quot;filepath/datafile.csv&quot;) You can see this is very simple. We just need to specify a file path to the data. I will talk more about file paths later but for now we will use absolute file paths, although it is highly suggested not to use them. In general, DO NOT USE ABSOLUTE FILE PATHS! This chapter is more about the different functions to import various types of data files. First, figure out the absolute file path to your downloads folder (or wherever the unzipped data folder is located). On Windows the absolute file path will usually start from the C:/ drive. On Macs, it starts from ~/ Import the Flanker_Scores.csv file. You might have something that looks like read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) However, this just printed the output of read_csv() to the console. To actually import this file into R, we need to assign it to an object in our Environment. import_csv &lt;- read_csv(&quot;~/Downloads/Flanker_Scores.csv&quot;) You can name the object whatever you like. I named it import_csv. To view the data frame View(import_csv) 4.1.2 Output .csv We can output a csv file using write_csv() from the readr package. write_csv(object, &quot;filepath/filename.csv&quot;) Let’s output the object import_csv to a csv file named: new_Flanker_Scores.csv to the downloads folder write_csv(import_csv, &quot;~/Downloads/new_Flanker_Scores.csv&quot;) Note that whenever writing (outputting) a file to our computer there is no need to assign the output to an object. 4.2 Tab-Delimited tab-delimited files are a little more tedious to import just because they require specifying more arguments. Which means you have to memorize more to import tab-delimited files. 4.2.1 Import .txt To import a tab-delimited file we can use read_delim() from the readr package. read_delim(&quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) There are three additional arguments we have to specify: delim, escape_double, and trim_ws. The notation for tab-delimted files is \"\\t\". Let’s import the Flanker_raw.txt file import_tab &lt;- read_delim(&quot;~/Downloads/Flanker_raw.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) View the import_tab object 4.2.2 Output .txt We can output a tab-delimited file using write_delim() from the readr package. write_delim(object, path = &quot;filepath/filename.txt&quot;, delim = &quot;\\t&quot;) Output the import_tab object to a file named: new_Flanker_raw.txt write_delim(import_tab, path = &quot;~/Downloads/Flanker_raw.txt&quot;, delim = &quot;\\t&quot;) 4.3 SPSS As horrible as it might sound, there might be occasions where we need to import an SPSS data file. And worse, we might need to output an SPSS data file! I will suggest to use different packages for importing and outputing spss files. 4.3.1 Import .sav To import an SPSS data file we can use read.spss() from the foreign package. library(foreign) read.spss(&quot;filepath/filename.sav&quot;, to.data.frame = TRUE, use.value.labels = TRUE) The use.value.labels argument allows us to import the value labels from an SPSS file. Import and View the sav file CH9 Salary Ex04.sav import_sav &lt;- read.spss(&quot;~/Downloads/CH9 Salary Ex04.sav&quot;) 4.3.2 Output .sav To output an SPSS data file we can use write_sav() from the haven packge. library(haven) write_sav(object, &quot;filepath/filename.sav&quot;) Go ahead and output the import_sav object to a file: new_CH9 Salary Ex04.sav write_sav(import_sav, &quot;~/Downloads/new_CH9 Salary Ex04.sav&quot;) 4.4 RStudio Import GUI The nice thing about R Studio is that there is also a GUI for importing data files. When you are having difficulty importing a file correctly or unsure of the file format you can use the RStudio Import GUI. In the Environment window click on “Import Dataset”. You will see several options available, these options all rely on different packages. Select whatever data type you want to import You will see a data import window open up that looks like this Select “Browse” on the top right and select the data file you want to import. The “Data Preview” window will let you see if it is importing it in the right format. You can change the import options below this. You might want to change the “Name” but you can always do this later in the R Script. Make sure all the settings are correct by assessing the “Data Preview” window. Does the data frame look as you would expect it to? Finally, copy and paste the code you need in the “Code Preview” box at the bottom right. You might not always need the library(readr) or View(data) lines. Rather than selecting “Import” I suggest just closing out of the window and pasting the code into your R script. csv files have a nice feature in that RStudio knows that these are file types we might want to import. So instead of navigating through the Import Dataset GUI we can just click on the file in the Files window pane. 4.5 Import and Merge Multiple Data Files You might find yourself in a situation where you need to import multiple data files and merge them into a single data frame. You could import each data file one at a time using the functions above and then merge them using some type of merging function. However, if you have a lot of data files you need to import and merge this can be very tedious. Therefore, I have written a general use function to do this for us. In R, a “join” is merging data frames together that have at least some rows in common (e.g. Same Subject IDs) and have at least one column that is different. The rows that are common serve as the reference for how to “join” the data frames together. In R, a “bind” is combining data frames together by staking either the rows or columns. It is unlikely that we you will need to do a column bind so we can skip that. A row “bind” takes data frames that have the same columns but different rows. This will happen if you have separate data files for each subject from the same task. Each subject data file will have their unique rows (subject by trial level data) but they will all have the same columns. For example, the E-Merge software program is performing a row “bind” of each subject .edat file. For E-Prime data we have to go through the E-Merge software program to bind individual subject files. However, you might have individual subject data files not from E-Prime that you need to merge. Or you may want to merge data files from multiple tasks into one big merged file. My datawrangling package contains two functions to merge data files together: files_join() files_bind() They both work in a similar way. The files you want to merge need to be in the same folder on your computer. You specify the location of this folder using the path = argument. You need to specify a pattern that uniquely identifies the files you want to merge (e.g. “.txt”, or “Flanker”) using the pattern = argument. Then specify the directory and file name you want to save the merge file to using the output.file = argument. If you do not specify output.file then it will not save the file to your computer but it will import the files as a single merged data frame in your R environment. Here are the arguments that can be specified: path: Folder location of files to be merged pattern: Pattern to identify files to be merged delim: Delimiter used in files. output.delim: Delimiter to be used in output file. Default is , (csv) na: How are missing values defined in files to be merged. Default is NA output.file: File name and path to be saved to. id: Subject ID column name. ONLY for files_join() For example: library(datawrangling) files_bind(&quot;filepath/data/subj&quot;, pattern = &quot;Flanker&quot;, delim = &quot;\\t&quot;, output.file = &quot;filepath/data/filename_merged.csv&quot;) This will bind any files in the directory filepath/data/subj that contain the string \"Flanker\" and output the merged data to a file called filename_merged.csv to the directory filepath/data. "],
["data-manipulation-using-dplyr.html", "5 Data Manipulation using dplyr 5.1 Setup 5.2 Import 5.3 rename() 5.4 filter() 5.5 select() 5.6 mutate() 5.7 case_when() 5.8 group_by() 5.9 summarise() 5.10 pivot_wider() 5.11 pivot_longer() 5.12 Pipe Operator %&gt;%", " 5 Data Manipulation using dplyr In this Chapter you will learn the fundamentals of data manipulation in R. In the Getting Started in R section you learned about the various types of objects in R. The most important object you will be using is the dataframe. Last Chapter you learned how to import data files into R as dataframes. Now you will learn how to do stuff to that data frame using the dplyr package (which is of course part of the tidyverse) dplyr is one of the most useful packages in R. It uses a Grammar of Data Manipulation that is intuitive and easy to learn. The language of dplyr will be the underlying framework for how you will think about manipulating a dataframe. Not only is the language of dplyr intuitive but it allows you to perform data manipulations all within the dataframe itself, without having to create external variables, lists, for loops, etc. It can be tempting to hold information outside of a data frame but in general I suggest avoiding this strategy. Instead, hold the information in a new column within the data frame itself. For example: A common strategy I see any many R scripts is to hold the mean or count of a column of values outside the dataframe and in a new variable in the Environment. data &lt;- data.frame(x = c(1,6,4,3,7,5,8,4), y = c(2,3,2,1,4,6,4,3)) x_mean &lt;- mean(data$x) This variable then could be used to subtract out the mean from the values in column y data &lt;- mutate(data, y_new = y - x_mean) head(data) ## x y y_new ## 1 1 2 -2.75 ## 2 6 3 -1.75 ## 3 4 2 -2.75 ## 4 3 1 -3.75 ## 5 7 4 -0.75 ## 6 5 6 1.25 mutate() is a dplyr function you will learn about in a second. In general, I would advise against this strategy. A better strategy is to do all this without leaving the data frame data. data &lt;- data.frame(x = c(1,6,4,3,7,5,8,4), y = c(2,3,2,1,4,6,4,3)) data &lt;- mutate(data, x_mean = mean(x), y_new = y - x_mean) head(data) ## x y x_mean y_new ## 1 1 2 4.75 -2.75 ## 2 6 3 4.75 -1.75 ## 3 4 2 4.75 -2.75 ## 4 3 1 4.75 -3.75 ## 5 7 4 4.75 -0.75 ## 6 5 6 4.75 1.25 It can tempting to also think about writing for loops in your R script, but honestly for the most part for loops are avoidable thanks to a dplyr function called group_by(). The only time I end up needing a for loop is when importing a long list of files, or when creating code to put into a function. dplyr uses intuitive language that you are already familiar with. As with any R function, you can think of functions in the dplyr package as verbs - that refer to performing a particular action on a data frame. The core dplyr functions are: rename() renames columns filter() filters rows based on their values in specified columns select() selects (or removes) columns mutate() creates new columns based on transformation from other columns, edits values within existing columns group_by() splits data frame into separate groups based on specified columns summarise() aggregates across rows to create a summary statistic (means, standard deviations, etc.) For more information on these functions Visit the dplyr webpage If you have not done so already, install the dplyr package install.packages(&quot;dplyr&quot;) You will also need the tidyr package (a tidyverse package) for this Chapter install.packages(&quot;tidyr&quot;) Save a new R script file as 5_dplyr.R For this Chapter we will use an example data set from the Flanker task. This data set is a tidy raw data file for over 100 subjects on the Flanker task. There is one row per Trial per Subject and there is RT and Accuracy data on each Trial. Each Trial is either congruent or incongruent. What we will want to do is calculate a FlankerEffect for each Subject so that we end up with one score for each Subject. Go ahead and download the example data set and save it wherever you wish. We will talk about how to organize your data and R scripts in section III. Workflow. Download Example Tidyverse Data 5.1 Setup At the top of your script load the three packages you will need for this Chapter ## Setup library(readr) library(dplyr) library(tidyr) Notice how I added a commented line at the top. Adding comments to your scripts is highly advisable, as it will help you understand your scripts when you come back to them after not working on them for a while. You only need to add a single # to create a commented line. You will also notice that it printed out some warning messages. Sometimes different packages have the same function names. So when you load a package it may override or mask functions from other packages that are already loaded. 5.2 Import Import the data file you downloaded. Refer to Chapter 5 for importing data into R. import &lt;- read_csv(&quot;Data Files/tidyverse_example.csv&quot;) It is always a good idea to get to know your dataframe before you start messing with it. What are the column names? What kind of values are stored in each column? How many observations are there? How many Subjects? How many Trials? etc. What are the column names? use colnames() for a quick glance at the column names colnames(import) ## [1] &quot;Subject&quot; &quot;TrialProc&quot; &quot;Trial&quot; ## [4] &quot;Condition&quot; &quot;RT&quot; &quot;ACC&quot; ## [7] &quot;Response&quot; &quot;TargetArrowDirection&quot; &quot;SessionDate&quot; ## [10] &quot;SessionTime&quot; To take a quick look at the first few rows of a dataframe use head(). head(import) ## # A tibble: 6 x 10 ## Subject TrialProc Trial Condition RT ACC Response TargetArrowDire… SessionDate ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 14000 practice 1 incongru… 1086 1 left left 08-30-2017 ## 2 14000 practice 2 incongru… 863 1 left left 08-30-2017 ## 3 14000 practice 3 congruent 488 1 right right 08-30-2017 ## 4 14000 practice 4 incongru… 588 1 right right 08-30-2017 ## 5 14000 practice 5 congruent 581 1 right right 08-30-2017 ## 6 14000 practice 6 incongru… 544 1 right right 08-30-2017 ## # … with 1 more variable: SessionTime &lt;time&gt; This gives you a good idea of what column names you will be working with and what kind of values they contain. To evaluate what are all the unique values in a column you can use unique(). You can also use this in combination with length() to evaluate how many unique values are in a column. unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; unique(import$Trial) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## [23] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ## [45] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 ## [67] 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 ## [89] 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 ## [111] 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 ## [133] 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 ## [155] 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 ## [177] 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 max(import$Trial) ## [1] 192 length(unique(import$Subject)) ## [1] 410 unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$ACC) ## [1] 1 0 All these functions we just used from colnames() to unique() were to temporarily evaluate our data. They are not required to perform the actual data analysis. Therefore, I usually just type these in the console. A general rule of thumb is that if it is not required to be saved in your Script file then just type it in the console. Okay let’s take a look at how to use the dplyr functions to score this data. 5.3 rename() We do not really need to, but let’s go ahead and rename() a column. How about instead of ACC let’s label it as Accuracy. Pretty simple data &lt;- rename(import, Accuracy = ACC) rename() is really only useful if you are not also using select() or mutate(). In select() you can also rename columns as you select them to keep. This will be illustrated this later Notice that I passed the output of this function to a new object data. I like to keep the object import as the original imported file and any changes will be passed onto a new data frame, such as data. This makes it easy to go back and see what the original data is. Because if we were to overwrite import then we would have to execute the read_csv() import function again to be able to see the original data file, just a little more tedious. 5.4 filter() filter() is an inclusive filter and requires the use of logical statements. In Chapter 2: Basic R I talked a little bit about logical statements. Here is a list of logical operators in R: In addition to these logical operators, these functions can be used infilter(): is.na() - include if missing !is.na() - include if not missing between() - values that are between a certain range of numbers near() - values that are near a certain value We do not want to include practice trials when calculating the mean on RTs. We will use filter() to remove these rows. First let’s evaluate the values in these columns unique(import$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; unique(import$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; We can specify our filter() in a couple of different ways data &lt;- filter(data, TrialProc != &quot;practice&quot;, Condition != &quot;neutral&quot;) or data &lt;- filter(import, TrialProc == &quot;real&quot;, Condition == &quot;congruent&quot; | Condition == &quot;incongruent&quot;) Specifying multiple arguments separated by a comma , in filter() is equivalent to an &amp; (and) statement. In the second option, since there are two types of rows on Condition that we want to keep we need to specify Condition == twice, separated by | (or). We want to keep rows where Condition == \"congruent\" or Condition == \"incongruent\" Notice that the arguments have been separated on different lines. This is okay to do and makes it easier to read the code. Just make sure the end of the line still has a comma. Go ahead and view data. Did it properly remove practice trials? How about neutral trials? unique(data$TrialProc) ## [1] &quot;real&quot; unique(data$Condition) ## [1] &quot;incongruent&quot; &quot;congruent&quot; Again you should type these in the console NOT in the R Script! There is a lot of consistency of how you specify arguments in the dplyr package. You always first specify the data frame that the function is being performed on, followed by the arguments for that function. Column names can be called just like regular R objects, that is without putting the column name in \" \" like you do with strings. If all you know is dplyr, then this might not seem like anything special but it is. Most non-tidyverse functions will require you to put \" \" around column names. 5.5 select() select() allows you to select which columns to keep and/or remove. Let’s keep Subject, Condition, RT, Trial, and Accuracy and remove TrialProc, TargetArrowDirection, SessionDate, and SessionTime. select() is actually quite versatile - you can remove columns by specifying certain patterns. I will only cover a couple here, but to learn more Visit the select() webpage We could just simply select all the columns we want to keep data &lt;- select(data, Subject, Condition, RT, Trial, Accuracy) alternatively we can specify which columns we want to remove by placing a - in front of the columns data &lt;- select(data, -TrialProc, -TargetArrowDirection, -SessionDate, -SessionTime) or we can remove (or keep) columns based on a pattern. For instance SessionDate and SessionTime both start with Session data &lt;- select(data, -TrialProc, -TargetArrowDirection, -starts_with(&quot;Session&quot;)) You might start realizing that there is always more than one way to perform the same operation. It is good to be aware of all the ways you can use a function because there might be certain scenarios where it is better or even required to use one method over another. In this example, you only need to know the most straightforward method of simply selecting which columns to keep. You can also rename variables as you select() them… let’s change Accuracy back to ACC… just because we are crazy! data &lt;- select(data, Subject, Condition, RT, Trial, ACC = Accuracy) We are keeping Subject, Condition, RT, Trial, and renaming ACC to Accuracy. 5.6 mutate() mutate() is a very powerful function. It basically allows you to do any computation or transformation on the values in the data frame. You can change the values in already existing columns create new columns based on transformation of other columns 5.6.1 Changing values in an existing column Reaction times that are less than 200 milliseconds most likely do not reflect actual processing of the task. Therefore, it would be a good idea to not include these when calculating means. What we are going to do is is set any RTs that are less than 200 milliseconds to missing, NA. First let’s make sure we even have trials that are less than 200 milliseconds. Two ways to do this. 1) View the data frame and click on the RT column to sort by RT. You can see there are RTs that are as small as 1 millisecond! Oh my, that is definitely not a real reaction time. 2) you can just evaluate the minimum value in the RT column: min(data$RT) ## [1] 0 Now lets mutate() data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT)) Since we are replacing values in an already existing column we can just specify that column name, RT = followed by the transformation. Here we need to specify an if…then… else statement. To do so within the mutate() function we use the function called ifelse(). ifelse() evaluates a logical statement specified in the first argument, RT &lt; 200. mutate() works on a row-by-row basis. So for each row it will evaluate whether RT is less than 200. If this logical statement is TRUE then it will perform the next argument, in this case sets RT = NA. If the logical statement is FALSE then it will perform the last argument, in this case sets RT = RT (leaves the value unchanged). 5.6.2 Creating a new column Let’s say for whatever reason we want to calculate the difference between the RT on a trial minus the overall grand mean RT (for now, across all subjects and all trials). This is not necessary for what we want in the end but what the heck, let’s be a little crazy. (I just need a good example to illustrate what mutate() can do.) So first we will want to calculate a “grand” mean RT. We can use the mean() function to calculate a mean. mean(data$RT, na.rm = TRUE) ## [1] 529.1414 Since we replaced some of the RT values with NA we need to make sure we specify in the mean() function to remove NAs by setting na.rm = TRUE. We can use the mean() function inside of a mutate() function. Let’s put this “grand” mean in a column labeled grandRT. First take note of how many columns there are in data ncol(data) ## [1] 5 So after calculating the grandRT we should expect there to be one additional column for a total of 6 columns data &lt;- mutate(data, grandRT = mean(RT, na.rm=TRUE)) Cool! Now let’s calculate another column that is the difference between RT and grandRT. data &lt;- mutate(data, RTdiff = RT - grandRT) We can put all these mutate()s into one mutate() data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) Notice how I put each one on a separate line. This is just for ease of reading and so the line doesn’t extend too far off the page. Just make sure the commas are still there at the end of each line. 5.7 case_when() Often times you will want to mutate() values conditionally based on values in other columns. There are two functions that will help you do this, ifelse() and case_when(). ifelse() is a base R function and case_when() is a dplyr function. ifelse() takes the format: ifelse(conditional argument, value if TRUE, value if FALSE) As an example, lets say we want to code a new variable that indicates whether the reaction time on a trial met a certain response deadline or not. Let’s call this column Met_ResponseDeadline and give a value of 1 to trials that met the deadline and 0 to trials that did not meet the deadline. Let’s set the response deadline at a reaction time of 500 milliseconds. The conditional argument will take the form: RT is less than or equal to 500. If this statement is TRUE, then we will assign a value of 1 to the column Met_ResponseDeadline. If this statement is FALSE, then we will assign a value of 0 to the column Met_ResponseDeadline. The code looks like: data &lt;- import %&gt;% mutate(Met_ResponseDeadline = ifelse(RT &lt;= 500, 1, 0)) Check out data to make sure it worked. You can even combine multiple ifelse() statements into one. Let’s say we actually want to recode the column ACC to reflect not just correct and incorrect response but also whether they met the response deadline or not. That is, a value of 1 will represent responses that were correct AND met the response deadline and values of 0 represent responses that were either incorrect, did not meet the response deadline, or both. data &lt;- import %&gt;% mutate(ACC = ifelse(ACC == 1, ifelse(RT &lt;= 500, 1, 0), 0)) The arguments for the first ifelse() are as follows: Accuracy is equal to 1. If TRUE, then second ifelse() statement. If FALSE, then 0. This makes sense because if the accuracy is 0 (incorrect), then the value needs to remain 0. However, if the accuracy is 1, the value will depend on whether the reaction time is less than 500 (thus the second ifelse()). If accuracy is equal to 1, then if reaction time is less than or equal to 500, then set accuracy to 1. If FALSE, then set accuracy to 0. Know that you can place the additional ifelse() statement in either the TRUE or FALSE argument and can keep iterating on ifelse() statements for as long as you need (however that can get pretty complicated). case_when() is an alternative to an ifelse(). Anytime you need multiple ifelse() statements case_when() tends to simplify the code and logic involved. Let’s see examples of the two examples provided for ifelse() as a comparison. data &lt;- import %&gt;% mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1, RT &gt; 500 ~ 0)) Notice that the notation is quite different here. Each argument contains the format: conditional statement followed by the symbol ~ (this should be read as “then set as”) and then a value to be assigned when the conditional statement is TRUE. There is no value to specify when it is FALSE. Therefore, it is important when using the case_when() function to either 1) include enough TRUE statement arguments to cover ALL possible values or 2) use the uncharacteristically non-intuitive notation - TRUE ~ \"some value\". In the example above, all possible RT values are included in the two arguments RT &lt;= 500 and RT &gt; 500. To provide an example of the second option: data &lt;- import %&gt;% mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1, TRUE ~ 0)) The case_when() function will evaluate each argument in sequential order. So when it gets to the last argument (and this should always be the last argument), this is basically saying, when it is TRUE that none of the above arguments were TRUE (hence why this argument is being evaluated) then (~) set the value to “some value” (whatever value you want to specify). Now this function gets a little more complicated if you want to set values to NA. NA values are technically logical values like TRUE or FALSE. The values in a column can only be of one type; numerical, character, logical, etc. Therefore, if you have numerical values in a column but want to set some to NA, then this becomes an issue when using case_when() (hopefully this will be fixed in future updates to dplyr). For now, how to get around this is changing the type of value that NA is. For instance; as.numeric(NA), as.character(NA). data &lt;- import %&gt;% mutate(Met_ResponseDeadline = case_when(RT &lt;= 500 ~ 1, RT &gt; 500 ~ 0, TRUE ~ as.numeric(NA))) Now on to the example in which we used two ifelse() statements. data &lt;- import %&gt;% mutate(ACC = case_when(ACC == 1 &amp; RT &lt;= 500 ~ 1, ACC == 1 &amp; RT &gt; 500 ~ 0, ACC == 0 ~ 0)) When you have multiple ifelse() statements case_when() becomes easier to read. Compare this use of case_when() with the equivalent ifelse() above. The case_when() function makes it very explicit what is happening. There are three conditional statements, therefore three categories of responses. A correct response and reaction time that meets the deadline. A correct response and reaction time that DOES NOT meet the deadline. An incorrect response These three options cover all possible combinations between the the two columns ACC and RT. Accuracy should only be set to 1 (correct) with the first option and that is made quite clearly because it is the only one with ~ 1. This is not as obvious in the ifelse() example. Let’s move on to the next dplyr function. 5.8 group_by() This function is very handy if we want to perform functions separately on different groups or splits of the data frame. For instance, maybe instead of calculating an overall “grand” mean we want to calculate a “grand” mean for each Subject separately. Instead of manually breaking the data frame up by Subject, the group_by() function does this automatically in the background. Like this… data &lt;- group_by(data, Subject) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) You will now notice that each subject has a different grandRT, simply because we specified group_by(data, Subject). Let’s say we want to do it not just grouped by Subject, but also Condition. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) group_by() does not only work on mutate() - it will work on any other functions you specify after group_by(). Therefore, it can essentially replace most uses of for loops. I suggest exercising caution when using group_by() because the grouping will be maintained until you specify a different group_by() or until you ungroup it using ungroup(). So I always like to ungroup() immediately after I am done with it. data &lt;- group_by(data, Subject, Condition) data &lt;- mutate(data, RT = ifelse(RT &lt; 200, NA, RT), grandRT = mean(RT, na.rm = TRUE), RTdiff = RT - grandRT) data &lt;- ungroup(data) 5.9 summarise() The summarise() function will reduce a data frame by summarizing values in one or multiple columns. The values will be summarised on some statistical value, such as a mean, median, or standard deviation. Remember that in order to calculate the FlankerEffect for each subject, we first need to calculate each subject’s mean RT on incongruent trials and their mean RT on congruent trials We’ve done our filtering, selecting, mutating, now let’s aggregate RTs across Condition to calculate mean RT. We will use a combo of group_by() and summarise(). summarise() is almost always used in conjunction with group_by(). Let’s also summarise the mean accuracy across conditions. data &lt;- group_by(data, Subject, Condition) data &lt;- summarise(data, RT.mean = mean(RT, na.rm = TRUE), ACC.mean = mean(ACC, na.rm = TRUE)) ## `summarise()` regrouping output by &#39;Subject&#39; (override with `.groups` argument) data &lt;- ungroup(data) To summarise() you need to create new column names that will contain the aggregate values. RT.mean seems to make sense to me. What does the resulting data frame look like? There should be three rows per subject, one for incongruent trials, one for congruent trials, and one for neutral trials. You can see that we now have mean RTs on all conditions for each subject. Also, notice how non-group_by columns got removed: Trial, and ACC. 5.10 pivot_wider() Our data frame now looks like head(data) ## # A tibble: 6 x 4 ## Subject Condition RT.mean ACC.mean ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 congruent 401. 0.931 ## 2 14000 incongruent 510. 0.574 ## 3 14001 congruent 392. 0.980 ## 4 14001 incongruent 423. 0.852 ## 5 14002 congruent 462. 0.765 ## 6 14002 incongruent 536. 0.463 Ultimately, we want to have one row per subject and to calculate the difference in mean RT between incongruent and congruent conditions. It is easier to calculate the difference between two values when they are in the same row. Currently, the mean RT for each condition is on a different row. What we need to do is reshape the data frame. To do so we will use the pivot_wider() function from the tidyr package. The tidyr package, like readr and dplyr, is from the tidyverse set of packages. The pivot_wider() function will convert a long data frame to a wide data frame. In other words, it will spread values on different rows across different columns. In our example, what we want to do is pivot_wider() the mean RT values for the two conditions across different columns. So we will end up with is one row per subject and one column for each condition. Rather than incongruent, and congruent trials being represented down rows we are spreading them across columns (widening the data frame). The three main arguments to specify in pivot_wider() are id_cols: The column names that uniquely identifies (e.g. “Subject”) each observation and that you want to be retained when reshaping the data frame. names_from: The column name that contains the variables to create new columns by (e.g. “Condition”). The values in this column will become Column names in the wider data format values_from: The column name that contains the values (e.g. “RT”). data_wide &lt;- pivot_wider(data, id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = &quot;RT.mean&quot;) Now our data frame looks like head(data_wide) ## # A tibble: 6 x 3 ## Subject congruent incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. ## 2 14001 392. 423. ## 3 14002 462. 536. ## 4 14003 567. 679. ## 5 14004 548. 655. ## 6 14005 472. 559. Notice that the ACC.mean column and values were dropped. To add more transparency to our data frame it would be a good idea to label what values the “congruent” and “incongruent” columns contain. You can do this with the optional names_prefix argument. For instance: data_wide &lt;- pivot_wider(data, id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = &quot;RT.mean&quot;, names_prefix = &quot;RT_&quot;) head(data_wide) ## # A tibble: 6 x 3 ## Subject RT_congruent RT_incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. ## 2 14001 392. 423. ## 3 14002 462. 536. ## 4 14003 567. 679. ## 5 14004 548. 655. ## 6 14005 472. 559. Now a stranger (or a future YOU) will be able to look at this data frame and immediately know that reaction time values are contained in these columns. From here it is pretty easy, we just need to create a new column that is the difference between incongruent and congruent columns. We can use the mutate() function to do this data_wide &lt;- mutate(data_wide, FlankerEffect_RT = RT_incongruent - RT_congruent) head(data_wide) ## # A tibble: 6 x 4 ## Subject RT_congruent RT_incongruent FlankerEffect_RT ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. 109. ## 2 14001 392. 423. 31.3 ## 3 14002 462. 536. 74.0 ## 4 14003 567. 679. 113. ## 5 14004 548. 655. 107. ## 6 14005 472. 559. 87.1 Perfect! Using the readr, dplyr, and tidyr packages we have gone from a “tidy” raw data file to a data frame with one row per subject and a column of FlankerEffect scores. What if we have multiple columns we want to get id_cols, names_from, or values_from? pivot_wider() allows for this very easily. For instance: data_wide &lt;- pivot_wider(data, id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = c(&quot;RT.mean&quot;, &quot;ACC.mean&quot;)) head(data_wide) ## # A tibble: 6 x 5 ## Subject RT.mean_congruent RT.mean_incongruent ACC.mean_congruent ACC.mean_incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14000 401. 510. 0.931 0.574 ## 2 14001 392. 423. 0.980 0.852 ## 3 14002 462. 536. 0.765 0.463 ## 4 14003 567. 679. 0.294 0.0926 ## 5 14004 548. 655. 0.5 0.0370 ## 6 14005 472. 559. 0.686 0.204 Now you can see that we have four columns corresponding to reaction times and accuracy values across the two conditions. You can use the same notation c() if you want to use multiple column for id_cols, names_from, values_from. Now we can calculate a FlankerEffect for both RT and Accuracy values data_wide &lt;- mutate(data_wide, FlankerEffect_RT = RT.mean_incongruent - RT.mean_congruent, FlankerEffect_ACC = ACC.mean_incongruent - ACC.mean_congruent) 5.11 pivot_longer() For our goal with this data set, we do not need to switch back to a longer data format, however reshaping your data to a longer format may be something you want to do one day. Let’s try to reshape the data_wide back to a long format that we originally started with. When you have multiple value columns this is not as intuitive as pivot_wider(). To see more documentation and examples use ?tidyr::pivot_longer(). data_long &lt;- pivot_longer(data_wide, contains(&quot;mean&quot;), names_to = c(&quot;.value&quot;, &quot;Condition&quot;), names_sep = &quot;_&quot;) 5.12 Pipe Operator %&gt;% One last thing about the dplyr package. dplyr allows for passing the output from one function to another using what is called a pipe operator. The pipe operator is: %&gt;% This makes code more concise, easier to read, and easier to edit. When you pass the output of one function to another with %&gt;% you do not need to specify the data frame (input) on the next function. %&gt;% implies that the input is the output from the previous function, so this is made implicit. We can pipe all the functions in the chapter together as such ## Setup library(readr) library(dplyr) library(tidyr) ## Import import &lt;- read_csv(&quot;Data Files/tidyverse_example.csv&quot;) ## Score data &lt;- import %&gt;% rename(Accuracy = ACC) %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% select(Subject, Condition, RT, Trial, ACC = Accuracy) %&gt;% group_by(Subject, Condition) %&gt;% mutate(RT = ifelse(RT&lt;200, NA, RT), grandRT = mean(RT, na.rm=TRUE), RTdiff = RT - grandRT) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), ACC.mean = mean(ACC, na.rm = TRUE)) %&gt;% ungroup() %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;Condition&quot;, values_from = c(&quot;RT.mean&quot;, &quot;ACC.mean&quot;)) %&gt;% mutate(FlankerEffect_RT = RT.mean_incongruent - RT.mean_congruent, FlankerEffect_ACC = ACC.mean_incongruent - ACC.mean_congruent) Virtually all the R scripts you write will require the dplyr package. The more you know what it can do, the easier it will be for you to write R Scripts. I highly suggest checking out these introductions to dplyr. https://dplyr.tidyverse.org https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html "],
["common-data-manipulations.html", "6 Common Data Manipulations 6.1 Descriptive Statistics 6.2 Centering and Standardizing Variables 6.3 Trimming 6.4 Composites 6.5 Scale Transformations 6.6 Custom Transformations 6.7 Row-wise Computations 6.8 Remove subjects with too many missing values", " 6 Common Data Manipulations In R, the term data wrangling is often times used to refer to performing data manipulation and transformations. Some of the functions you will learn about in this Chapter come from the datawrangling package I developed. There are certain data transformations we use on a regular basis that would require several steps and lines of code to do. datawrangling allows you to perform these transformation in a single line of code. I am hosting the datawrangling package on GitHub. To download packages on GitHub you first need to download the devtools package. install.packages(&quot;devtools&quot;) Now install the datawrangling package: devtools::install_github(&quot;dr-JT/datawrangling&quot;) Save a new R script file as 6_transform.R For this Chapter, let’s create a data frame to use as an example for common data manipulations using datawrangling. Don’t worry about what this code means for now, just copy it into your script and run it. import &lt;- data.frame(ID = c(1:100), Group = rep(c(1:2), 50), Score1 = rnorm(100, mean = 2, sd = .8), Score2 = rnorm(100, mean = 7, sd = 1.1), Score3 = rnorm(100, mean = 10, sd = 1.8), Score4 = rnorm(100, mean = 20, sd = 2.3)) head(import) ## ID Group Score1 Score2 Score3 Score4 ## 1 1 1 2.108583 6.909211 9.569533 21.10629 ## 2 2 2 1.729564 6.348592 9.488073 17.62381 ## 3 3 1 1.168912 8.357614 12.028537 21.40660 ## 4 4 2 1.612341 5.694310 11.830850 22.75000 ## 5 5 1 3.006348 6.261088 14.137273 20.82632 ## 6 6 2 2.751853 6.430017 12.013493 21.24679 6.1 Descriptive Statistics First you should know how to compute some basic descriptive statistics. Basic descriptive statistics include mean, median, standard deviation, max, min, skew, kurtosis, etc… The functions to calculate these are pretty straightforward: Base R maximum: max() minimum: min() count:n() mean: mean() median: median() standard deviation: sd() variance: var() quantiles (percentiles): quantile() specify the percentiles with the argument probs = (default is c(0, .25, .5, .75, 1)) From the e1071 package skewness: skewness(variable, na.rm = TRUE, type = 2) kurtosis: kurtosis(variable, na.rm = TRUE, type = 2) For all of these you need to specify na.rm = TRUE if the variable column has missing data. It is best to just always set na.rm = TRUE. For example, mean(variable, na.rm = TRUE) To calculate the overall mean on Score1 would look like library(dplyr) data &lt;- import %&gt;% mutate(Score1.mean = mean(Score1, na.rm = TRUE)) 6.2 Centering and Standardizing Variables The function datawrangling::center() will create either unstandardized or standardized (z-scored) centered variables. The list of arguments that can be passed onto the function are: x: data frame variables: c() of columns to center standardize: Logical. Do you want to calculate zscores? (Default = FALSE) Example: library(datawrangling) data &lt;- center(import, variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), standardize = TRUE) View the data frame data. You will notice that there are now 4 additional columns: Score1_z, Score2_z, Score3_z, and Score4_z. If you choose to to calculate centered (unstandardized) scores, then standardize = FALSE. And it will create variables with the suffix _c. This can be combined with a group_by() to calculate standardized values within each group separately. data &lt;- import %&gt;% group_by(Group) %&gt;% center(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), standardize = TRUE) %&gt;% ungroup() 6.3 Trimming The function datawrangling::trim() will replace outlier scores that exceed a certain z-score cutoff. There are several options for how to replace the outlier scores. Replace with “NA” (missing value) “cutoff” (the z-score cutoff value, e.g. 3.5 SDs) “mean” “median” The arguments that can be specified are: x: data frame variables: c() of variables to be trimmed. option to set variables = \"all\" to trim all variables in a data frame. But then must specify id = cutoff: z-score cutoff to use for trimming (default: 3.5) replace: What value should the outlier values be replaced with. (default: replace = “NA”) id: Column name that contains subject IDs. **ONLY needs to be used if variables = \"all\" Example: data &lt;- import %&gt;% trim(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;), cutoff = 3.5, replace = &quot;NA&quot;, id = &quot;ID&quot;) Notice how you don’t even need to center() the variables first. The centering is being done inside of trim(). You can evaluate outliers and replace with different values (replace =) all in one function and one line of code. 6.4 Composites The datawrangling::composite() function allows you to easily create a composite score from multiple variables and also specify a certain criteria for how many missing values are allowed. data &lt;- import %&gt;% composite(variables = c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;), name = &quot;Score_comp&quot;, type = &quot;mean&quot;, standardize = TRUE, missing.allowed = 1) The function composite() will create composite scores out of specified columns. Right now you can only create “mean” composite scores. In the future I plan on adding “sum” and “factor score” composite types. Here is a list of the arguments you can specify: x: data frame variables: c() of columns to create the composite from name: Name of the new composite variable to be created type: What type of composite should be calculated?, i.e. mean or sum. (Default = “mean”). standardize: Logical. Do you want to calculate the composite based on standardized (z-score) values? (Default = TRUE) missing.allowed: Criteria for the number of variables that can having missing values and still calculate a composite for that subject The remaining functions do not come from the datawrangling package but you may find them useful nonetheless. 6.5 Scale Transformations 6.5.1 polynomial You can create orthogonal polynomials of variables using the poly() function and specify the degree of polynomial to go up to with degree = poly(import$Score1, degree = 3) You can see it creates up to three degrees of polynomials on the Score1 variable. The first degree is a linear, second is a quadratic, and third is cubic. Let’s say we want to create three new columns with each of these three polynomials. To do so we need to individually access each vector such as poly(import$Score1, degree = 3)[,1] library(dplyr) data &lt;- import %&gt;% mutate(Score1.linear = poly(Score1, degree = 3)[ , 1], Score1.quadratic = poly(Score1, degree = 3)[ , 2], Score1.cubic = poly(Score1, degree = 3)[ , 3]) Here is plot to show you visually what happened 6.6 Custom Transformations In general, with mutate() you can specify any custom transformation you want to do on a variable. For instance, if you want to subtract each score by 5, and divide by 10 then you can do it! I don’t know why you would ever want to do that, but you can. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 - 5) / 10) Or take the sum of Score1 and Score2 and divide by the difference between Score3 and Score4. library(dplyr) data &lt;- import %&gt;% mutate(Score_crazy = (Score1 + Score2) / (Score3 - Score4)) 6.7 Row-wise Computations The examples for Custom Transformations allowed you to calculate a new variable across multiple columns and within a row. For those examples that was no problem. However, sometimes you want to do more complicated computations or use certain functions such as mean() to calculate the average score across multiple columns. The problem is that mean() works across rows and only within one column. How can we get around that? In the near future (May 15th, 2020), dplyr version 1.0.0 will be released and includes a new function rowwise() for this very purpose. Until then we can use a base R function rowMeans() and rowSums(). data &lt;- import %&gt;% mutate(score_mean = rowMeans(data[ ,c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;)])) 6.8 Remove subjects with too many missing values In our lab, when doing latent variable analyses we often times like to remove subjects that have too many missing values on any given latent factor. For instance, we often times have three task indicators for a latent factor. If a subject has missing values for 2 out of 3 of those task indicators (therefore only has one indicator) then we will remove them. But how do go about doing this in R? First let’s modify the import data frame to be more suited for this by adding some missing values import &lt;- import %&gt;% mutate(Score1 = ifelse(Score1 &lt; 1, NA, Score1), Score2 = ifelse(Score2 &lt; 6, NA, Score2), Score3 = ifelse(Score3 &lt; 9, NA, Score3), Score4 = ifelse(Score4 &lt; 16, NA, Score4), Score5 = rnorm(100, mean = 13, sd = 5)) View the data frame and notice how there are quite a few missing values. Some subjects have missing values on multiple columns. The steps to get rid of subjects with too many missing values on a select number of columns are: Create a column with a count of how many columns has missing values for each subject (a row-wise computation) Filter subjects that have a certain criteria of missing values Just to get a sense of how these steps work we can do this in multiple ways. First let’s calculate the total number of missing values across all columns and remove subjects that have missing values on 3 of the columns. data &lt;- import %&gt;% mutate(missing.total = rowSums(is.na(import[,c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;, &quot;Score5&quot;)]))) View the data frame and notice that some subjects have 0, 1, 2, or 3 columns with missing values. unique(data$missing.total) ## [1] 0 1 2 3 Now let’s remove those with 3 or more columns missing. What we will actually specify is to keep those with less than 3 missing columns data &lt;- import %&gt;% mutate(missing.total = rowSums(is.na(import[,c(&quot;Score1&quot;, &quot;Score2&quot;, &quot;Score3&quot;, &quot;Score4&quot;, &quot;Score5&quot;)]))) %&gt;% filter(missing.total &lt; 3) Compare the number of rows between import and data nrow(import) ## [1] 100 nrow(data) ## [1] 98 Two subjects were removed. Now let’s calculate the total number of missing values for two different latent factors. Let’s say latent factor 1, latent1, includes the column indicators Score1 and Score2, whereas latent factor 2, latent2, includes the column indicators Score3, Score4, and Score5. Let’s remove subjects that have missing values on 1 out of the 2 columns for latent1 and 2 out of 3 for latent2. data &lt;- import %&gt;% mutate(missing.1 = rowSums(is.na(import[,c(&quot;Score1&quot;, &quot;Score2&quot;)])), missing.2 = rowSums(is.na(import[,c(&quot;Score3&quot;, &quot;Score4&quot;, &quot;Score5&quot;)]))) %&gt;% filter(missing.1 &lt; 1, missing.2 &lt; 2) View and evaluate data. Also compare the number of rows nrow(import) ## [1] 100 nrow(data) ## [1] 70 There are now only 70 subjects, we removed 30 total subjects! Yikes. "],
["reproducible-workflows-overview.html", "Reproducible Workflows: Overview", " Reproducible Workflows: Overview As you start becoming more proficient in R you will be able manage, process, and analyze your data at all stages of analysis through R scripts alone. Goodbye Excel, goodbye SPSS, and good riddance to EQS! Although we deal with data all the time as scientists, we have never really been educated on what good data science practices look like. In psychology, I believe this is partly due to our reliance on programs like SPSS or Excel for data analysis and visualization. While these programs offer a nice user interface, they do not offer any tools to help us manage and process our data in a reproducible way. Open Science and Reproducibility practices are very quickly becoming the norm in most fields of science. Programming languages like R are perfectly suited to help us implement these practices. Reliance on programs like SPSS and Excel will hinder your ability to join the Open Science and Reproducibility movement. However, simply learning the R syntax alone is not enough. You will need to start thinking about what are good data science practices that allow me to manage, process, and analyze my data in a way that is consistent with Open Science and Reproducibility? I think it is useful to start thinking and implementing these practices from the get go. You could go through a lot of trial and error (which is what I have done) but why not just build on what others have learned. This section is divided into 3 Chapters: Project Organization I will start at a higher level and consider how we organize our research projects from the data collection phase to the data analysis phase Reproducible Practices I will then talk about how we need to think about interacting with the R Environment, R Scripts, and the data files on our computer in order to not undermine reproducibility. R Script Templates The script workflow is more or less the same from one task and research project to another. I have developed generic script templates that you should use. This will help you more quickly start using R for your research projects. The purpose of this section is to get you thinking about these topics now and introduce you to ways of automating them so that you do not have to think about them later! "],
["project-organization.html", "7 Project Organization 7.1 Data Collection 7.2 Data Preparation 7.3 Data Analysis 7.4 workflow package", " 7 Project Organization In the EngleLab, we often conduct large-scale data collection studies in which there are many research projects going on at once. Although many of the tasks will be shared between these research projects, there will be unique tasks for each project. This overlap in shared tasks can make it difficult to figure out how and at what stage to separate data preparation and analysis between these projects. I will suggest a particular organizational method to deal with this issue. This method will also make it easier in the future to go back and work with unanalyzed or archival data. The basic idea is that you have a data collection directory for a study - where data collection happens. That directory will store: Raw data files R Scripts To convert messy raw data files to tidy raw data files The programmed tasks administered to subjects Any documents related to the study Methods document, Consent form, demographic forms, etc. Essentially the initial directory used for data collection will become an archival directory you can use to grab raw data files to start an analysis project. From this data collection directory you can create separate directories for different data analysis projects. To do so, and this is the key part, you only need to copy and paste the tidy raw data files. (The tidy raw data files are created during the data preparation stage and I will discuss this more below.) An advantage to this is that for each data analysis project, you will be able to FULLY reproduce your data processing and analyses independently of other data analysis projects. (This is very different from how we have done it in the past. In the past, we have typically shared a single “final” data file that contains scores for all the tasks. The problem with this is you have no idea how those scores were created, how the data were cleaned, you cannot calculate reliability or look at trial level data, and you cannot go back and re-analyze the data with different scoring or cleaning methods.) It is critical that you copy the tidy raw data files ONLY from the data collection directory to a data analysis directory You do not copy data files from one data analysis directory to another! You CAN copy R Scripts from one data analysis directory to another. For instance, if you or someone else already created an R Script (for a separate data analysis project) to score a particular task that you are also using, then there is no problem in copying the R Script. In general, you will probably copy R scripts from one project to another. The important point is that you are not copying data files from one project to another! This is because you can reproduce the data files from your R Scripts but not vice-versa. If you copy data files you lose reproducibility and the transparency of how the data file was created. 7.1 Data Collection Unfortunately, you have to actually collect data before you can start analyzing anything. Therefore, you start out with a single directory: Data Collection. At the start all the folders in this directory that you really need are: Tasks Documents The Tasks folder is where the E-Prime task files are that will be used to administer each task to the subjects. In our lab we typically have multiple Sessions and multiple Tasks in each session. As you begin data collection, .edat data files will start to accumulate in each Task folder. Documents is where you may store various documents related to the study, such as a Methods.docx document describing each task in detail. This is an important document for archival purposes. Some of your other documents in this directory may not be as important for archival purposes, such as an informed consent form. 7.2 Data Preparation At some point you will need to start analyzing the data. However, you first need to prepare the data so that it is ready to analyze. There are several steps in this process and it can be quite tedious. Nevertheless, undergraduate RAs are trained on how to do most of these steps, so recruit their help. There are also step-by-step instructions for Data Preparation. Once you are ready for Data Preparation you will need to create a Data Files and R Scripts folders in the Data Collection directory. There are two scenarios in which you may need to start processing and analyzing data: Before data collection has finished After data collection has finished For both of these scenarios, you will start with messy raw data files in some file format. Messy raw data files are hard to understand, have poor column and value labels, contain way too many columns and rows, and are just hard to work with. Data preparation is all about getting raw data files that are easy to work with. The end product of the data preparation stage is tidy raw data files. Tidy raw data files are easy to understand, have sensible column and value labels, contain only relevant columns and rows, and are very easy to work with. 7.3 Data Analysis Okay, now say you are ready to analyze some data! It is tempting to do your analysis in the original Data Collection directory where the data are already stored. I highly suggest not doing this. You will be mixing up a Data Collection directory with a Data Analysis directory. This distinction is particularly important when we conduct large-scale studies with many data analysis projects for a single data collection study. Instead, you should copy over the tidy raw data files from the data preparation stage to a separate Data Analysis directory. You also might as well create an Archival Backup of the Data Collection directory on some other hard drive. That way you are at less risk of a hard drive crashing and losing all your precious data. In the Data Analysis directory you have three main folders: Data Files R Scripts Results Data Files is where you will store tidy raw data files, scored data files, and a single merged data file ready for statistical analysis. It is advisable to store ALL your R scripts in one single place. I also like to prefix them with numbers corresponding to the order they need to be executed - that way they will be organized in an easy to find way. Finally, you should create a separate folder to hold all your outputs from statistical analysis and data visualization in a Results folder. The overall workflow for data analysis looks like: We will get into more of the details in the next section. You may have other folders in your Data Analysis directory: Figures Manuscript Presentations Figures is where any image files, that are used in a manuscript or presentations, are stored. You may also have a PowerPoint file stored here. Manuscript is where the manuscript and any drafts for this project are stored. Presentations is where any PowerPoint presentation files related to this project can be stored. These other folders are more optional. 7.4 workflow package I will show you how to automatically create Data Collection and Data Analysis directories using RStudio Projects and my workflow package The most important thing to remember is that you need to copy and paste the tidy task_raw.csv data files from a Data Collection directory to a Data Analysis directory - but never copy and paste data files from one Data Analysis directory to another. When should you create a separate Data Analysis directory? Basically, if the set of analyses is going to be it’s own Manuscript then create a new directory. If the set of analyses (whether exploratory or supplemental) is part of a larger set of analyses already in the works for a manuscript then no need to create a separate directory. 7.4.1 Install Install the workflow package devtools::install_github(&quot;dr-JT/workflow&quot;) 7.4.2 Create a New R Project One of the features this package allows is for you to automatically setup the organization of a Data Collection or Data Analysis project. Navigate to __File -&gt; New Project… -&gt; New Directory And browse until you see the option: Research Study Click on that and you will see a dialogue box like this Here are what the different options mean: Directory Name: This will be the name of the folder for the study Create project as subdirecotry of: Select Browse and choose where the folder (Directory Name) should be located. Repository Type: data collection or data analysis. Depending on which one you choose it will create the corresponding directories and files: Notice that if you choose the data collection repository it will download a generic template for converting “messy” raw data files to “tidy” raw data files. And if you choose the data analysis repository it will download generic templates for creating scored data files from “tidy” raw data files and to merge the Scored data files into one final data file. # of Sessions: How many sessions will the study have? This will create folders in the Tasks folder for each session. For instance, if there will be 4 sessions it will create the the folders “Session 1”, “Session 2”, “Session 3”, and “Session 4”. Obviously this is not needed for a data analysis repository. Other Directories: I talked earlier about some other folders you may want to include in a Data Analysis repository. Well you can automatically add them here. Go ahead and play around with creating different types of repositories. "],
["reproducible-practices.html", "8 Reproducible Practices 8.1 What is real? 8.2 Where does your analysis live? 8.3 Environment", " 8 Reproducible Practices An advantage of using R is learning how to manage and handle your data processing workflow in a way that empowers your ability to analyze and explore your data. If you treat R as just an alternative to SPSS, then it is all too easy to create poorly written scripts and disorganized projects that completely undermine reproducibility. Honestly, I am not sure if it is worth taking the time and effort to learn R as simply an alternative to SPSS. In this chapter I will talk about how we need to think about interacting with the R Environment, R Scripts, and the data files on our computer in order to not undermine reproducibility. Parts of this next section are taken directly from the excellent book on Data Science in R R for Data Science One day you will need to quit R, go do something else and return to your analysis the next day. One day you will be working on multiple analyses and/or projects simultaneously that all use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 8.1 What is real? As a beginner R user, it is tempting to consider whatever is in your Environment as “real” (i.e. data we have imported and objects we have created with &lt;-). However, we should consider our R Scripts and Data Files saved on our computer as real. With your R scripts (and your data files), you can recreate the Environment. It’s much harder to recreate R scripts from your Environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your R history. Think of the Environment as more of a temporary workspace. This workspace will get cleared out every time you Restart or Quit out of R and RStudio. If you treat your Environment as real this can have disastrous consequences and you can lose a lot of productivity and undermine your reproducibility. Your R Scripts and Data Files are real Even if you need to Quit R, come back to an analysis the next day, or want to run your analysis on a different computer than you started with, you should be able to reproduce your analyses. You should be able to Reproduce all your analyses from your saved R scripts and original data files. 8.2 Where does your analysis live? “One day you will be working on multiple analyses and/or projects simultaneously that all use R and you want to keep them separate” There are two levels at which your analysis will live Project level Individual R Script level 8.2.1 Project Level The Project Level refers to where you are storing all your files associated with the project (i.e. R scripts, data files, figures, results) as well as to the organization of your folders and files. I talked about Project Organization in the previous chapter but now I want to talk about why using RStudio Projects is a good idea. If you are working on multiple projects at one time, then it is vital to: Keep your analysis from different projects separated You do not want objects created in your Environment from one project to get mixed up with objects (perhaps with the same object names) in a different project. We have not talked about the concept of Working Directories yet, but you also need to ensure your working directory is correctly set in order to import and output files. If you are working on multiple projects at one time, not keeping them separated will create issues Reproducing your analysis because the working directory might not be set correctly. The three most important elements to an environment are: The working directory Loaded packages Objects The best way to keep project Environments separated is to work on them in separate R Sessions. You can have multiple Sessions of R open at one time. The three elements of an Environment in one R Session will be different and independent from those in another R Session. RStudio has an excellent way of managing separate projects with a feature called RStudio Projects. Use RStudio Projects to create separate Environments for your projects See below for more details on RStudio Projects. 8.2.2 Script Level Within a Project, you will have multiple R Scripts that are performing a different analysis. It is also important to keep the Environment of an R Script independent of the Environment from other R Scripts, even within the same project. Obviously your R Scripts will be dependent on one another in the sense that one R Script might be creating data files that other R Scripts will later use. For example, you might have an R Script (or multiple scripts) that prepare the data for statistical analysis by creating a scored merged data file. Then another R Script will actually run and output the statistical analyses. The statistical analysis R Script is dependent on a data file created by the first R Script. However, the Environment of the R Scripts are independent from one another. R Scripts are linked in a data processing workflow through the data files they create not by the objects in the Environment Therefore, it is important that the Environment of each R Script within a Project are independent from one another. This means that an R Script should not depend on objects created or data imported in other R Scripts. Any packages required for an R Script should be loaded in THAT script and not depend on them being loaded in other R Scripts. As long as the data files required for an R Script are already created, you should be able to open a completely new and fresh session of R and successfully execute all the lines of code in that one Script (without having ran any other scripts). At the end of each R Script I had you include the line of code rm(list = ls()) This removes all Objects in your Environment. That way when you run the next R Script it is starting from a blank Environment Some people in the R community say this is not a good practice, but in reality it is not harmful to add this line of code. 8.3 Environment Because we should treat our Environment as a temporary workspace we need to make sure that our Environments are as independent as possible from one another, at both the Project and Scripts Levels. I mentioned above that the three most important elements to an environment are: The working directory Loaded packages Objects 8.3.1 Working Directory You could use what are referred to as absolute file paths to import and export files but this is not good practice. The reason is that the absolute file path is specific to a particular computer. No one computer is going to have the same absolute file path. An absolute file path starts from the root directory on your computer and may look something like: Mac: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial Windows: C:\\Users\\jasontsukahara\\Dropbox (GaTech)\\My Work\\Coding Projects\\R\\R-Tutorial You do not want to write scripts that can only work on a specific computer! One of the great advantages to programming for data processing is reproducibility. You and your future self (and other researchers) can reproduce your exact same data processing steps. If you use absolute file paths you are undermining the reproducibility of your scripts. It is good practice to use relative file paths instead. Relative file paths start from a working directory. Let’s say you have a working directory set to the following location: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study And you want to import files from a Raw Data directory within Cool Study. The absolute path to raw data files in Cool Study might look like: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study/Data Files/Raw Data Whereas a relative file path from the working directory would be: Data Files/Raw Data You can see that with relative file paths, only the internal organization of the project directory matters. This allows your script to be ran on different computers, systems, and environments! Your working directory can be specified in various ways. By default, if you open up RStudio directly it will set the working directory to some default location such as your User root directory. This is not good. If you open up RStudio by directly opening an R script file then it will set the working directory to the location of that file. This is better but still not ideal. Because the working directory changes depending on how you open RStudio a lot of R Users will set a working directory at the top of their script using setwd() However, this is not good because it requires the use of an absolute file path. Ideally, we should not even have to think about the working directory since it can change depending on how R is opened. Thankfully there is a solution to make our dreams come true! 8.3.1.1 RStudio Projects and here::here() Visit this page for more details on R Projects. Using RStudio Projects, helps keep your scripts and Environments from one project to the next separate from each other. RStudio Projects allow you to open a fresh session of R that automatically sets the working directory to the location where the R Project is saved. R Projects have the file extension .Rproj. Save .Rproj to your project’s root directory There are a couple of ways you can open an RStudio Project. One way is to just simply open the .Rproj file. This will open a new R Session (and RStudio window). If you already have an RStudio window open you can navigate to the very top-right of the application window and browse different projects you have recently worked on. This is where you can also see which Project you currently have open. The here package in combination with RStudio Projects allows you to not even have to think about working directories. For a passionate ode to the here package see: https://github.com/jennybc/here_here First go ahead and install the here package, install.packages(\"here\"). Basically, when the here package is loaded, library(here), it will search for an .Rproj file and it will set a starting file path at that location when you use here(). (It is not technically changing the working directory though but that doesn’t matter when using here()) For instance, I have an .Rproj file saved in my “UseR_Guide” folder. When I use here() it will output a file path to that location. library(here) here() ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding/R/R Books/UseR_Guide&quot; You can then use here() to set a relative file path here(&quot;Data Files/Raw Data/flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding/R/R Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; This is equivalent to here(&quot;Data Files&quot;, &quot;Raw Data&quot;, &quot;flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding/R/R Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; I typically like to set the first argument as the relative file path and the second argument as the file name here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding/R/R Books/UseR_Guide/Data Files/Raw Data/flanker_raw.csv&quot; This visually separates the file path and the file name, making your script easier to read. You can use here() directly in import and output functions: import &lt;- read_csv(here(&quot;Data Files/Raw Data&quot;, &quot;flanker_raw.csv&quot;)) write_csv(data, here(&quot;Data Files/Scored Data&quot;, &quot;flanker_scored.csv&quot;)) And you know that everytime you use here() that the file path will start at where you have your .Rproj file saved. Instead of messing around with working directories with setwd() or getwd(), just use here() and RStudio Projects. This becomes especially helpful when working with RMarkdown documents. Avoid using setwd() by using RStudio Projects and here::here() 8.3.2 Loaded Packages To make sure that the packages used in one R Script do not depend on the packages loaded in other R Scripts: Load all required packages for that one R Script at the top of the script This allows you to easily evaluate which packages are required for that script. 8.3.3 Objects To make sure that the objects and functions created in one R Script do not depend on the objects and functions created in other R Scripts: Include the following line of code at the bottom of the script rm(list=ls()) This will remove any objects and functions in the current environment. That way when the next R Script is ran, there will be no leftover objects or functions from previous R Scripts. "],
["r-script-templates.html", "9 R Script Templates 9.1 Install workflow package 9.2 Data Analysis", " 9 R Script Templates One of the nice features of using R scripts to analyze your data is that you can use a lot of the same code from one step or one project to another. For each of these steps, the script organization from one task, analysis, or project to the next will be more or less the same. This means we can take advantage of using R script templates. Using R Script templates will not just save time but help you stay organized and use reproducibility practices talked about in the previous chapters. 9.1 Install workflow package devtools::install_github(&quot;dr-JT/workflow&quot;) I have created some templates you can easily download using a single R function workflow::template() There are different types of templates you can download using workflow::template(). You can always use ?workflow::template to see help documentation. 9.1.1 Data Preparation Data Preparation will occur in a Data Collection directory, separately from Data Analysis. It will have its own .RProj and mastercript.R files. The main purpose of data preparation is converting messy raw data files to tidy raw data files. 9.1.2 messy-to-tidy script If you want to download a template script for the step of converting messy raw data files to tidy raw data files during data preparation you can simply specify: workflow::template(rawscript = TRUE) Let us take a look at what the rawscript template looks like #### Setup #### ## Load packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Merged&quot; output_dir &lt;- &quot;Data Files&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import_file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ################ #### Import #### data_import &lt;- read_delim(here(import_dir, import_file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE, guess_max = 10000) ################ #### Tidy raw data #### data_raw &lt;- data_import %&gt;% filter() %&gt;% rename() %&gt;% mutate() %&gt;% select() ####################### #### Output #### write_csv(data_raw, here(output_dir, output_file)) ################ rm(list=ls()) There are 4 main blocks of R code: Set up the script Load required packages using library() Set the import/output directories and filenames Doing these steps at the top of the script makes it obvious, without having to read the rest of the script, what packages the script will require and what data file it is importing and outputing. Import a data file using read_delim() from the readr package Tidy the imported data frame using dplyr functions, such as filter(), rename(), mutate(), case_when(), and select() Output a “tidy” data file using write_csv() from readr Last, remove all objects from the environment with rm(list=ls()). To me this template is beautiful. The only thing you need to change is task &lt;- \"taskname\" to the name of the task used in the file name Fill in what happens in the Tidy raw data block. The rest can LITERALLY stay the same. How easy! 9.1.3 masterscript: Data Preparation In the EngleLab, we often have up to 40 tasks for a single Data Collection study. When you have a lot of R Scripts it is quite tedious to open, Source, and exit, each R script one at a time. The masterscript allows you to Source all of your scripts at once. The source() function is a way to execute all the lines of code in a script file. Rather than having to manually open each script file and sourcing it from there you can control your entire data processing workflow from the masterscript using source(). The argument echo = TRUE will print the results of the script to the console that way you can still see what the script is doing. This allows you to run each script from the masterscript and control the order in which you run them. For Data Preparation the order does not really matter, but once you get to Data Analysis the order is crucial. The masterscript template for Data Preparation can be downloaded with: workflow::template(masterscript = &quot;data preparation&quot;) This masterscript is simple: ## Data Preparation for Study Name ############################################# #------ 0. &quot;messy&quot; to &quot;tidy&quot; raw data ------# ############################################# source(&quot;R Scripts/0_taskname_raw.R&quot;, echo=TRUE) rm(list=ls()) ############################################# 9.2 Data Analysis Data Analysis will occur in it’s own directory separate from the Data Collection directory and separate from Data Preparation. The tidy raw data files created during Data Preparation will be copy and pasted over to Data Files/Raw Data in the Data Analysis directory Typically there are at least two steps that need to be taken in order to create a final data file that is ready for statistical analysis. Clean and Score the data from each task by aggregating performance over trials and removing any outlier trials or problematic/poor performing subjects. Merge the scored data file from each task into one final data file ready for statistical analysis. There may be some additional steps that are required but these are the basic ones. 9.2.1 clean and score script If you want to download a template script for the step of cleaning and scoring a raw data file you can type in the console: workflow::template(scorescript = TRUE) Let us take a look at what the scorescript template looks like #### Setup #### ## Load Packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Raw Data&quot; output_dir &lt;- &quot;Data Files/Scored Data&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output_file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ## Set Data Cleaning Params ############### #### Import #### data_import &lt;- read_csv(here(import_dir, import_file)) ################ #### Data Cleaning and Scoring #### data_scores &lt;- data_import %&gt;% filter() %&gt;% group_by() %&gt;% summarise() ################################### #### Output #### write_csv(data_scores, here(output_dir, output_file)) ################ rm(list=ls()) Like the rawscript, there are 4 main blocks of R code: Set up the script Load required packages using library() Set the import/output directories and filenames Set Data Cleaning Params You can set some optional data cleaning parameters at the top of the script. This will be explained in more detail in the next section. Import a data file using read_csv() from the readr package Clean and Score the imported data frame using dplyr functions, such as filter(), mutate(), group_by(), and summarise() Output a data file with task Scores using write_csv() from readr Last, remove all objects from the environment with rm(list=ls()). The parts you may need to change are: What packages are loaded, library() task &lt;- “taskname” Optional data cleaning parameters in the “Set Data Cleaning Params” section The “Data Cleaning and Scoring” block 9.2.2 merge script To download a template script to merge several Scored data files together: workflow::template(mergescript = TRUE) The template looks like this: #### Set up #### ## Load packages library(here) library(datawrangling) # for files_join() and trim() library(dplyr) ## Set import/output directories import_dir &lt;- &quot;Data Files/Scored Data&quot; output_dir &lt;- &quot;Data Files&quot; output_file &lt;- &quot;name_of_datafile.csv&quot; ################ #### Import Files #### data_import &lt;- files_join(here(import_dir), pattern = &quot;Scores&quot;, id = &quot;Subject&quot;) ###################### #### Select only important variables #### data_merge &lt;- data_import %&gt;% select() ## Create list of final subjects subj.list &lt;- select(data_merge, Subject) ################################################################# #### Output #### write_csv(data_merge, here(output_dir, output_file)) write_csv(subj.list, here(output_dir, &quot;subjlist_final.csv&quot;)) ################ rm(list=ls()) Again, this template script has 4 main blocks: Set up the script Load required packages using library() Set the import/output directories Notice that in the setup section, the import and output filenames are not specified. The import file names are not specified because 1) we may need to import quite a lot of Scored data files and 2) I created a function to do this without having to specify each individual filename. You could add a section to include an output filename if you want. Import the data files with task scores. You can merge multiple files with the same rows (Subjects) and different columns (variables or task scores) using datawrangling::files_join(). This will be explained in more detail later on. Select only relevant variables. It is likely that the individual Scored data files will have way more columns of scores than you are interested in. Output a final data file that is ready for statistical analysis with write_csv() from readr Optional to also create a file with a list of all subjects that have made it through all these stages of processing. Last, remove all objects from the environment with rm(list=ls()). 9.2.3 masterscript: Data Analysis The masterscript template for Data Analysis can be downloaded with: workflow::template(masterscript = &quot;data analysis&quot;) ## Data Analysis for StudyName ################################################# #------ 1. &quot;tidy&quot; raw data to Scored data ------# ################################################# source(&quot;R Scripts/1_taskname_score.R&quot;, echo=TRUE) rm(list=ls()) ############################################################# #------ 2. Create Final Merged Data File for Analysis ------# ############################################################# source(&quot;R Scripts/2_merge.R&quot;, echo=TRUE) rm(list=ls()) ############################### #------ 3. Data Analysis ------# ############################### library(rmarkdown) render(&quot;R Scripts/3_MainAnalyses.Rmd&quot;, output_dir = &quot;Results&quot;, output_file = &quot;MainAnalyses.html&quot;) rm(list=ls()) ################################################# You can see that it is organized in the order in which the scripts need to be ran and how the scripts are named. The render(), function seen in the “3. Data Analysis” section, is how to knit an RMarkdown document. We will see later that this creates a flexible way to knit RMarkdown documents because you can specify the output file name and location, as well as certain parameters for what data set to import or analysis parameters to set. render() comes from the package rmarkdown which is why it is loaded at the top of the data analysis section. "],
["create-r-scripts-overview.html", "Create R Scripts: Overview 9.3 Importing and Outputting Data Files", " Create R Scripts: Overview The previous section was more of a high-level overview of the organizational structure and workflow of preparing and analyzing data in R. In this section we will dive into more of the details for actually creating R scripts at each of the stages of data preparation and analysis. You may wish to refer back to earlier sections for more details about certain functions and working with data in R. 9.3 Importing and Outputting Data Files If you are not familiar with coding, then you may have difficulty just importing or outputting a data file. This can become very annoying as it should be such a simple step and may discourage you a bit. Therefore, I want to go over this again. (Note I have covered this in Chapters 4 and 8). The first thing to get a hang of is how to specify the file path and file name. There are different ways of doing this, some good and some bad. A bad way of doing this is using what is called an absolute file path. The reason is that the absolute file path is specific to a particular computer. No one computer is going to have the same absolute file path. An absolute file path starts from the root directory on your computer and may look something like: Mac: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Coding Projects/R/R-Tutorial Windows: C:\\Users\\jasontsukahara\\Dropbox (GaTech)\\My Work\\Coding Projects\\R\\R-Tutorial You do not want to write scripts that can only work on a specific computer! One of the great advantages to programming for data processing is reproducibility. You and your future self (and other researchers) can reproduce your exact same data processing steps. If you use absolute file paths you are undermining the reproducibility of your scripts. It is good practice to use relative file paths instead. Relative file paths start from what is called a working directory. Let’s say you have a working directory set to the following location: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study And you want to import files from a Raw Data directory within Cool Study. The absolute path to raw data files in Cool Study might look like: ~/Users/jasontsukahara/Dropbox (GaTech)/My Work/Research Projects/Cool Study/Data Files/Raw Data Whereas a relative file path starting from the working directory would be: Data Files/Raw Data You can see that with relative file paths, only the internal organization of the project directory matters. This allows your script to be ran on different computers, systems, and environments! Then to import a file you would specify the file with it’s relative file path using some import function: some_import_function(&quot;Data Files/Raw Data/a_data_file.csv&quot;) 9.3.1 Setting a working directory When you open up RStudio a working directory is automatically set. You can evaluate what the current working directory is with getwd(). Alternatively, you can set the current working directory with setwd(). I highly suggest not to use setwd() because you will have to use an absolute file path to do so. We want to avoid absolute file paths at all costs. Fortunately, there is an elegant alternative that combines the use of .Rproj files and the here package. 9.3.2 RProject Files and here You need to be using RProjects for your projects. There should be a single .Rproj file in each data collection and data analysis directory. I discussed RProject files in more detail in Chapter 8. RProjects allow you to open isolated instances of R and RStudio for each of your projects. In combination with the here package it will also provide a simple and fool proof way of specifying file paths. The here package takes a bit of explaining. When you load the here package with library(here), it will start searching for a .Rproj file in the current directory the R Script is stored. If it does not find one, it will go up one directory and search there, and so on until it finds one. For instance, let’s say in my project directory I have the following file and folders: ProjectName.Rproj R Scripts Data Files If I load the here package in a script in the “R Scripts” folder it will look for a .Rproj file there. It will not find one and so it will go up one directory (to the projects root directory) and will find the file “ProjectName.Rproj”. This is important - Then it will set a file path starting at the projects root directory, where “ProjectName.Rproj” is located. Note that it is not setting the working directory, this is an important distinction. What it is doing is setting where a file path should start when you subsequently use the here() function. Therefore, you need to use here() function when specifying a file path and file name - here(\"filepath\", \"filename\"). When you load library(here), it will output the starting location in the R console. library(here) Now, when you use here() it will start at that location and you can just use a relative file path. For instance, in the project directory example above, if I run a script in the R Scripts folder that imports a data file located in “Data Files” I would specify: here(&quot;Data Files/a_data_file.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding/R/R Books/UseR_Guide/Data Files/a_data_file.csv&quot; Notice how the relative file path was appended to the start location set by library(here). If you were to leave the argument blank inside the function then you would just get the starting location. Anything you add inside the function appends to it. here() ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding/R/R Books/UseR_Guide&quot; I like to separate out the relative file path and the file name. I think it makes my code easier to read. For instance: here(&quot;Data Files&quot;, &quot;a_data_file.csv&quot;) ## [1] &quot;/Users/jtsukahara3/Dropbox (GaTech)/My Work/Coding/R/R Books/UseR_Guide/Data Files/a_data_file.csv&quot; Notice how I did not have to add the “/”. The here() function automatically adds “/” to arguments separated by commas. This way it is easy to read - on the left side you have the file path and on the right you have the file name. This is more evident when the file path is longer and involves more subfolders. Now you would simply include this in some import and output functions: some_import_function(here(&quot;Data Files&quot;, &quot;a_data_file.csv&quot;)) some_output_function(data, here(&quot;Data Files&quot;, &quot;a_new_data_file.csv&quot;)) No matter where your R script is saved (although you should have them all saved in one location) or where your working directory is set to - you can use here() starting at your projects root directory (as long as you have a .Rproj file saved there). You no longer have to think about working directories! Unless you have a spelling error in the file path or file name, or the file does not exist - importing and outputting files should just work. Simple and easy. "],
["data-preparation-setup.html", "10 Data Preparation: Setup 10.1 Project Organization 10.2 Setup the Data Collection Directory", " 10 Data Preparation: Setup 10.1 Project Organization In Chapter 7: Project Organization, we discussed how our projects should be organized. Essentially, there are two types of directories. The Data Collection directory, which is where data collection happened as well as data preparation, and Data Analysis directories, which is where the stage of data analysis will happen. These need to be kept separate from one another because in the EngleLab we often conduct these large-scale data collection studies that contain multiple research projects. Therefore, from the Data Collection directory we can create various Data Analysis directories for each separate research project. Once data collection is finished (or whenever Randy asks for a report on the data), we will need to start processing the raw data files. The first step in this processing is to convert the messy raw e-prime files into tidy (easy to read and work with) raw data files. This first step of data preparation happens in the Data Collection directory. 10.2 Setup the Data Collection Directory Typically the data collection directory will already have been setup. Here is an overview of how you would go about creating a data collection folder and R project. In Chapter 7, we covered how to setup data collection and data analysis repositories in more detail. Follow these steps to setup the data collection repository. Open RStudio File -&gt; New Project… New Directory -&gt; Research Study Browse to where the project folder will be located and specify the project folder name Choose Repository Type: data collection Select ‘Create Project’ Once you create the R Project, you will see the following directory setup in the “Files” window pane in RStudio. Let’s take a little time to understand this setup Data Files: This is where ALL the data files will be located. Within Data Files there are folders for Subject Files and Merged. The Subject Files are where the individual subject .edat files will be stored by task. The Merged folder is where E-Merged and exported .txt files will be stored. The .txt merged files are the “messy” raw data files we need to convert into “tidy” raw data files. Documents: We can put any documents here related to the study and analysis plan. We might like to put a methods document in here or any articles that are relevant to the tasks used in the study. R Scripts: ALL R scripts, with exception of the masterscript, will go in this folder. You should not put R scripts anywhere else! Scripts for converting messy raw data files to tidy raw data files will be labeled with the prefix: 0_ and contain the suffix: **_raw.R** (e.g. 0_antisaccade_raw.R). The zero denotes that this is an initial data preparation stage and does not necessarily need to be added to an open science repository (shared with other researchers). This is because we are not really changing the data with cleaning/trimming or scoring procedures yet. Just creating raw data files that are easier to work with. masterscript.R: Rather than opening each R script - running it - then closing out of it, opening the next script - running it - then closing out of it, and so on… you can run all of your R scripts in the masterscript. .Rproj: You only need to interact with this file when opening your R Project. When you want to work on your R project - YOU SHOULD ALWAYS OPEN RSTUDIO BY OPENING THE .Rproj FILE!!! "],
["data-preparation-messy-to-tidy.html", "11 Data Preparation: Messy to Tidy 11.1 Overview 11.2 Steps of Data Preparation 11.3 Raw script template 11.4 Setup 11.5 Import 11.6 Tidy raw data 11.7 Output 11.8 Final 11.9 masterscript.R", " 11 Data Preparation: Messy to Tidy 11.1 Overview Data Preparation is all about creating tidy raw data files from messy raw data files. This part is not always fun and it can be very tempting to skip and go straight to creating a scored data file that is ready for data analysis. However, I strongly advise against that. There are at least a few good reasons why: Sometimes you actually NEED the raw trial level data. For instance, to do reliability or internal consistency analyses. Visualizing and analyzing trial-level data can help you better understand your data. You or some other researcher might want to go back and run analyses starting from the trial-level data. Maybe you/they want to score the data slightly differently than you did before. Data storage. Storing and sharing your data in a tidy raw data format makes SO MUCH more sense than storing your messy raw data. If working with your messy raw data gives you a headache now imagine how much worse that is when you have not thought about that data for a long time. And many other reasons 11.2 Steps of Data Preparation Data preparation occurs both during and immediately after data collection. As such, the R scripts and resultant tidy raw data files are stored in the Data Collection directory. There are 4 data preparation steps: Organize the raw data files. This involves moving the raw .edat files from - Tasks/Session #/#. Task/ to - Data Files/Subject Files/Task/. Merge the individual .edat files into a single task.emrg file using E-Merge Export the task.emrg file to a task.txt file so we can process the data in R Source the 0_task_raw.R Scripts 0_task_raw.R imports a task.txt file and creates a tidy raw data file, task_raw.csv Step 4 is the only step that requires using R (R scripts can also be used to automate Step 1 but that will not be covered here). See Data Preparation Instructions for a detailed guide on how to perform the other steps. 11.3 Raw script template The R script template to convert messy raw data files to tidy raw data files was explained in Chapter 9. In this Chapter we will walk through a script to create a tidy raw data file from a messy raw data file from a visual arrays task. Feel free to follow along creating your own R script. This is the R script template downloaded by: workflow::template(type = \"raw\") #### Setup #### ## Load packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Merged&quot; output_dir &lt;- &quot;Data Files&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import_file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ################ #### Import #### data_import &lt;- read_delim(here(import_dir, import_file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ################ #### Tidy raw data #### data_raw &lt;- data_import %&gt;% rename() %&gt;% filter() %&gt;% mutate() %&gt;% select() ####################### #### Output #### write_csv(data_raw, here(output_dir, output_file)) ################ rm(list=ls()) 11.4 Setup Load packages Any packages required for this script are loaded at the top. For this task all we will need are the here, readr, and dplyr packages so we do not need to change anything. Set Import/Output Directories To make this example easier you will not have to actually import/output any files. Set Import/Output Filenames The only line we need to change here is the task &lt;- \"taskname\" to task &lt;- \"VAorient_S\". 11.5 Import This section can stay exactly the same. As long as you are using these templates this line of code can always remain untouched. In fact, if you are using these templates, every line of code except task &lt;- \"taskname\" can likely stay exactly the same. 11.6 Tidy raw data This is the meat of the script, where the action happens. It will also be different for every task - obviously. We will walk through each line of code for this section using the visual arrays task as an example. It will be easier if you have an example data set so you can follow along. Type the following lines of code in your console. library(englelab) data_import &lt;- visualarrays_messy Now you should see the object (data frame) data_import in your environment window. Click on the data frame to view it. This is a messy raw data set from the visual arrays task. You are starting as though you had already imported the data file and ready to create the R code for the “Tidy raw data” section. When creating a tidy raw data file it is best to keep as much of the data as possible - just reorganize and rename things (though we will see there is a little extra work that is needed for the visual arrays task). Therefore, we might as well keep the information from the practice trials as well. 11.6.1 filter() Often times there might be rows of data in the messy raw data file from the instruction or end procedures. There is not much need to keep these so we typically remove them and only keep the rows for practice and real trials. The column that contains the information as to which procedure the current row corresponds to is typically named Procedure[Trial] but it may be different depending on how the task was programmed. Let’s go ahead and evaluate the unique values in this column. This can help us figure out how to apply the filter and also check if this is the correct column. In the console window, type: unique(data_import$`Procedure[Trial]`) ## [1] NA &quot;pracproc&quot; &quot;showproc&quot; Practice trials correspond to the values “pracproc” and real task trials correspond to values “showproc”. You have to know how the task was programmed to know this. filter() is usually the first step in creating a tidy raw data file because it helps to first remove a bunch of unwanted and irrelevant rows. Often times before filter() I will go ahead and rename() TrialProc[Trial] simply to TrialProc because we might need to keep on referring to this column as we move forward. data_raw &lt;- data_import %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% filter(TrialProc == &quot;showproc&quot; | TrialProc == &quot;pracproc&quot;) We are only keeping rows that either have the value “showproc” or “pracproc” in the TrialProc column. 11.6.2 mutate() In the console type in unique(data_import$VisResponse.RESP) ## [1] NA 5 6 The subject’s response was coded as 5 or 6. What the hell does that mean? They pressed the 5 or 6 key but that tells us nothing about what that response meant - which was a same vs. different judgment. This is only something you would know if you knew how the task was programmed. This is a property of a messy raw data file - it requires knowledge of how the task was programmed. The purpose of creating a tidy raw data file is to allow understanding of the columns and values without knowledge of how the task was programmed. In this case 5 corresponds to a “same” judgment and 6 corresponds to a “different” judgment. We can make this more clear by changing value 5 and 6 to same and different respectively. These sort of changes are typically the next step in creating tidy raw data. Let’s also change showproc and pracproc to real and practice respectively. data_raw &lt;- data_import %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% filter(TrialProc == &quot;showproc&quot; | TrialProc == &quot;pracproc&quot;) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;showproc&quot; ~ &quot;real&quot;, TrialProc == &quot;pracproc&quot; ~ &quot;practice&quot;), Response = case_when(VisResponse.RESP == 5 ~ &quot;same&quot;, VisResponse.RESP == 6 ~ &quot;different&quot;, TRUE ~ as.character(NA)), CorrectResponse = case_when(VisResponse.CRESP == 5 ~ &quot;same&quot;, VisResponse.CRESP == 6 ~ &quot;different&quot;)) Great! For some tasks, that is really all that will be required at this step. For others, it might take a lot more. For instance, in the visual arrays task it is important to know whether a trial response was a Correct Rejection When the correct response was “same” and the actual response was “same” False Alarm When the correct response was “same” but the actual response was “different” Miss When the correct response was “different” but the actual response was “same” or Hit When the correct response was “different” and the actual response was “different” Let’s go ahead and add this information the the tidy raw data. data_raw &lt;- data_import %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% filter(TrialProc == &quot;showproc&quot; | TrialProc == &quot;pracproc&quot;) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;showproc&quot; ~ &quot;real&quot;, TrialProc == &quot;pracproc&quot; ~ &quot;practice&quot;), Response = case_when(VisResponse.RESP == 5 ~ &quot;same&quot;, VisResponse.RESP == 6 ~ &quot;different&quot;, TRUE ~ as.character(NA)), CorrectResponse = case_when(VisResponse.CRESP == 5 ~ &quot;same&quot;, VisResponse.CRESP == 6 ~ &quot;different&quot;), CorrrectRejection = case_when(CorrectResponse == &quot;same&quot; &amp; Response == &quot;same&quot; ~ 1, TRUE ~ 0), FalseAlarm = case_when(CorrectResponse == &quot;same&quot; &amp; Response == &quot;different&quot; ~ 1, TRUE ~ 0), Miss = case_when(CorrectResponse == &quot;different&quot; &amp; Response == &quot;same&quot; ~ 1, TRUE ~ 0), Hit = case_when(CorrectResponse == &quot;different&quot; &amp; Response == &quot;different&quot; ~ 1, TRUE ~ 0)) 11.6.3 select() The final step is to subset only the columns we need to keep. Right now there are way more columns than what we need. In the console type: colnames(data_import) ## [1] &quot;ExperimentName&quot; &quot;Subject&quot; ## [3] &quot;Session&quot; &quot;Clock.Information&quot; ## [5] &quot;DataFile.Basename&quot; &quot;Display.RefreshRate&quot; ## [7] &quot;ExperimentVersion&quot; &quot;Group&quot; ## [9] &quot;RandomSeed&quot; &quot;RuntimeCapabilities&quot; ## [11] &quot;RuntimeVersion&quot; &quot;RuntimeVersionExpected&quot; ## [13] &quot;SessionDate&quot; &quot;SessionStartDateTimeUtc&quot; ## [15] &quot;SessionTime&quot; &quot;StudioVersion&quot; ## [17] &quot;Block&quot; &quot;BlockList&quot; ## [19] &quot;BlockList.Cycle&quot; &quot;BlockList.Sample&quot; ## [21] &quot;BlockName&quot; &quot;goodbye.DEVICE&quot; ## [23] &quot;Instruct1.DEVICE&quot; &quot;Instruct2.DEVICE&quot; ## [25] &quot;Instruct3.DEVICE&quot; &quot;Instruct4.DEVICE&quot; ## [27] &quot;Instruct5.DEVICE&quot; &quot;Procedure[Block]&quot; ## [29] &quot;Running[Block]&quot; &quot;Trial&quot; ## [31] &quot;Answer&quot; &quot;AttendColor&quot; ## [33] &quot;attendIS&quot; &quot;change&quot; ## [35] &quot;colchange&quot; &quot;color1&quot; ## [37] &quot;color2&quot; &quot;color3&quot; ## [39] &quot;color4&quot; &quot;color5&quot; ## [41] &quot;color6&quot; &quot;color7&quot; ## [43] &quot;color8&quot; &quot;Correct&quot; ## [45] &quot;IgnoreColor&quot; &quot;ignoreIS&quot; ## [47] &quot;PracList&quot; &quot;PracList.Cycle&quot; ## [49] &quot;PracList.Sample&quot; &quot;probesq&quot; ## [51] &quot;Procedure[Trial]&quot; &quot;RealList&quot; ## [53] &quot;RealList.Cycle&quot; &quot;RealList.Sample&quot; ## [55] &quot;Running[Trial]&quot; &quot;SetSize&quot; ## [57] &quot;size&quot; &quot;sp1&quot; ## [59] &quot;sp2&quot; &quot;sp3&quot; ## [61] &quot;sp4&quot; &quot;sp5&quot; ## [63] &quot;sp6&quot; &quot;sp7&quot; ## [65] &quot;sp8&quot; &quot;TrialPrompt.DEVICE&quot; ## [67] &quot;VisResponse.ACC&quot; &quot;VisResponse.CRESP&quot; ## [69] &quot;VisResponse.DEVICE&quot; &quot;VisResponse.DurationError&quot; ## [71] &quot;VisResponse.OnsetDelay&quot; &quot;VisResponse.OnsetTime&quot; ## [73] &quot;VisResponse.OnsetToOnsetTime&quot; &quot;VisResponse.RESP&quot; ## [75] &quot;VisResponse.RT&quot; &quot;VisResponse.RTTime&quot; ## [77] &quot;x1&quot; &quot;x2&quot; ## [79] &quot;x3&quot; &quot;x4&quot; ## [81] &quot;x5&quot; &quot;x6&quot; ## [83] &quot;x7&quot; &quot;x8&quot; ## [85] &quot;y1&quot; &quot;y2&quot; ## [87] &quot;y3&quot; &quot;y4&quot; ## [89] &quot;y5&quot; &quot;y6&quot; ## [91] &quot;y7&quot; &quot;y8&quot; Wow we started with over 90 columns! This is not usable. Let’s reduce it. data_raw &lt;- data_import %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% filter(TrialProc == &quot;showproc&quot; | TrialProc == &quot;pracproc&quot;) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;showproc&quot; ~ &quot;real&quot;, TrialProc == &quot;pracproc&quot; ~ &quot;practice&quot;), Response = case_when(VisResponse.RESP == 5 ~ &quot;same&quot;, VisResponse.RESP == 6 ~ &quot;different&quot;, TRUE ~ as.character(NA)), CorrectResponse = case_when(VisResponse.CRESP == 5 ~ &quot;same&quot;, VisResponse.CRESP == 6 ~ &quot;different&quot;), CorrectRejection = case_when(CorrectResponse == &quot;same&quot; &amp; Response == &quot;same&quot; ~ 1, TRUE ~ 0), FalseAlarm = case_when(CorrectResponse == &quot;same&quot; &amp; Response == &quot;different&quot; ~ 1, TRUE ~ 0), Miss = case_when(CorrectResponse == &quot;different&quot; &amp; Response == &quot;same&quot; ~ 1, TRUE ~ 0), Hit = case_when(CorrectResponse == &quot;different&quot; &amp; Response == &quot;different&quot; ~ 1, TRUE ~ 0)) %&gt;% select(Subject, TrialProc, Trial, SetSize, Accuracy = VisResponse.ACC, Response, CorrectResponse, CorrectRejection, FalseAlarm, Miss, Hit, SessionDate, SessionTime) Now in the console window type in: colnames(data_raw) ## [1] &quot;Subject&quot; &quot;TrialProc&quot; &quot;Trial&quot; &quot;SetSize&quot; ## [5] &quot;Accuracy&quot; &quot;Response&quot; &quot;CorrectResponse&quot; &quot;CorrectRejection&quot; ## [9] &quot;FalseAlarm&quot; &quot;Miss&quot; &quot;Hit&quot; &quot;SessionDate&quot; ## [13] &quot;SessionTime&quot; We have gone from over 90 columns to only 13 columns of data! I would say that we have tidied this messy data! 11.7 Output The final section of the script is to save the tidy raw data file. You do not need to change this section at all. 11.8 Final The final script will look like: #### Setup #### ## Load packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Merged&quot; output_dir &lt;- &quot;Data Files&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import_file &lt;- paste(task, &quot;.txt&quot;, sep = &quot;&quot;) output_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) ################ #### Import #### data_import &lt;- read_delim(here(import_dir, import_file), &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ################ #### Tidy raw data #### data_raw &lt;- data_import %&gt;% rename(TrialProc = `Procedure[Trial]`) %&gt;% filter(TrialProc == &quot;showproc&quot; | TrialProc == &quot;pracproc&quot;) %&gt;% mutate(TrialProc = case_when(TrialProc == &quot;showproc&quot; ~ &quot;real&quot;, TrialProc == &quot;pracproc&quot; ~ &quot;practice&quot;), Response = case_when(VisResponse.RESP == 5 ~ &quot;same&quot;, VisResponse.RESP == 6 ~ &quot;different&quot;, TRUE ~ as.character(NA)), CorrectResponse = case_when(VisResponse.CRESP == 5 ~ &quot;same&quot;, VisResponse.CRESP == 6 ~ &quot;different&quot;), CorrectRejection = case_when(CorrectResponse == &quot;same&quot; &amp; Response == &quot;same&quot; ~ 1, TRUE ~ 0), FalseAlarm = case_when(CorrectResponse == &quot;same&quot; &amp; Response == &quot;different&quot; ~ 1, TRUE ~ 0), Miss = case_when(CorrectResponse == &quot;different&quot; &amp; Response == &quot;same&quot; ~ 1, TRUE ~ 0), Hit = case_when(CorrectResponse == &quot;different&quot; &amp; Response == &quot;different&quot; ~ 1, TRUE ~ 0)) %&gt;% select(Subject, TrialProc, Trial, SetSize, Accuracy = VisResponse.ACC, Response, CorrectResponse, CorrectRejection, FalseAlarm, Miss, Hit, SessionDate, SessionTime) ####################### #### Output #### write_csv(data_raw, here(output_dir, output_file)) ################ rm(list=ls()) Wow congrats! You have created an R script to convert a messy raw data file to a tidy raw data fie. 11.9 masterscript.R The masterscript.R file in a Data Collection directory might look something like ## Data Preparation for StudyName ############################################# #------ 0. &quot;messy&quot; to &quot;tidy&quot; raw data ------# ############################################# source(&quot;R Scripts/0_rapm_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_lettersets_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_numberseries_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_ospan_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_symspan_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_rotspan_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_antisaccade_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_sact_raw.R&quot;, echo=TRUE) source(&quot;R Scripts/0_visualarrays_raw.R&quot;, echo=TRUE) rm(list=ls()) ############################################# This allows you to run all the scripts to create tidy raw data files all in one place. The organization structure and workflow for data preparation is depicted below: "],
["data-analysis-setup.html", "12 Data Analysis: Setup 12.1 Project Organization 12.2 Setup Data Analysis Directories", " 12 Data Analysis: Setup 12.1 Project Organization Once Data Preparation is finished you will need to copy and paste the tidy raw data files from the Data Collection directory to a new Data Analysis directory Ideally, there will be a tidy raw data file for EVERY task in the data collection directory However, you don’t necessarily need EVERY task for your research project. The Data Collection directory will serve as an archive of where you should grab data files that are relevant to your research project. A key part of this is that you copy the tidy raw data files ONLY from the data collection directory to a data analysis project. You do not copy data files from one data analysis project to another! Even if they use the same tasks. In the EngleLab, we often conduct large-scale data collection studies in which there are many different research projects going on at once. Many of the tasks will be shared between these research projects. This means each separate project is using a lot of the same data. This can make it difficult to figure out how and at what stage to separate data processing and analysis between these studies. If someone already created a data file, for their research project, with scored variables on a lot of the tasks that you need, should you copy and paste that data file into your analysis project? NO! This will undermine the reproducibility of your research project and analyses. You will also have no way of figuring out how they scored their variables, what data cleaning procedures they used, etc. There is no transparency of how they got from A (the raw data) to B (the scored data file). They simply gave you B. Whereas with the Project Organization method I outlined in Chapter 7, you can copy the tidy raw data files from the Data Collection directory into your Data Analysis directory. And if someone has already created scripts to score some of the tasks you need, you can also copy those R scripts over to your project. Now you will have A (the raw data) and the R script of how to get from A to B (the scored data file). This will give you full transparency and you can completely reproduce the analyses. The Data Collection directory is where both the messy and tidy raw data files are stored. No data analysis steps occur in the Data Collection directory. Data analysis steps occur in the Data Analysis directories. Data analysis is where we aggregate and score data from the tasks, conduct statistical analyses, and create data visualizations of our findings. 12.2 Setup Data Analysis Directories Here is an overview of how to setup a data analysis directory In Chapter 7, we covered how to setup data collection and data analysis directories in more detail. Follow these steps to create the Data Analysis directory: Open RStudio File -&gt; New Project… New Directory -&gt; Research Study Browse to where the project folder will be located and specify the project folder name Choose Repository Type: data analysis Select ‘Create Project’ Once you create the R Project, you will see the following directory setup in the “Files” window pane in RStudio. Let’s take a little time to understand this setup Data Files: This is where ALL the data files will be located. Within Data Files there are folders for Raw Data and Scored Data. The Raw Data folder will be where trial-level tidy raw data files will be located. From the tidy raw data files we will use scripts to create scored data files (one subject per row with columns as task scores) in the Scored Data folder. Eventually we will create a final merged data file that we will use for our main analyses that has the final set of subjects and only the relevant task scores we need for our analyses. Documents: We can put any documents here related to the study and analysis plan. We might like to put a methods document in here or a document with our analysis plan. Or any articles that are relevant to the study R Scripts: ALL R scripts, with exception of the masterscript, will go in this folder. You should not put R scripts anywhere else! We will label the R scripts in a way that they can be easily organized to be consistent with the logical order that they need to be ran. Scripts for scoring tasks will be labeled with the prefix: 1_ and contain the suffix: _score.R (e.g. 1_antisaccade_score.R). The one denotes that this is the first step in data analysis. This is where we clean/trim, remove problematic subjects, and score the task. The script to create a final merged data file that we will use for our main analyses will be labeled as: 2_merge.R . The two denotes that this script has to be ran after the scoring scripts. If you organize your folder by ‘Name’ it will put the _score.R scripts first followed by the 2_merge.R script. This organizes your files by the order they need to be ran and makes it easy to find the script you want to edit or change. Results: This is where any results output will be saved to masterscript.R: Rather than opening each R script - running it - then closing out of it, opening the next script - running it - then closing out of it, and so on… you can run all of your R scripts in the masterscript. This also allows you to visualize the order of your scripts in an easy way. Also, if you choose to share your project to an open science repository this allow other researchers to quickly see your data processing workflow and the order of your R scripts. .Rproj: You only need to interact with this file when opening your R Project. When you want to work on your R project - YOU SHOULD ALWAYS OPEN RSTUDIO BY OPENING THE .Rproj FILE!!! "],
["data-analysis-tidy-to-scored.html", "13 Data Analysis: Tidy to Scored 13.1 Overview 13.2 Score script template 13.3 Setup 13.4 Import 13.5 Data Scoring 13.6 Data Cleaning 13.7 Calculate reliability 13.8 Output 13.9 Create a data cleaning log 13.10 Final", " 13 Data Analysis: Tidy to Scored 13.1 Overview Data Analysis requires many steps and decisions that need to be made along the way. Often times the goal is to create a single data file that is ready for statistical analysis. But before we can get to this single data file a couple of steps are involved. Create a scored data file for each task At this stage we also remove any subjects that are suspect or have too poor of performance, and remove univariate outliers. Merge the scored data files into a single data file At this stage we also can create composite factors if needed. In this chapter, I will use the visual arrays task as an example of how to do step 1. 13.2 Score script template The R script template to score raw data files was explained in Chapter 9. In this Chapter we will walk through a script to score a raw data file from a visual arrays task. This is the R script template downloaded by: workflow::template(type = \"score\") #### Setup #### ## Load Packages library(here) library(readr) library(dplyr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Raw Data&quot; output_dir &lt;- &quot;Data Files/Scored Data&quot; ## Set Import/Output Filenames task &lt;- &quot;taskname&quot; import_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output_file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) ## Set Data Cleaning Params ############### #### Import #### data_import &lt;- read_csv(here(import_dir, import_file)) ################ #### Score Data #### data_scores &lt;- data_import %&gt;% filter() %&gt;% group_by() %&gt;% summarise() #################### #### Clean Data #### #################### #### Calculate Reliability #### ############################### #### Output #### write_csv(data_scores, here(output_dir, output_file)) ################ rm(list=ls()) 13.3 Setup Load packages Any packages required for this script are loaded at the top. For this task we will need the here, readr, dplyr packages in addition to tidyr and knitr. Go ahead and add library(tidyr) and library(knitr) to this section. Set Import/Output Directories To make this example easier you will not have to actually import/output any files. Set Import/Output Filenames The only line we need to change here is the task &lt;- \"taskname\" to task &lt;- \"VAorient_S\". Set Data Cleaning Params In this section of the script we can set certain data cleaning criteria to variables. This makes it easy to see what data cleaning criteria were used right at the top of the script rather than having to read through and try to interpret the script. For the visual arrays task we should remove subjects who had low accuracy scores - lets say those with accuracy lower than 3.5 SDs Add acc_criteria &lt;- -3.5 to this section of the script. Optionally it would be a good idea to add a comment about what this criteria is for. 13.4 Import This section can stay exactly the same. As long as you are using these templates this line of code can always remain untouched. 13.5 Data Scoring This is the meat of the script, where the action happens. It will also be different for every task - obviously. We will walk through each line of code for this section using the visual arrays task as an example. It will be easier if you have an example data set so you can follow along. Type the following lines of code in your console. library(englelab) data_import &lt;- visualarrays_tidy Now you should see the object (data frame) data_import in your environment window. Click on the data frame to view it. This is a tidy raw data set from the visual arrays task. You are starting as though you had already imported the data file and ready to score the visual arrays task. 13.5.1 filter() Notice how the data set has both practice and real trials: unique(visualarrays_tidy$TrialProc) ## [1] &quot;practice&quot; &quot;real&quot; We need to get rid of practice trials. data_import &lt;- visualarrays_tidy %&gt;% filter(TrialProc == &quot;real&quot;) 13.5.2 group_by() and summarise() View the data frame. Notice how there are only real trials. The next step is to score the visual arrays data. There are set sizes of 5 and 7 in this visual arrays data: unique(data_import$SetSize) ## [1] 5 7 Therefore, there will be two resulting k scores. We can average the two k scores to get one final k score for the visual arrays task. This means we need to calculate a k score for each subject and each set size. First let’s talk a little more about how the visual arrays task is scored. The visual arrays task is scored with the following formula. k = N * (P(H) + P(CR) - 1) Where N is the set size, P(H) is the probability of hits, and P(CR) is the probability of correct rejections. To understand how to calculate P(H) and P(CR) the figure below will be useful. Any given trial can have a correct response as either “same” or “different” - not both so these probabilities are independent. However, on a “same” trial the response made can either be a correct rejection (“same” response made) or a false alarm (“different” response made). The probability of these responses are dependent such that: P(CR) = 1 - P(FA) and P(FA) = 1 - P(CR). If you rearrange these you get that 1 = P(CR) + P(FA). We need to calculate P(CR), and we can do this by writing this formula in terms of number of trials rather than probability. 1 = (CR.n + FA.n) / Same.n, because CR.n + FA.n = Same.n Therefore, the probability of a CR is the proportion of CR.n to Same.n P(CR) = CR.n / Same.n, P(CR) = CR.n / (CR.n + FA.n) The same logic can be applied to Hits and Misses for calculating P(H) P(H) = H.n / Different.n, P(H) = H.n / (H.n + M.n) This means we need to know the total number of trials that were correct rejections, false alarms, misses, and hits. We need to calculate this separately for each subject and each set size. We can do this using group_by() and summarise() View the data frame. You will notice there are columns for CorrectRejection, FalseAlarm, Miss, and Hit. The values in these columns are 1’s or 0’s. If a row has a value of 1 on the CorrectionRejection column that means the response was a Correct Rejection. We can simply calculate the number of Correct Rejection (and the other response options) by summing those columns by subject and set size. data_scores &lt;- data_import %&gt;% group_by(Subject, SetSize) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() View the data frame. You can see it is now reduced to two rows per subject, one for each set size. We also have columns for the number of trials on correct rejections, false alarms, misses, and hits. Now we simply need to calculate P(CR) and P(H) using the formulas above. data_scores &lt;- data_import %&gt;% group_by(Subject, SetSize) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n)) Now we can calculate k for each set size using the k formula k = N * (P(H) + P(CR) - 1) Where N is the set size. data_scores &lt;- data_import %&gt;% group_by(Subject, SetSize) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) Let’s take a moment to further understand this formula and how it is interpreted as a capacity score. If a subject gets a trial correct, either a correct rejection or a hit, it is assumed that they retained all items in memory from the target array (a BIG assumption). For instance, if the set size is 5 then on that trial they would assume to have a storage capacity of 5. P(H) and P(CR) are the proportion of correct trials on “different” and “same” trials, respectively. If a subject’s true capacity or k is 5 then they would be expected to make a correct response on all trials or nearly all trials. Therefore, P(H) and P(CR) would both equal 1. Let’s rearrange the k formula as: k = (N * P(H)) + (N * P(CR)) - N With this example we would get k = (5 * 1) + (5 * 1) - 5 = 5 What about if a subject was completely guessing, a true capacity of 0: k = (5 * .5) + (5 * .5) - 5 = 0 P(H) and P(CR) are at .5 when guessing What if a subject’s true capacity was 3: k = (5 * .8) + (5 * .8) - 5 = 3, there are actually more than one combination of P(H) and P(CR) values to result in a k = 3, as long as (P(H) + P(CR)) / 2 = .8. In the end, we want a data frame that has only one row per subject and columns corresponding to k scores on set size 5 and 7. In other words we need to make this data frame wider. To make the data frame wider we will use the pivot_wider() function from the tidyr package. data_scores &lt;- data_import %&gt;% group_by(Subject, SetSize) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;SetSize&quot;, values_from = c(&quot;CR&quot;, &quot;H&quot;, &quot;k&quot;)) View the data frame. Now there is only one row per subject with k scores for each set size spread across columns. All that is left to do is calculate an average k score data_scores &lt;- data_import %&gt;% group_by(Subject, SetSize) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;SetSize&quot;, values_from = c(&quot;CR&quot;, &quot;H&quot;, &quot;k&quot;)) %&gt;% mutate(VA_k = (k_5 + k_7) / 2) Yay! We calculated a single k score for the visual arrays task! We are done!… or are we? No we are not. There are a few more things we can do to make further analyses and reporting easier. This will also allow us to create a log of the data cleaning procedures we employed. 13.6 Data Cleaning 13.6.1 Remove problematic subjects First, it would be a good idea to remove problematic subjects. Problematic subjects are usually those that are performing so poorly, below chance, that they either are not really doing the task or completely misunderstood the instructions. In the past we have used a sort of blanket 3.5 SDs below mean accuracy - however I am not sure that is the best approach. Also for the sake of this example there are no subjects below 3.5 SDs, therefore, let’s set it at 2.0 SDs. Before we get into the steps to perform, let’s set a data cleaning parameter at the top of the script in the Setup section. ## Set Data Cleaning Params acc_criteria &lt;- -2 # Remove subjects with accuracy 2 SDs below the mean ############### We set acc_criteria to -2 because we will want to remove subjects that had accuracy 2 SDs below the mean. The steps for this are pretty simple: Calculate the mean accuracy for each subject - summarise() Create z-scores on mean accuracy - mutate() and scale() Create a data frame that contains only those that fall below the 2 SDs - filter() Remove those subjects from the scored data file we just created - filter() data_remove &lt;- data_import %&gt;% group_by(Subject) %&gt;% summarise(Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(ACC.mean_z = scale(Accuracy.mean, center = TRUE, scale = TRUE)[,1]) %&gt;% filter(ACC.mean_z &lt; acc_criteria) data_scores &lt;- filter(data_scores, !(Subject %in% data_remove$Subject)) What we did was create a separate data frame to hold subjects that we want to remove because they are problematic. And then we simply applied a filter on the original data frame to keep only those subjects that are not present in the removed data frame. 13.6.2 Remove univariate outliers Next, we can remove univariate outliers at this stage too. If we remove it at a later stage it is harder to create a log of how many outliers were removed for this task. Before we get into the steps to perform, let’s set another data cleaning parameter at the top of the script in the Setup section. ## Set Data Cleaning Params acc_criteria &lt;- -2 # Remove subjects with accuracy 2 SDs below the mean outlier_criteria &lt;- 2.0 # Trim outliers 2 SDs from the mean ############### Here I am deciding to use an outlier cutoff on VA_k scores at +/- 2 SDs from the mean. This is too small of a cutoff, but I am using it for the sake of this example because there are no subjects at a higher cutoff. The steps to do this are real simple: Calculate z-scores on the VA_k score - mutate() and scale() Create a data frame that contains only those that are considered outliers - filter() Remove those subjects from the scored data file we just created - filter() data_outliers &lt;- data_scores %&gt;% mutate(VA_k_z = scale(VA_k, center = TRUE, scale = TRUE)) %&gt;% filter(VA_k_z &gt;= outlier_criteria | VA_k_z &lt;= -1 * outlier_criteria) data_scores &lt;- filter(data_scores, !(Subject %in% data_outliers$Subject)) Great! Now we have cleaned up the data a bit. At the end of the script we will go over how to write code to create a log of this data cleaning. 13.7 Calculate reliability Finally, we might as well go ahead and calculate reliability here seeing as how we have both the raw and cleaned data set loaded in the environment already. To calculate split-half reliability for this task we need to split on even and odd items for both set-size 5 and set-size 7. Then we can calculate a visual arrays k score for even items and for odd items. We basically just need to repeat the process for calculating k scores but first split between even and odd items. First, let’s make sure we are only calculating reliability from the raw data on those Subjects that did not get removed in the previous steps (Subjects that have made it through data cleaning). reliability &lt;- data_import %&gt;% filter(Subject %in% data_scores$Subject) This next code block way is how to split trials into “even” and “odd”. In this case we want to split separately on each set size so we group by both Subject and SetSize. reliability &lt;- data_import %&gt;% filter(Subject %in% data_scores$Subject) %&gt;% group_by(Subject, SetSize) %&gt;% mutate(Index = row_number(), Split = ifelse(Index %% 2, &quot;odd&quot;, &quot;even&quot;)) Since the data frame is split by Subject and SetSize, row_number() provides the occurrence of that trial within the set size (e.g. 1 = the first occurrence of set size 5 for a given subject, 2 = the second occurrence of set size 5 for that subject, and so on). This information is then stored in the column Index. We can then evaluate whether the value in Index is divisible by two. Index %% 2 will return TRUE if there is a remainder (if it is not divisible by two) and FALSE if there is no remainder (if it is divisible by two). We can then set the value of Split to “odd” when it evaluates as TRUE and “even” when it evaluates as FALSE. Now we have a column specifying whether a trial is an even or odd trial within each set size for a given subject. Now we just need to calculate k scores like we did above. Copy and paste the previous code up until pivot_wider() reliability &lt;- data_import %&gt;% filter(Subject %in% data_scores$Subject) %&gt;% group_by(Subject, SetSize) %&gt;% mutate(Index = row_number(), Split = ifelse(Index %% 2, &quot;odd&quot;, &quot;even&quot;)) %&gt;% group_by(Subject, SetSize, Split) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) Take a look at what the data frame looks like View(reliability) When we pivot_wider(), we need to make sure to also split on the Split column in addition to the SetSize column. Then we can calculate a VA_k score for both even and odd split-halves. reliability &lt;- data_import %&gt;% filter(Subject %in% data_scores$Subject) %&gt;% group_by(Subject, SetSize) %&gt;% mutate(Index = row_number(), Split = ifelse(Index %% 2, &quot;odd&quot;, &quot;even&quot;)) %&gt;% group_by(Subject, SetSize, Split) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = c(&quot;SetSize&quot;, &quot;Split&quot;), values_from = c(&quot;CR&quot;, &quot;H&quot;, &quot;k&quot;)) %&gt;% mutate(VA_k_even = (k_5_even + k_7_even) / 2, VA_k_odd = (k_5_odd + k_7_odd) / 2) Now let’s calculate the correlation with a Spearman-Brown Correction using summarise() and mutate() reliability &lt;- data_import %&gt;% filter(Subject %in% data_scores$Subject) %&gt;% group_by(Subject, SetSize) %&gt;% mutate(Index = row_number(), Split = ifelse(Index %% 2, &quot;odd&quot;, &quot;even&quot;)) %&gt;% group_by(Subject, SetSize, Split) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = c(&quot;SetSize&quot;, &quot;Split&quot;), values_from = c(&quot;CR&quot;, &quot;H&quot;, &quot;k&quot;)) %&gt;% mutate(VA_k_even = (k_5_even + k_7_even) / 2, VA_k_odd = (k_5_odd + k_7_odd) / 2) %&gt;% summarise(r = cor(VA_k_even, VA_k_odd)) %&gt;% mutate(r = (2 * r) / (1 + r)) Finally let’s add this as a column to the main data frame with the overall VA_k score. data_scores$VA_splithalf &lt;- reliability$r Now view the data frame - View(data_scores). You should see an extra column with the split half reliability value. This will make it easier during data analysis to create a table output of the reliability values from every task. 13.8 Output The final section of the essential parts of the script is to save the scored data file. You do not need to change this section at all from the template. #### Output #### write_csv(data_scores, here(output_dir, output_file)) ################ 13.9 Create a data cleaning log This addition is more optional but it can be nice to have. When reporting your findings in a manuscript it may be a good idea to report how many subjects were removed on each task due to being problematic and being an outlier. This creates more transparency for those that may read your manuscript. It is also a good idea for you to just be aware of how many subjects are being removed. This can also be a good time to check that you didn’t do anything completely wrong - like remove way too many subjects! When we get to the masterscript in the next chapter - you will see that there is a line of code to create a blank log file - Data Files/ Scored Data/log.txt. We need to create this in the masterscript because we will have every _score.R script append information to this log. That way we have one single file with information from all tasks. To append information to the log file, we will use a combination of paste(), sink(), and cat() functions. These are all base R functions so no need to load extra libraries - which is always nice. A key part of this is making it easy to read because the more tasks we have the more cluttered this log file will be. Let’s start out by creating the header and footer for this task in the log file. ## Append to Log #### log_header &lt;- paste(&quot;===== Visual Arrays: 1_visualarrays_score.R =====\\n\\n&quot;, format(Sys.Date(), &quot;%B %d %Y&quot;), &quot;\\n&quot;, sep = &quot;&quot;) log_footer &lt;- &quot;==================================================\\n&quot; ################## The output in the log.txt file would look something like: cat(log_header, log_footer, fill = FALSE, sep = &quot;\\n&quot;) ===== Visual Arrays: 1_visualarrays_score.R ===== September 21 2020 ================================================== This is nice. The header clearly tells us what task this information is for and what script was ran. The date is when the script was ran - given by format(Sys.Date(), \"%B %d %Y\"). Then there is a clear line segment separating it from anything that comes below. Note that the notation \"\\n\" means new line - it is like hitting the enter/return key on the keyboard to move down to the next line. Creating this extra spacing will make it easier to scan the log.txt file. Now let’s put information between the header and footer. First, let’s report the total number of initial subjects that we had raw data on. ## Append to Log #### log_init_n &lt;- paste(&quot;-- &quot;, length(unique(data_import$Subject)), &quot;\\t Initial subject count&quot;, sep = &quot;&quot;) ################## length(unique(data_import$Subject)) returns the number of unique subject IDs in data_import. Now let’s add information about the subjects that were removed because they were problematic and outliers. log_remove &lt;- paste(&quot;-- &quot;, length(unique(data_remove$Subject)), &quot;\\t Number of problematic subjects removed&quot;, sep = &quot;&quot;) log_outlier &lt;- paste(&quot;-- &quot;, length(unique(data_outliers$Subject)), &quot;\\t Number of outliers removed&quot;, sep = &quot;&quot;) Now we are evaluating the number of subjects in the data_remove and data_outliers data frames. And a line for the final subject count: log_final_n &lt;- paste(&quot;-- &quot;, length(unique(data_scores$Subject)), &quot;\\t Final subject count&quot;, &quot;\\n&quot;, sep = &quot;&quot;) Again, just to see what this will look like run the following line of code in your console: cat(log_header, log_init_n, log_remove, log_outlier, log_final_n, log_footer, fill = FALSE, sep = &quot;\\n&quot;) ===== Visual Arrays: 1_visualarrays_score.R ===== September 21 2020 -- 201 Initial subject count -- 4 Number of problematic subjects removed -- 4 Number of outliers removed -- 193 Final subject count ================================================== Now let’s add two tables - the data_remove and data_outlier tables. We need to convert the data frame to an easy to read format in a text file. We will use a markdown format. To do so we can use the kable() function from the knitr package. table_remove &lt;- kable(data_remove, digits = 2, format = &quot;markdown&quot;) table_outlier &lt;- kable(data_outliers, digits = 2, format = &quot;markdown&quot;) This is what the table will look like: cat(table_remove, fill = FALSE, sep = &quot;\\n&quot;) | Subject| Accuracy.mean| ACC.mean_z| |-------:|-------------:|----------:| | 15102| 0.46| -2.10| | 15147| 0.45| -2.22| | 15176| 0.46| -2.10| | 15210| 0.46| -2.10| cat(table_outlier, fill = FALSE, sep = &quot;\\n&quot;) | Subject| CR_5| CR_7| H_5| H_7| k_5| k_7| VA_k| VA_k_z| |-------:|----:|----:|----:|----:|-----:|-----:|-----:|---------:| | 15014| 0.90| 0.55| 0.20| 0.30| 0.50| -1.05| -0.27| -2.032757| | 15042| 0.75| 0.65| 0.15| 0.35| -0.50| 0.00| -0.25| -2.011692| | 15139| 0.45| 0.20| 0.70| 0.55| 0.75| -1.75| -0.50| -2.222346| | 15175| 0.55| 0.40| 0.50| 0.45| 0.25| -1.05| -0.40| -2.138084| So we have created the formatting for all the information we want to add. All that is left is to append it to the log.txt file. We can use a combination of sink() and cat(). sink() basically opens and closes the requested file. Like you would open and close it on your computer. Except it is doing it behind the scenes. We need to include sink() twice, once to open and once to close the file. In between them will be a cat() function that will concatenate all the formatted information together. If we put this all together, we get: ## Append to Log #### log_header &lt;- paste(&quot;===== Visual Arrays: 1_visualarrays_score.R =====\\n\\n&quot;, format(Sys.Date(), &quot;%B %d %Y&quot;), &quot;\\n&quot;, sep = &quot;&quot;) log_init_n &lt;- paste(&quot;-- &quot;, length(unique(data_import$Subject)), &quot;\\t Initial subject count&quot;, sep = &quot;&quot;) log_remove &lt;- paste(&quot;-- &quot;, length(unique(data_remove$Subject)), &quot;\\t Number of problematic subjects removed&quot;, sep = &quot;&quot;) log_outlier &lt;- paste(&quot;-- &quot;, length(unique(data_outliers$Subject)), &quot;\\t Number of outliers removed&quot;, sep = &quot;&quot;) log_final_n &lt;- paste(&quot;-- &quot;, length(unique(data_scores$Subject)), &quot;\\t Final subject count&quot;, &quot;\\n&quot;, sep = &quot;&quot;) table_remove &lt;- kable(data_remove, digits = 2, format = &quot;markdown&quot;) table_outlier &lt;- kable(data_outliers, digits = 2, format = &quot;markdown&quot;) log_footer &lt;- &quot;==================================================\\n&quot; sink(file = here(&quot;log.txt&quot;), append = FALSE, split = TRUE) cat(log_header, log_init_n, log_remove, log_outlier, log_final_n, print(table_remove), &quot;\\n&quot;, print(table_outlier), &quot;\\n&quot;, log_footer, fill = FALSE, sep = &quot;\\n&quot;) sink() ################## The resulting log.txt file would look like: 13.10 Final The final script would look like. You can see it gets quite long… It would be ideal to add more comments describing how the data is being scored, cleaned, and reliability calculated. #### Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(tidyr) library(knitr) ## Set Import/Output Directories import_dir &lt;- &quot;Data Files/Raw Data&quot; output_dir &lt;- &quot;Data Files/Scored Data&quot; removed_dir &lt;- &quot;Data Files/Scored Data/removed&quot; ## Set Import/Output Filenames task &lt;- &quot;VisualArrays&quot; import_file &lt;- paste(task, &quot;raw.csv&quot;, sep = &quot;_&quot;) output_file &lt;- paste(task, &quot;Scores.csv&quot;, sep = &quot;_&quot;) removed_file &lt;- paste(task, &quot;_removed.csv&quot;, sep = &quot;&quot;) ## Set Data Cleaning Params acc_criteria &lt;- -2 # Remove subjects with accuracy 2 SDs below the mean outlier_criteria &lt;- 2.0 # Trim outliers 2 SDs from the mean ############### #### Import #### data_import &lt;- read_csv(here(import_dir, import_file)) %&gt;% filter(TrialProc == &quot;real&quot;) ################ #### Score Data #### data_scores &lt;- data_import %&gt;% group_by(Subject, SetSize) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = &quot;SetSize&quot;, values_from = c(&quot;CR&quot;, &quot;H&quot;, &quot;k&quot;)) %&gt;% mutate(VA_k = (k_5 + k_7) / 2) #################### #### Clean Data #### data_remove &lt;- data_import %&gt;% group_by(Subject) %&gt;% summarise(Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(ACC.mean_z = scale(Accuracy.mean, center = TRUE, scale = TRUE)[,1]) %&gt;% filter(ACC.mean_z &lt; acc_criteria) data_scores &lt;- filter(data_scores, !(Subject %in% data_remove$Subject)) data_outliers &lt;- data_scores %&gt;% mutate(VA_k_z = scale(VA_k, center = TRUE, scale = TRUE)) %&gt;% filter(VA_k_z &gt;= outlier_criteria | VA_k_z &lt;= -1 * outlier_criteria) data_scores &lt;- filter(data_scores, !(Subject %in% data_outliers$Subject)) #################### #### Calculate Reliability #### reliability &lt;- data_import %&gt;% filter(Subject %in% data_scores$Subject) %&gt;% group_by(Subject, SetSize) %&gt;% mutate(Index = row_number(), Split = ifelse(Index %% 2, &quot;odd&quot;, &quot;even&quot;)) %&gt;% group_by(Subject, SetSize, Split) %&gt;% summarise(CR.n = sum(CorrectRejection, na.rm = TRUE), FA.n = sum(FalseAlarm, na.rm = TRUE), M.n = sum(Miss, na.rm = TRUE), H.n = sum(Hit, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(CR = CR.n / (CR.n + FA.n), H = H.n / (H.n + M.n), k = SetSize * (H + CR -1)) %&gt;% pivot_wider(id_cols = &quot;Subject&quot;, names_from = c(&quot;SetSize&quot;, &quot;Split&quot;), values_from = c(&quot;CR&quot;, &quot;H&quot;, &quot;k&quot;)) %&gt;% mutate(VA_k_even = (k_5_even + k_7_even) / 2, VA_k_odd = (k_5_odd + k_7_odd) / 2) %&gt;% summarise(r = cor(VA_k_even, VA_k_odd)) %&gt;% mutate(r = (2 * r) / (1 + r)) data_scores$VA_splithalf &lt;- reliability$r ############################### #### Output #### write_csv(data_scores, here(output_dir, output_file)) ################ #### Append to Log #### log_header &lt;- paste(&quot;===== Visual Arrays: 1_visualarrays_score.R =====\\n\\n&quot;, format(Sys.Date(), &quot;%B %d %Y&quot;), &quot;\\n&quot;, sep = &quot;&quot;) log_init_n &lt;- paste(&quot;-- &quot;, length(unique(data_import$Subject)), &quot;\\t Initial subject count&quot;, sep = &quot;&quot;) log_remove &lt;- paste(&quot;-- &quot;, length(unique(data_remove$Subject)), &quot;\\t Number of problematic subjects removed&quot;, sep = &quot;&quot;) log_outlier &lt;- paste(&quot;-- &quot;, length(unique(data_outliers$Subject)), &quot;\\t Number of outliers removed&quot;, sep = &quot;&quot;) log_final_n &lt;- paste(&quot;-- &quot;, length(unique(data_scores$Subject)), &quot;\\t Final subject count&quot;, &quot;\\n&quot;, sep = &quot;&quot;) table_remove &lt;- kable(data_remove, digits = 2, format = &quot;markdown&quot;) table_outlier &lt;- kable(data_outliers, digits = 2, format = &quot;markdown&quot;) log_footer &lt;- &quot;==================================================\\n&quot; sink(file = here(&quot;log.txt&quot;), append = FALSE, split = TRUE) cat(log_header, log_init_n, log_remove, log_outlier, log_final_n, print(table_remove), &quot;\\n&quot;, print(table_outlier), &quot;\\n&quot;, log_footer, fill = FALSE, sep = &quot;\\n&quot;) sink() ####################### rm(list=ls()) "],
["data-analysis-single-merged-file.html", "14 Data Analysis: Single Merged File 14.1 Overview 14.2 Set up 14.3 Import 14.4 Select only important variables 14.5 Output 14.6 masterscript.R", " 14 Data Analysis: Single Merged File 14.1 Overview The two steps required to get the data ready for statistical analysis are: Create a scored data file for each task At this stage we also remove any subjects that are suspect or have too poor of performance, and remove univariate outliers. Merge the scored data files into a single data file At this stage we also can create composite factors if needed. The first step was covered in the previous chapter. The second step will be covered in this chapter. However, I will not provide an example as it would require you to download several files and organize them… but I will explain how the script template works for creating a single merged file. As with all the other script templates we have used there are 4 sections in the merge script: Setup This is where any required packages are loaded and the import/output file directories and names are set Import files This is where the multiple _Scores.csv files are imported and merged Select only important variables and create composite scores This is where we select only the variables (columns) that we want to use for statistical analysis. We can also rename the variables to be shorter and more concise - this can make statistical analysis easier. This is also where we can create composite scores. Output Finally we need to save the single merged data file. 14.2 Set up This is what the Setup section looks like. We need the datawrangling package for the files_join() function. #### Set up #### ## Load packages library(here) library(datawrangling) # for files_join() library(dplyr) ## Set import/output directories import_dir &lt;- &quot;Data Files/Scored Data&quot; output_dir &lt;- &quot;Data Files&quot; output_file &lt;- &quot;name_of_datafile.csv&quot; ################ 14.3 Import The datawrangling::files_join() function becomes very useful here. This function will import multiple files, that contain the string \"Scores\", located in import_dir and merge them into a single data frame all in one line of code. #### Import Files #### data_import &lt;- files_join(here(import_dir), pattern = &quot;Scores&quot;, id = &quot;Subject&quot;) ###################### 14.4 Select only important variables We can simply use the select() function to keep only the variables we need for statistical analysis and also rename variables. #### Select only important variables #### data_merge &lt;- data_import %&gt;% select() ## Create list of final subjects subj.list &lt;- select(data_merge, Subject) ################################################################# Not shown here, but this would also be the place to create composite variables if needed using datawrangling::composite(). This function was explained in detail in Chapter 6. If you want to exclude subjects that have too much missing data across certain tasks this would be the place to do it. Finally, I think it is a good idea to create a data file that only contains one column - a list of subjects that have made it through to this stage of data cleaning and scoring. 14.5 Output The last thing to do is save data_merge and subj.list. #### Output #### write_csv(data_merge, here(output_dir, output_file)) write_csv(subj.list, here(output_dir, &quot;subjlist_final.csv&quot;)) ################ rm(list=ls()) 14.6 masterscript.R The masterscript template for data analysis is explained in Chapter 9. This is what it looks like: *Notice the Create/Clear log file line - this only required if you are going to append log information from each of the _score.R script files.* ## Data Analysis for StudyName ################################################# #------ 1. &quot;tidy&quot; raw data to Scored data ------# ################################################# ## Create/Clear log file write(paste(&quot;log: &quot;, format(Sys.Date(), &quot;%B %d %Y&quot;), &quot;\\n&quot;, sep = &quot;&quot;), file = &quot;Data Files/Scored Data/log.txt&quot;, append = FALSE) source(&quot;R Scripts/1_taskname_score.R&quot;, echo=TRUE) rm(list=ls()) ############################################################# #------ 2. Create Final Merged Data File for Analysis ------# ############################################################# source(&quot;R Scripts/2_merge.R&quot;, echo=TRUE) rm(list=ls()) ############################### #------ 3. Data Analysis ------# ############################### library(rmarkdown) render(&quot;R Scripts/3_MainAnalyses.Rmd&quot;, output_dir = &quot;Results&quot;, output_file = &quot;MainAnalyses.html&quot;) rm(list=ls()) ################################################# The organizational structure and workflow for data analysis is depcited here: You start with only the task_raw.csv files located in Data Files/Raw Data, copied over from the Data Collection directory. The 1_task_score.R scripts imports a task_raw.csv file and performs data cleaning and scoring to create a task_Scores.csv file located in Data Files/Scored Data. The 2_merge.R script merges all the task_Scores.csv files together into one Merged_Data.csv located in Data Files. This file is ready for statistical analysis, it will have all the variables you are interested in and univariate outliers removed. The 3_Analysis.Rmd is an R Markdown script document for conducting statistical analyses and data visualization on Merged_Data.csv. The output of this script document is an Analysis.html results output file located in Results "],
["data-visualization-overview.html", "Data Visualization: Overview 14.7 R Markdown", " Data Visualization: Overview 14.7 R Markdown Phew! You’ve made it this far, good job. Up until now you have been learning how to do data preparation steps in R. Now for the fun part, statistical analyses and data visualization! This is the third and final step in the data workflow process depicted above. Traditionally you have likely done these analyses in SPSS or EQS and have created figures in Excel or PowerPoint. The rest of the guide will cover how to do these steps in R. Writing scripts to do statistical analyses is an entirely different process than writing scripts for data preparation. Therefore, we should first go over the general process of conducting and outputting statistical analyses in R. In programs like SPSS when you run a statistical analysis, it will be outputted to a viewable .spv document. One downfall of this is that .spv files are propriety format so can only be opened if you have SPSS installed. However, there is the option to export a .spv file as a PDF. One downfall about R is that unlike SPSS, there is not a native way to create output documents from statistical analyses. Fortunately, RStudio has an output document format called R Markdown. 14.7.1 What is an R Markdown File? R Markdown is a powerful way to create reports of statistical analyses. Reports can be outputted in a lot of different formats; html, Microsoft Word, PDF, presentation slides, and more. In fact, this guide was created using R Markdown. The easiest format to output as is html. html documents are opened in a web browser and therefore can be opened on any computer and device (phones, tablets, Windows, Mac). Follow this link for a brief Intro to R Markdown First, you need to install the rmarkdown package install.packages(&quot;rmarkdown&quot;) To open an R Markdown document go to File -&gt; New File -&gt; R Markdown… Select HTML and click OK An example R Markdown document will open. Go ahead and read the contents of the document. There are three types of content in an R Markdown document: A YAML header R code chunks Formatted text 14.7.2 YAML header The YAML header contains metadata about how the document should be rendered and the output format. It is located at the very top of the document and is surrounded by lines of three dashe, --- title: &quot;Title of document&quot; output: html_document --- There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html 14.7.3 R code chunks Unlike a typical R script file (.R), an R Markdown document (.Rmd) is a mixture of formatted text and R code chunks. Not everything in an R Markdown document is executed in the R console, only the R code chunks. To run chunks of R code you can click on the green “play” button on the top right of the R code chunk. Go ahead and do this for the three R code chunks in the R Markdown document you opened. (cars and pressure are just example data frames that come pre-loaded with R). We have not gone over these functions yet, but you can see that the results of the R code are now displayed in the document. The first R code chunk is just setting some default options of how the output of R code chunks should be displayed. We will cover these options in more detail later. 14.7.4 Formatted text The formatted text sections are more than just adding comments to lines of code. You can write up descriptive reports, create bulletted or numbered lists, embed images or web links, create tables, and more. The text is formatted using a language known as Markdown, hence the name R Markdown. Markdown is a convenient and flexible way to format text. When a Markdown document is rendered into some output (such as html or PDF), the text will be formatted as specified by the Markdown syntax. In the R Markdown document you have open you can see some Markdown syntax. The pound signs ## at the beginning of a line are used to format headers. One # is a level one header, two ## is a level two header and so on. Also notice in the second paragraph, the word Knit is surrounded by two asterisks on each side. When this document is rendered, the word Knit will be bold. Go ahead and render the R Markdown document by clicking on the Knit button at the top of the window. Once it is done rendering you will see a new window pop up. This is the outputted html file. You can see how the document has formatted text based on the Markdown syntax. There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. Here is one I reference often: Markdown Cheatsheet "],
["fundamentals-of-data-visualization.html", "15 Fundamentals of Data Visualization 15.1 Grammar of Graphics 15.2 Plotting Functions in R", " 15 Fundamentals of Data Visualization Data visualization is an essential skill for anyone working with data. It is a combination of statistical understanding and design principles. In this way, data visualization is about graphical data analysis and communication and perception. Data visualization is often times glossed over in our stats courses. This is unfortunate because it is so important for better understanding our data, for communicating our results to others, and frankly it is too easy to create poorly designed visualizations. As a scientist, there are two purposes for visualizing our data. Data exploration: it is difficult to fully understand our data just by looking at numbers on a screen arranged in rows and columns. Being skilled in data visualization will help you better understand your data. Explain and Communicate: You will also need to explain and communicate your results to colleagues or in scientific publications. The same data visualization principles apply to both purposes, however for communicating your results you may want to place more emphasis on aesthetics and readability. For data exploration your visualizations do not have to be pretty. 15.1 Grammar of Graphics Leland Wilkinson (Grammar of Graphics, 1999) formalized two main principles in his plotting framework: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings The essential grammatical elements to create any visualization are: 15.2 Plotting Functions in R It is possible to create plots in R using the base R function plot(). The neat thing about plot() is that it is really good at knowing what kind of plot you want without you having to specify. However, these are not easy to customize and the output is a static image not an R object that can be modified. To allow for data visualization that is more in line with the principles for a grammar of graphics, Hadley Wickham (pictured below) created the ggplot2 package. This by far the most popular package for data visualization in R. "],
["introduction-to-ggplot2.html", "16 Introduction to ggplot2 16.1 Grammar of Graphics 16.2 Data layer 16.3 Aesthetic Layer 16.4 Geometries Layer 16.5 Facets Layer 16.6 Statistics Layer 16.7 Coordinates Layer 16.8 Themes Layer", " 16 Introduction to ggplot2 16.1 Grammar of Graphics We saw from the last chapter that the two main components in a grammar of graphics are: Graphics = distinct layers of grammatical elements Meaningful plots through aesthetic mappings We also saw that the three essential elements are the data layer, aesthetics layer, and geometrics layer. In ggplot2 there are a total of 7 layers we can add to a plot 16.2 Data layer The Data Layer specifies the data being plotted. Let’s see what this means more concretely with an example data set. A very popular data set used for teaching data science is the iris data set. In this data set various species of iris were measured on their sepal and petal length and width. This data set actually comes pre-loaded with R, so you can simply view it by typing in your console View(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa We can see that this data is in wide format. What type of graph we can visualize will depend on the format of the data set. On occasion, in order to visualize a certain pattern of the data will require you to change the formatting of the data. Let’s go ahead and start building our graphical elements in ggplot2. Load the ggplot2 library. Then: library(ggplot2) ggplot(data = iris) You can see that we only have a blank square. This is because we have not added any other layers yet, we have only specified the data layer. 16.3 Aesthetic Layer The next grammatical element is the aesthetic layer, or aes for short. This layer specifies how we want to map our data onto the scales of the plot The aesthetic layer maps variables in our data onto scales in our graphical visualization, such as the x and y coordinates. In ggplot2 the aesthetic layer is specified using the aes() function. Let’s create a plot of the relationship between Sepal.Length and Sepal.Width, putting them on the x and y axis respectively. ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) You can see we went from a blank box to a graph with the variable and scales of Sepal.Length mapped onto the x-axis and Sepal.Width on the y-axis. However, there is no data yet :( What are we to do? 16.4 Geometries Layer The next essential element for data visualization is the geometries layer or geom layer for short. Just to demonstrate to you that ggplot2 is creating R graphic objects that you can modify and not just static images, let’s assign the previous graph with data and aesthetics layers only onto an R object called p, for plot. p &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) Now let’s say we want to add the individual raw data points to create a scatterplot. To do this we can use the function geom_point(). This is a geom layer and the type of geom we want to add are points. In ggplot2 there is a special notation that is similar to the pipe operator %&gt;% seen before. Except it is plus sign + p + geom_point() And walla! Now we have a scatterplot of the relationship between Sepal.Length and Sepal.Width. Cool. If we look at the scatterplot it appears that there are at least two groups or clusters of points. These clusters might represent the different species of flowers, represented in the Species column. There are different ways we can visualize or separate this grouping structure. First, we will consider how to plot these species in separate plots within the same visualization. 16.5 Facets Layer The facet layer allows you to create subplots within the same graphic object The previous three layers are the essential layers. The facet layer is not essential, however given your data you may find it helps you to explore or communicate your data. Let’s create facets of our scatterplot by Species ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) 16.6 Statistics Layer The statistics layer allows you plot statistical values calculated from the data So far we have only plotted the raw data values. However, we may be interested in plotting some statistics or calculated values, such as a regression line, means, standard error bars, etc. Let’s add a regression line to the scatterplot. First without the facet layer then with the facet layer ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) 16.7 Coordinates Layer The coordinate layer allows you to adjust the x and y coordinates You can adjust the min and max values, as well as the major ticks. This is more useful when you have separate graphs (non-faceted) and you want to plot them on the same scale for comparison. This is actually a very important design principle in data visualization. If you want to compare two separate graphs, then they need to be on the same scale!!! library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) library(dplyr) ggplot(filter(iris, Species == &quot;setosa&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = c(4, 8), ylim = c(2, 5)) ggplot(filter(iris, Species == &quot;versicolor&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = c(4, 8), ylim = c(2, 5)) ggplot(filter(iris, Species == &quot;virginica&quot;), aes(Sepal.Length, Sepal.Width)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(xlim = c(4, 8), ylim = c(2, 5)) Just a note. I highly suggest not using scale_x_continuous() or scale_y_continuous() functions. The coord_cartesian() function is like zooming in and out of the plot area. The scale_ functions actually change the shape of the data and statistics layers. If a data point falls outside of the scale limits then it will be removed from any statistical analyses (even if the individual data points are not plotted geom_point()) 16.8 Themes Layer The Themes Layer refers to all non-data ink. You can change the labels of x or y axis, add a plot title, modify a legend title, add text anywhere on the plot, change the background color, axis lines, plot lines, etc. There are three types of elements within the Themes Layer; text, line, and rectangle. Together these three elements can control all the non-data ink in the graph. Underneath these three elements are sub-elements and this can be represented in a hierarchy such as: For instance, you can see that you can control the design of the text for the plot title and legend title theme(title = element_text()) or individually with theme(plot.title = element_text(), legend.title = element_text()). Any text element can be modified with element_text() Any line element can be modified with element_line() Any rect element can be modified with element_rect() You can then control different features such as the color, linetype, size, font family, etc. As an example let’s change some theme elements to our facet plot. Let’s change the axis value labels to red font and increase the size ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text = element_text(color = &quot;red&quot;, size = 14)) Now let’s only change the x-axis text and not the y-axis text. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) It is a good idea to have a consistent theme across all your graphs. And so you might want to just create a theme object that you can add to all your graphs. a_theme &lt;- theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14), panel.grid = element_blank(), panel.background = element_rect(fill = &quot;pink&quot;)) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + a_theme 16.8.1 Built-in Themes For the most part you can probably avoid the theme() function by using built-in themes, unless there is a specific element you want to modify. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(axis.text.x = element_text(color = &quot;red&quot;, size = 14)) + theme_linedraw() You can also set a default theme for the rest of your ggplots at the top of your script. That way you do not have to keep on specifying the theme for evey ggplot. theme_set(theme_linedraw()) Now you can create a ggplot with theme_linedraw() without specifying theme_linedraw() every single time. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + facet_wrap(~ Species) + stat_smooth(method = &quot;lm&quot;, se = FALSE) You can do a google search to easily find different types of theme templates. I personally like theme_linedraw() "],
["scatterplots.html", "17 Scatterplots 17.1 Scatterplots", " 17 Scatterplots You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) The main type of plots we typically want to create in psychological science are: Scatterplots Bar graphs Line graphs Histograms 17.1 Scatterplots We have already spent a good amount of time creating scatterplots using stat_smooth() and/or geom_smooth(). These two functions are essentially identical. In fact, many of the geom_ functions are just wrappers around stat_ functions. The scatterplot we created from last chapter is essentially an interaction plot. The interaction of Species x Sepal.Length on Sepal.Width. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) For modeling an interaction effect in regression it is easier to interpret if the lines extend to all possible values - not just across the values within a group. We can do this by specifying the argument geom_smooth(fullrange = TRUE) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) Now what if the moderator was a continuous variable and not categorical like Species? We would want to set the color aesthetic to be on +/- 1 SD on the mean. How would we go about doing this? The answer is: It would be very difficult to do so. This is where the function plot_model() from the sjPlot package comes in handy. 17.1.1 Adding other geoms There might be other geoms we want to add to a scatterplot. Let’s add some summary statistics to the graph. Specifically, a horizontal dashed line representing the mean on Sepal.Width and a vertical dashed line representing the mean on Sepal.Length. To make it more simple let’s only do this for Species == \"setosa\". library(dplyr) iris_means &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% mutate(Sepal.Width_mean = mean(Sepal.Width, na.rm = TRUE), Sepal.Length_mean = mean(Sepal.Length, na.rm = TRUE)) ggplot(iris_means, aes(Sepal.Length, Sepal.Width)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + geom_hline(aes(yintercept = Sepal.Width_mean), linetype = &quot;dashed&quot;, color = &quot;red4&quot;) + geom_vline(aes(xintercept = Sepal.Length_mean), linetype = &quot;dashed&quot;, color = &quot;green4&quot;) "],
["plotting-means.html", "18 Plotting Means 18.1 Bar Graphs 18.2 Alternatives to Bar Graphs 18.3 Two-Way Interaction Plots 18.4 Three-Way Interaction Plots", " 18 Plotting Means You can go ahead and set a default theme for your plots theme_set(theme_linedraw()) 18.1 Bar Graphs Bar graphs are the standard. They are ubiquitous across psychology. Basically everyone uses them. But in all honesty, Bar graphs SUCK!. The worst part about them is that they hide the distribution of the raw data points (even when error bars are included). Even worse, too often you will see bar graphs with NO ERROR BARS! Yikes! A bar graph with no error bars tells you almost NOTHING! To illustrate this let’s use a data set containing information on mammalian sleep patterns from the data set msleep. head(msleep) ## # A tibble: 6 x 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chee… Acin… carni Carn… lc 12.1 NA NA 11.9 NA 50 ## 2 Owl … Aotus omni Prim… &lt;NA&gt; 17 1.8 NA 7 0.0155 0.48 ## 3 Moun… Aplo… herbi Rode… nt 14.4 2.4 NA 9.6 NA 1.35 ## 4 Grea… Blar… omni Sori… lc 14.9 2.3 0.133 9.1 0.00029 0.019 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 0.667 20 0.423 600 ## 6 Thre… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 0.767 9.6 NA 3.85 Let’s plot the relationship between the different eating habits (vore) and total sleep time (sleep_total). msleep1 &lt;- filter(msleep, !is.na(vore)) ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) This only tells us what the means are. We have no idea about the distributions. Well for this reason people usually like to see error bars. Okay well let’s add error bars. ggplot(msleep1, aes(vore, sleep_total)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2) Okay better. But we still cannot see the underlying distribution. 18.2 Alternatives to Bar Graphs Here is a crazy idea. What if we plotted the raw data points. Like we do with scatterplots! Whoa! What a concept ggplot(msleep1, aes(vore, sleep_total)) + geom_point() When plotting raw data points with categorical variables on the x-axis it makes more sense to jitter the points so they are not all just lying on top of each other. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) Wow! Does this give you a completely different picture than the bar graph with error bars? It does to me! Especially look at the insecti and omni eating habits. There is definitely a bi-modal distribution happening there. From the bar graph with error bars, we might be fooled into thinking that the distributions for carni and omnie are pretty similar. But are they? Not at all! THIS IS WHY YOU SHOULD ALWAYS PLOT THE RAW DATA POINTS But means and error bars are also useful information so let’s add those ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2)) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) Another aesthetic option that is useful when we are plotting means and error bars on top of raw data is the alpha aesthetic. This can allow us to make the raw data points more transparent, fade into the background a little more. ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) ggplot(msleep1, aes(vore, sleep_total)) + geom_point(position = position_jitter(width = .2), alpha = .3) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, color = &quot;dodgerblue&quot;, size = 4, shape = &quot;diamond&quot;) + stat_summary(fun.data = mean_cl_normal, na.rm =TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;dodgerblue&quot;) + stat_summary(fun.y = mean, na.rm = TRUE, aes(group = 1), geom = &quot;line&quot;, color = &quot;dodgerblue&quot;, size = .75, shape = &quot;diamond&quot;) 18.3 Two-Way Interaction Plots library(tidyr) iris.long &lt;- iris %&gt;% mutate(Flower = row_number()) %&gt;% gather(&quot;Part&quot;, &quot;Inches&quot;, -Flower, -Species) %&gt;% separate(Part, into = c(&quot;Part&quot;, &quot;Measurement&quot;)) %&gt;% arrange(Flower, Species) %&gt;% select(Flower, Species, Part, Measurement, Inches) head(iris.long) ## Flower Species Part Measurement Inches ## 1 1 setosa Sepal Length 5.1 ## 2 1 setosa Sepal Width 3.5 ## 3 1 setosa Petal Length 1.4 ## 4 1 setosa Petal Width 0.2 ## 5 2 setosa Sepal Length 4.9 ## 6 2 setosa Sepal Width 3.0 ggplot(iris.long, aes(Measurement, Inches, group = Species, color = Species)) + geom_point(position = position_jitterdodge(jitter.width = .2, dodge.width = .7), alpha = .1) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, shape = &quot;diamond&quot;, size = 4, color = &quot;black&quot;, position = position_dodge(width = .7)) + stat_summary(fun.data = mean_cl_normal, na.rm = TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;black&quot;, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set1&quot;) 18.4 Three-Way Interaction Plots Just add facet_wrap(~ Part) after the first ggplot() line. ggplot(iris.long, aes(Measurement, Inches, group = Species, color = Species)) + facet_wrap(~ Part) + geom_point(position = position_jitterdodge(jitter.width = .2, dodge.width = .7), alpha = .1) + stat_summary(fun.y = mean, na.rm = TRUE, geom = &quot;point&quot;, shape = &quot;diamond&quot;, size = 4, color = &quot;black&quot;, position = position_dodge(width = .7)) + stat_summary(fun.data = mean_cl_normal, na.rm = TRUE, geom = &quot;errorbar&quot;, width = .2, color = &quot;black&quot;, position = position_dodge(width = .7)) + scale_color_brewer(palette = &quot;Set1&quot;) Therefore, you can see how to plot interactions using group/color and facet_wrap(). "],
["univariate-plots.html", "19 Univariate Plots", " 19 Univariate Plots This will be a chapter on univariate plots "],
["statistical-analysis-overview.html", "Statistical Analysis: Overview 19.1 R Markdown", " Statistical Analysis: Overview 19.1 R Markdown This section will cover what RMarkdown documents are and how to use them. Coming Soon! For a brief intro to R Markdown see https://rmarkdown.rstudio.com/lesson-1.html There are various metadata options you can specify, such as if you want to include a table of contents. To learn about a few of them see https://bookdown.org/yihui/rmarkdown/html-document.html There are a lot of guides on how to use Markdown syntax. I will not cover this so you should check them out on your own. One guide that I frequently reference is https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet "],
["statistics-in-r.html", "20 Statistics in R", " 20 Statistics in R This will be an introduction to statistical analysis in R "],
["structural-equation-modelling.html", "21 Structural Equation Modelling 21.1 lavaan 21.2 semoutput", " 21 Structural Equation Modelling By far the most common statistical analyses we do in this lab are confirmatory factor analysis (CFA) and structural equation modeling (SEM). This Chapter will cover how to conduct CFA’s and SEM’s in R using the lavaan package. 21.1 lavaan Install the lavaan package install.packages(&quot;lavaan&quot;) Visit the lavaan website and navigate to the Tutorial tab. This is an excellent resource for you to consult if you forget any syntax or want more details on using lavaan. You should go over the full tutorial yourself, but I will go ahead and cover the basics here. There are only two main steps to run a lavaan model. Build the model object Run the model with cfa() or sem() 21.1.1 Building the model object The model object is where you specify the model equation for the CFA or SEM. It is actually very easy and intuitive to do. Basically you specify the model equation within single quotes and pass it to an object called model. Let’s say we have want to run a model corresponding to this model diagram: We would simply specify: model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; This defines a CFA model with three latent factors; visual, textual, and speed with their respective indicators. The indicators need to correspond to column names in the data frame. There are certain defaults that lavaan uses so that we do not have to specify every single path in the model. For instance, by default it will add correlations between the latent factors in a CFA model. That is why in the model example above, the latent correlations are not explicit, yet they are implicitly part of the model. Model Syntax formula type operator | mnemonic latent variable definition =~ | is measured by regression ~ | is re is regressed on variance/covariance ~~ | is correlated with new parameter := is defined by 21.1.2 Run the model Then the model can be ran using cfa() or sem() functions fit &lt;- cfa(model, data) The first two arguments to pass onto the lavaan functions are model and data, respectively. There are other important arguments that we will cover later. The summary(fit, fit.measures = TRUE, standardized = TRUE) output to a lavaan model looks like ## lavaan 0.6-7 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 301 ## ## Model Test User Model: ## ## Test statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 918.852 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.931 ## Tucker-Lewis Index (TLI) 0.896 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3737.745 ## Loglikelihood unrestricted model (H1) -3695.092 ## ## Akaike (AIC) 7517.490 ## Bayesian (BIC) 7595.339 ## Sample-size adjusted Bayesian (BIC) 7528.739 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.092 ## 90 Percent confidence interval - lower 0.071 ## 90 Percent confidence interval - upper 0.114 ## P-value RMSEA &lt;= 0.05 0.001 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual =~ ## x1 1.000 0.900 0.772 ## x2 0.554 0.100 5.554 0.000 0.498 0.424 ## x3 0.729 0.109 6.685 0.000 0.656 0.581 ## textual =~ ## x4 1.000 0.990 0.852 ## x5 1.113 0.065 17.014 0.000 1.102 0.855 ## x6 0.926 0.055 16.703 0.000 0.917 0.838 ## speed =~ ## x7 1.000 0.619 0.570 ## x8 1.180 0.165 7.152 0.000 0.731 0.723 ## x9 1.082 0.151 7.155 0.000 0.670 0.665 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual ~~ ## textual 0.408 0.074 5.552 0.000 0.459 0.459 ## speed 0.262 0.056 4.660 0.000 0.471 0.471 ## textual ~~ ## speed 0.173 0.049 3.518 0.000 0.283 0.283 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.549 0.114 4.833 0.000 0.549 0.404 ## .x2 1.134 0.102 11.146 0.000 1.134 0.821 ## .x3 0.844 0.091 9.317 0.000 0.844 0.662 ## .x4 0.371 0.048 7.779 0.000 0.371 0.275 ## .x5 0.446 0.058 7.642 0.000 0.446 0.269 ## .x6 0.356 0.043 8.277 0.000 0.356 0.298 ## .x7 0.799 0.081 9.823 0.000 0.799 0.676 ## .x8 0.488 0.074 6.573 0.000 0.488 0.477 ## .x9 0.566 0.071 8.003 0.000 0.566 0.558 ## visual 0.809 0.145 5.564 0.000 1.000 1.000 ## textual 0.979 0.112 8.737 0.000 1.000 1.000 ## speed 0.384 0.086 4.451 0.000 1.000 1.000 Yikes!! You really should learn to understand this output, but nicer looking output would be nice right? This is where my semoutput package comes in handy. 21.2 semoutput If you have not done so already open the RStudio project file for this tutorial. Download the PoliticalDemocracy dataset used in the lavaan tutorial Save it to the folder Data Files of your project directory. Install semoutput devtools::install_github(&quot;dr-JT/semoutput&quot;) Additional packages you will need to have installed install.packages(&quot;sjPlot&quot;) install.packages(&quot;semPlot&quot;) Once you install semoutput you should Restart R by going to: Session -&gt; Restart R You can download an R Markdown template for doing CFA and SEM in lavaan. Go to: File -&gt; New File -&gt; R Markdown…From Template -&gt; CFA/SEM (lavaan) 21.2.1 YAML Header At the top of the document is what is called the YAML header. Here is where you can specify certain parameters that you may want to use as default in your analyses. You also need to specify the location and name of the data file you will be working with params: import.file: &quot;&quot; # Relative file path to data mimic: &quot;lavaan&quot; # Which software program to mimic for estimating models missing: &quot;ML&quot; # How to deal with missing values: &quot;ML&quot; or &quot;listwise&quot; std.lv: TRUE # For CFAs, default setting whether to set latent variances to 1 or not std.ov: FALSE # Standardize all observed varialbes? se: &quot;standard&quot; # How to calcualte standard errors: &quot;standard&quot; or &quot;bootstrap&quot; bootstrap: 1000 # If se = &quot;bootstrap&quot; how many boostrap samples? skipping the efa example for now The first R code chunk Required Packages is where you should load any packages used in the document. The next R code chunk is where the data file is imported. The next two R code chunk’s print out a descriptive and correlational tables. The next section is a template for conducting an exploratory factor analysis with psych::fa(). Let’s skip this for now. The next two sections are templates for CFA and SEM using lavaan. For each CFA and SEM section there are 4 subsections. The “Summary Output” subsection displays nice looking tables summarizing the model results The “Diagram Output” subsection will display a model diagram The “Residual Correlation Matrix” subsection will display the residual correlation matrix The “Full Output” subsection will display the results from summary() along with parameter estimates and modification indices. This way you can still get the full output from a lavaan model as it provides more information than the “Summary Output”. You can also add additional output to this section if you need more info about the model. 21.2.2 CFA Example Ultimately we will run the following SEM model. First let’s conduct a CFA of the model. The following error residuals are correlated: y1 and y5; y2 and y4; y2 and y6; y3 and y7; y4 and y8; y6 and y8 To correlate error residuals you would specify: y1 ~~ y5 Move down to the CFA section. First, you need to create a list of the latent factor labels (this is for the output and not running a lavaan model). factors &lt;- c(&quot;dem60&quot;, &quot;ind60&quot;, &quot;dem65&quot;) Then specify the model equation. The commented lines (e.g. # latent factors) are just optional and can be changed or removed. Remember, the factor correlations are implied. model &lt;- &#39; # latent factors # correlated errors # constraints &#39; model &lt;- &#39; # latent factors dem60 =~ y1 + y2 + y3 + y4 ind60 =~ x1 + x2 + x3 dem65 =~ y5 + y6 + y7 + y8 # correlated errors y1 ~~ y5 y2 ~~ y4 y2 ~~ y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 # constraints &#39; You do not need to change anything for cfa(). Unless you want to change some of the defaults you set in the YAML header. fit &lt;- cfa(model = model, data = data, mimic = params$mimic, missing = params$missing, std.lv = params$std.lv, std.ov = params$std.ov, se = params$se, bootstrap = params$bootstrap) Run this R code chunk by pressing the green arrow button. Then run each R code chunk in each subsection to print the output. sem_sig(fit) Table 21.1: Model Significance Sample.Size Chi.Square df p.value 75 38.125 35 0.329 sem_fitmeasures(fit) Table 21.1: Model Fit Measures CFI RMSEA RMSEA.Lower RMSEA.Upper AIC BIC 0.995 0.035 0 0.092 3179.582 3276.916 sem_factorloadings(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 21.1: Factor Loadings Standardized Latent Factor Indicator Loadings sig p Lower.CI Upper.CI SE z dem60 y1 0.850 *** 0 0.765 0.936 0.044 19.435 dem60 y2 0.717 *** 0 0.592 0.843 0.064 11.207 dem60 y3 0.722 *** 0 0.596 0.849 0.064 11.221 dem60 y4 0.846 *** 0 0.759 0.933 0.044 19.020 ind60 x1 0.920 *** 0 0.874 0.965 0.023 39.658 ind60 x2 0.973 *** 0 0.941 1.005 0.017 58.917 ind60 x3 0.872 *** 0 0.812 0.933 0.031 28.304 dem65 y5 0.808 *** 0 0.713 0.903 0.048 16.698 dem65 y6 0.746 *** 0 0.634 0.858 0.057 13.031 dem65 y7 0.824 *** 0 0.734 0.913 0.046 18.063 dem65 y8 0.828 *** 0 0.738 0.918 0.046 18.030 sem_factorcor(fit, factors = factors) Table 21.1: Latent Factor Correlations Factor 1 Factor 2 r sig p Lower.CI Upper.CI SE dem60 ind60 0.447 *** 0 0.242 0.652 0.105 dem60 dem65 0.967 *** 0 0.909 1.024 0.029 ind60 dem65 0.578 *** 0 0.403 0.753 0.089 sem_factorvar(fit, factors = factors) Table 21.1: Latent Factor Variance/Residual Variance Factor 1 Factor 2 var var.std sig p sem_rsquared(fit) Table 21.1: R-Squared Values Variable R-Squared y1 0.7232242 y2 0.5142640 y3 0.5217883 y4 0.7152245 x1 0.8461294 x2 0.9467924 x3 0.7606256 y5 0.6528920 y6 0.5565270 y7 0.6784378 y8 0.6853215 semPaths(fit, latents = factors, whatLabels = &quot;std&quot;, layout = &quot;tree2&quot;, rotation = 2, style = &quot;lisrel&quot;, optimizeLatRes = TRUE, intercepts = FALSE, residuals = TRUE, curve = 1, curvature = 2, sizeLat = 10, nCharNodes = 8, sizeMan = 11, sizeMan2 = 4, edge.label.cex = 1.2, edge.color = &quot;#000000&quot;) modificationIndices(fit, sort. = TRUE, minimum.value = 3) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 59 ind60 =~ y4 4.796 0.577 0.577 0.174 0.174 ## 60 ind60 =~ y5 4.456 0.559 0.559 0.215 0.215 ## 67 dem65 =~ y4 4.260 2.986 2.986 0.898 0.898 ## 72 y1 ~~ y3 3.771 0.849 0.849 0.274 0.274 ## 81 y2 ~~ x1 3.040 -0.155 -0.155 -0.200 -0.200 21.2.3 SEM Example Now let’s run the actual SEM model. Really the only difference is that we will add some regression paths factors &lt;- c(&quot;dem60&quot;, &quot;ind60&quot;, &quot;dem65&quot;) model &lt;- &#39; # latent factors dem60 =~ y1 + y2 + y3 + y4 ind60 =~ x1 + x2 + x3 dem65 =~ y5 + y6 + y7 + y8 # variances # covariances y1 ~~ y5 y2 ~~ y4 y2 ~~ y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 # regressions dem65 ~ dem60 + ind60 dem60 ~ ind60 &#39; fit &lt;- sem(model = model, data = data, mimic = params$mimic, missing = params$missing, std.lv = FALSE, std.ov = params$std.ov, se = params$se, bootstrap = params$bootstrap) sem_sig(fit) Table 21.2: Model Significance Sample.Size Chi.Square df p.value 75 38.125 35 0.329 sem_fitmeasures(fit) Table 21.2: Model Fit Measures CFI RMSEA RMSEA.Lower RMSEA.Upper AIC BIC 0.995 0.035 0 0.092 3179.582 3276.916 sem_factorloadings(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 21.2: Factor Loadings Standardized Latent Factor Indicator Loadings sig p Lower.CI Upper.CI SE z dem60 y1 0.850 *** 0 0.765 0.936 0.044 19.435 dem60 y2 0.717 *** 0 0.592 0.843 0.064 11.207 dem60 y3 0.722 *** 0 0.596 0.849 0.064 11.221 dem60 y4 0.846 *** 0 0.759 0.933 0.044 19.020 ind60 x1 0.920 *** 0 0.874 0.965 0.023 39.658 ind60 x2 0.973 *** 0 0.941 1.005 0.017 58.917 ind60 x3 0.872 *** 0 0.812 0.933 0.031 28.304 dem65 y5 0.808 *** 0 0.713 0.903 0.048 16.698 dem65 y6 0.746 *** 0 0.634 0.858 0.057 13.031 dem65 y7 0.824 *** 0 0.734 0.913 0.046 18.063 dem65 y8 0.828 *** 0 0.738 0.918 0.046 18.030 sem_paths(fit, standardized = TRUE, ci = &quot;standardized&quot;) Table 21.2: Regression Paths Standardized Predictor DV Path Values SE z sig p Lower.CI Upper.CI dem60 dem65 0.885 0.052 17.100 *** 0.000 0.784 0.987 ind60 dem65 0.182 0.073 2.498 0.013 0.039 0.325 ind60 dem60 0.447 0.105 4.267 *** 0.000 0.242 0.652 sem_factorcor(fit, factors = factors) Table 21.2: Latent Factor Correlations Factor 1 Factor 2 r sig p Lower.CI Upper.CI SE sem_factorvar(fit, factors = factors) Table 21.2: Latent Factor Variance/Residual Variance Factor 1 Factor 2 var var.std sig p dem60 dem60 3.956 0.800 *** 0.000 ind60 ind60 0.448 1.000 *** 0.000 dem65 dem65 0.172 0.039 0.434 sem_rsquared(fit) Table 21.2: R-Squared Values Variable R-Squared y1 0.7232243 y2 0.5142639 y3 0.5217879 y4 0.7152243 x1 0.8461294 x2 0.9467924 x3 0.7606255 y5 0.6528920 y6 0.5565263 y7 0.6784382 y8 0.6853214 dem60 0.1995522 dem65 0.9609949 semPaths(fit, latents = factors, whatLabels = &quot;std&quot;, layout = &quot;tree2&quot;, rotation = 2, style = &quot;lisrel&quot;, optimizeLatRes = TRUE, intercepts = FALSE, residuals = TRUE, curve = 1, curvature = 2, sizeLat = 10, nCharNodes = 8, sizeMan = 11, sizeMan2 = 4, edge.label.cex = 1.2, edge.color = &quot;#000000&quot;) "]
]
