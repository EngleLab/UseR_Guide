[
["data-cleaning-and-scoring.html", "Chapter 11 Data Cleaning and Scoring 11.1 Setup 11.2 Import 11.3 Data Cleaning and Scoring 11.4 Trimming 11.5 Calculate FlankerEffect 11.6 Remove Subjects 11.7 Calculate Binned Scores 11.8 Merge 11.9 Save data file 11.10 Masterscript", " Chapter 11 Data Cleaning and Scoring Stage 2 in the data processing workflow is data cleaning and scoring. In this Chapter you will go over an example of how to write an R Script for this stage of data processing. In the previous Chapter you created a tidy raw data file that had one row per trial (including practice trials). In this Chapter you will use the tidy raw data file to calculate the FlankerEffect for each Subject. You will end up with a data file that has only one row per Subject. The FlankerEffect is the difference of mean RTs on incongruent trials minus congruent trials. Save a new R script file as 2_flanker_score.R in the R Scripts folder There are 4 blocks of code to create in the script: Setup Import Clean and Score Output 11.1 Setup The Setup block for this script will be very similar to the Setup for the raw script you created in the previous Chapter. However, we will also add some parameters for data cleaning criteria. You can copy and paste the code from the raw script for the Setup block and then modify it. The import directory should be the raw data folder (the output of the raw script created last Chapter). The output directory should be the scored data folder. The import filename is Flanker_raw.csv (the outputed file from the raw script). The output filename is Flanker_Scores.csv. ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set Import/Output Filenames import.file &lt;- &quot;Flanker_raw.csv&quot; output.file &lt;- &quot;Flanker_Scores.csv&quot; ############# 11.1.1 Data Cleaning Parameters In the example we are about to go through we will implement the following data cleaning procedures when calculating the FlankerEffect. They will be implemented in the following order rt.min: Set RTs less than 200ms to missing (NA) and Accuracy to incorrect (0). rt.trim: Trim RTs. Replace Outlier RTs that are above or below 3.5 SDs of the mean, with values exactly at 3.5 SDs above or below the mean. This is evaluated for each Subject by each condition seprately. acc.criteria: Finally remove subjects that performed less than 3.5 standard deviations below the mean accuracy on any Condition (congruent or incongruent). Let’s add these data cleaning parameters to the Setup block ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set Import/Output Filenames import.file &lt;- &quot;Flanker_raw.csv&quot; output.file &lt;- &quot;Flanker_Scores.csv&quot; ## Set Data Cleaning Params rt.min &lt;- 200 rt.trim &lt;- 3.5 acc.criteria &lt;- -3.5 ############# You will now be able to use these objects rt.min, rt.trim, and acc.criteria when writing the code to actually do the data cleaning. This is useful because if you want to change these parameters you can do it right here in the Setup block rather than searching through all your code and figure out where you need to change the values. 11.2 Import The import block is very simple, especially since we are just importing a .csv file. ## Import #### import &lt;- read_csv(here(import.dir, import.file)) ## Parsed with column specification: ## cols( ## Subject = col_double(), ## TrialProc = col_character(), ## Trial = col_double(), ## Condition = col_character(), ## RT = col_double(), ## ACC = col_double(), ## Response = col_character(), ## TargetArrowDirection = col_character(), ## SessionDate = col_character(), ## SessionTime = col_time(format = &quot;&quot;) ## ) ############## 11.3 Data Cleaning and Scoring The next block is where we do the actual data cleaning and task scoring. This step is more complicated and often times requires some forethought. But we don’t always have the best forethought so you will likely re-write previous lines of code. One thing you must think about before writing the script for this stage is the statistical analyses you eventually plan on conducting. The type of statistical analyses you plan on conducting will determine the final dataframe you want to end up at in this stage of data preparation. What are the final dependent variables (or task scores) you want to calaculate? In the Flanker task there are several task scores we might want to calculate (in a regression context). FlankerEffect on RT: Mean reaction time difference between incongruent and congruent trials FlankerEffect on Accuracy: Mean accuracy difference between incongruent and congruent trials Flanker Binned Scores: A scoring method to combine accuracy and reaction time (an alternative to difference scores) 11.4 Trimming First we need to get rid of practice trials / keep only real trials. ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) ################################# Then, set RTs less than 200ms to missing (NA) and Accuracy to 0 using mutate() and ifelse() ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) ################################# And, Trim RTs, grouped by Subject and Condition using the trim() function from my datawrangling package. trim() is an easy way to trim values on a variable using a certain z-score cutoff. The main arguments to pass onto trim() are: variables: The column name that contains the values you want to trim cutoff: What z-score cutoff value you want to use replace: How you want to replace the outlier values. Options are, &quot;mean&quot;, &quot;cutoff&quot;, or &quot;NA You can use group_by() with trim() to trim independently for each Condition (congruent and incongruent). Always ungroup() afterwards. ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = rt.trim, replace = &quot;cutoff&quot;) %&gt;% ungroup() ################################# We will implement the third data cleaning procedure later. 11.5 Calculate FlankerEffect What we want to do is calculate both the FlankerEffect and FlankerBinned Scores. These are separate scoring procedures. The general approach we will take is to create two separate dataframes for each procedure, based off the data_trim. Then we will merge the two dataframes back into one. First calculate the FlankerEffect. We want to calcualte the FlankerEffect on RT using only Accurate trials. Since we also want to calculate the FlankerEffect on Accuracy we cannot just use a filter(). Instead we should mutate() the values in RT to be NA when Accuracy is 0. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) Next step is to calculate mean RT and mean Accuracy separately for congruent and incongruent trials. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() Because we used group_by(Subject, Condition), summarise() will calculate the mean RT and mean Accuracy separately for each Subject and each Condition. Always be sure to ungroup() afterwards. View the data frame. In the console type View(data_flanker) Now rather than having one row per trial, group_by() and summarise() has aggregated the data down to Subject x Condition. What we want to do is calculate the difference between incongruent and congruent conditions on RT.mean and Accuracy.mean. However, congruent and incongruent conditions are on separate rows. What we need to do is reshape the data so that there is a column for each Condition on RT.mean and Accuracy.mean. Our columns should be congruent_RT.mean incongruent_RT.mean congruent_Accuracy.mean incongruent_Accuracy.mean Typically, to reshape a data frame we would use the gather() and spread() functions from the tidyr package. However, these do not allow reshaping on more than one value column. We have two value columns, RT.mean and Accuracy.mean. Luckily I have created a function that can allow us to do this, reshape_spread() from my datawrangling package. The main arguments you need to specify are: variables: The column name that contains the key variables to spread on values: The column name(s) that hold the values to be used id: Which columns should be preserved (i.e. Subject) So we can add something like this ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), ACC.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;ACC.mean&quot;)) Now View() the data frame. There should now be only ONE row per Subject. We can now just use mutate() to calculate the difference between these columns to get the FlankerEffect. ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;)) %&gt;% mutate(FlankerEffect_RT = incongruent_RT.mean - congruent_RT.mean, FlankerEffect_ACC = incongruent_Accuracy.mean - congruent_Accuracy.mean) 11.6 Remove Subjects Next, we should implement the third data cleaning procedure listed above. acc.criteria: Finally remove subjects that performed less than 3.5 standard deviations below the mean accuracy on any Condition (congruent or incongruent). It is convenient to do it now because we have a column with mean Accuracy for congruent and incongruent trials. We also want to do this before applying the binning procedure. The approach I like to take with entirely removing subjects is to keep a record of those subjects in a data file somewhere. To do this we will 1) create a new data frame of subjects that will be removed and then 2) use a function I created, remove_save() from the datawrangling package. This function is a short hand way of doing two things at once. Removing the subjects from the full data file Saving the removed subjects to a specified directory. The criteria we are removing subjects based on are those who performed 3.5 SDs below the mean. So we first need to calculate a column of z-scores (on SD units), then filter those who are below 3.5 z-scores. ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) Then use remove_save(). The main arguments to specify are: x: the data frame that contains ALL subjects remove: the data frame that contains subjects to be removed output.dir: directory to output file with removed subjects to output.file: name of file with removed subjects I put the removed subjects in a folder called “removed” and name the file something like “Flanker_removed.csv”. ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) data_flanker &lt;- remove_save(data_flanker, data_remove, output.dir = here(output.dir, &quot;removed&quot;), output.file = paste(task, &quot;removed.csv&quot;, sep = &quot;_&quot;)) If any subjects were removed you should now see a folder called removed in the Scored Data folder with a file called “Flanker_removed.csv”. 11.7 Calculate Binned Scores Great! We have now calculated FlankerEffects scores and performed the data cleaning procedures. Now we need to calculate Binned scores. The data_flanker data frame is no longer in a format that we can calculate bin scores. We need to use the trimmed data frame that has trial level data. We should remove the poor performing subjects and Missing RTs. This step is actually really important for the binning procedure because bin scores are relative to other subjects in the data. ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), !(Subject %in% data_remove$Subject)) This is stating, keep only Trials without missing values on RT AND Subjects that are NOT (!) in data_remove. We also need to remove neutral trials to calculate bin scores. Bin scores are based on comparing one condition to a baseline condition. In this case we want to compare the incongruent condition to the baseline congruent condition. So we need to get rid of neutral conditions. ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) And finally calculate bin scores using bin_score() from the englelab package. The main arguments to specify are: x: The data frame rt.col: Column name that contains the reaction time data. Default = “RT” accuracy.col: Column name that contains the accuracy data. Default = “Accuracy” condition.col: Column name that contains the trial condition type. Default = “Condition” baseline.condition: The values that specify the baseline condition type: How should Bin trials be aggregated, “sum” or “mean”. Default = “mean” id: Column name that contains subject identifiers. Default = “Subject” ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) %&gt;% bin_score(baseline.condition = &quot;congruent&quot;, type = &quot;mean&quot;,) %&gt;% rename(FlankerBin = &quot;BinScore&quot;) Awesome! Now we have two data frames, one, data_flanker, with FlankerEffect scores and another, data_binned with FlankerBin scores. They both have one row per subject. 11.8 Merge Now we can merge these two data frames together using the merge() function from base R. ## Merge data_flanker &lt;- merge(data_flanker, data_binned, by = &quot;Subject&quot;, all = TRUE) Now view the data_flanker. It should be one row per subject and have columns for FlanekrEffect_RT, FlankerEffect_ACC, and FlankerBin. 11.9 Save data file And finally save the data file ## Output #### write_csv(data_flanker, here(output.dir, output.file)) ############## If we put it all together your R script should look something like: ## Setup #### ## Load Packages library(here) library(readr) library(dplyr) library(datawrangling) ## Set Import/Output Directories directories &lt;- readRDS(here(&quot;directories.rds&quot;)) import.dir &lt;- directories$raw output.dir &lt;- directories$scored ## Set Import/Output Filenames import.file &lt;- &quot;Flanker_raw.csv&quot; output.file &lt;- &quot;Flanker_Scores.csv&quot; ## Set Data Cleaning Params rt.min &lt;- 200 rt.trim &lt;- 3.5 acc.criteria &lt;- -3.5 ############# ## Data Cleaning and Scoring #### ## Trimming data_trim &lt;- import %&gt;% filter(TrialProc == &quot;real&quot;) %&gt;% mutate(RT = ifelse(RT &lt; rt.min, NA, RT), Accuracy = ifelse(RT &lt; rt.min, 0, Accuracy)) %&gt;% group_by(Subject, Condition) %&gt;% trim(variables = &quot;RT&quot;, cutoff = rt.trim, replace = &quot;cutoff&quot;) %&gt;% ungroup() ## Calculate Flanker Effect data_flanker &lt;- data_trim %&gt;% mutate(RT = ifelse(Accuracy == 0, NA, RT)) %&gt;% group_by(Subject, Condition) %&gt;% summarise(RT.mean = mean(RT, na.rm = TRUE), Accuracy.mean = mean(Accuracy, na.rm = TRUE)) %&gt;% ungroup() %&gt;% reshape_spread(variables = &quot;Condition&quot;, values = c(&quot;RT.mean&quot;, &quot;Accuracy.mean&quot;)) %&gt;% mutate(FlankerEffect_RT = incongruent_RT.mean - congruent_RT.mean, FlankerEffect_ACC = incongruent_Accuracy.mean - congruent_Accuracy.mean) ## Remove Subjects data_remove &lt;- data_flanker %&gt;% center(variables = c(&quot;congruent_Accuracy.mean&quot;, &quot;incongruent_Accuracy.mean&quot;, &quot;neutral_Accuracy.mean&quot;), standardize = TRUE) %&gt;% filter(congruent_Accuracy.mean_z &lt; acc.criteria | incongruent_Accuracy.mean_z &lt; acc.criteria | neutral_Accuracy.mean_z &lt; acc.criteria) data_flanker &lt;- remove_save(data_flanker, data_remove, output.dir = here(output.dir, &quot;removed&quot;), output.file = paste(task, &quot;removed.csv&quot;, sep = &quot;_&quot;)) ## Calculate Binned scores data_binned &lt;- data_trim %&gt;% filter(!is.na(RT), Condition != &quot;neutral&quot;, !(Subject %in% data_remove$Subject)) %&gt;% bin_score(baseline.condition = &quot;congruent&quot;, type = &quot;mean&quot;,) %&gt;% rename(FlankerBin = &quot;BinScore&quot;) ## Merge data_flanker &lt;- merge(data_flanker, data_binned, by = &quot;Subject&quot;, all = TRUE) ################################# ## Output #### write_csv(data_flanker, here(output.dir, output.file)) ############## rm(list=ls()) 11.10 Masterscript Now you can add lines of code in the manuscript to execute or source() the script “2_flanker_score.R”. Something "]
]
