# Project Organization

In this chapter I will explain a particular organizational method for data collection and data analysis projects.

----

## Data Collection vs. Data Analysis

In the EngleLab, we often conduct large-scale data collection studies in which there are many research projects going on at once. Although many of the tasks will be shared between these research projects, there will be a unique set of tasks for each project. This overlap in shared tasks can make it difficult to figure out how and when to separate data preparation and data analysis steps between these projects.

Essentially, every data analysis project should contain the raw (tidy) data files for the tasks relevant to that research project. From these raw data files, performance should be aggregated to calculate a *score* on each task. That way every data analysis project will be fully reproducible starting from the raw data files.

The **Data Collection** directory will serve as a central repository for the raw data files from the study. A new **Data Analysis** directory can be created by copying over the raw data files for the tasks relevant to that research project. This relationship between Data Collection and Data Analysis Repositories are depicted here:

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_relations.png"))
```

*In the past, we would create a "final" data file with task scores from all the main tasks and share this with each other to use for our own data analysis projects. The problem with this was that not only was the analysis not fully reproducible but it creates a lack of transparency as to how the scores were calculated and the data cleaned. This also does not allow one to estimate reliability since they will not have the raw data files.*

It is critical that you copy the raw data files only from the data collection directory to a data analysis directory. You do not copy data files from one data analysis directory to another.

You can copy R Scripts from one data analysis directory to another. For instance, if you or someone else already created an R Script (for a separate data analysis project) to score a particular task that you are also using, then there is no problem in copying the R Script.

**In general, you will probably copy R scripts from one project to another. The important point is that you are not copying _data files_ from one project to another.**

This is because you can reproduce the data files from your R Scripts but not vice-versa. If you copy data files you lose reproducibility and the transparency of how the data file was created. 

## Data Collection

Unfortunately, you have to actually collect data before you can start analyzing anything. Therefore, you start out with a single directory: **Data Collection**. At the start all the folders in this directory that you really need are:

* Tasks

* Documents

The **Tasks** folder is where the E-Prime task files are that will be used to administer each task to the subjects.

In our lab we typically have multiple *Sessions* and multiple *Tasks* in each session. As you begin data collection, **.edat** data files will start to accumulate in each *Task* folder.

**Documents** is where you may store various documents related to the study, such as a **Methods.docx** document describing each task in detail. This is an important document for archival purposes. Some of your other documents in this directory may not be as important for archival purposes, such as an informed consent form.

## Data Preparation

At some point you will need to start analyzing the data. However, you first need to prepare the data so that it is ready to analyze. There are several steps in this process and it can be quite tedious. Nevertheless, undergraduate RAs are trained on how to do most of these steps, so recruit their help. There are also [step-by-step instructions for Data Preparation.](http://englelab.gatech.edu/dataprep/){target="_blank"}

Once you are ready for **Data Preparation** you will need to create a **Data Files** and **R Scripts** folders in the Data Collection directory. 

There are two scenarios in which you may need to start processing and analyzing data:

- Before data collection has finished

- After data collection has finished

For both of these scenarios, you will start with messy raw data files in some file format. Messy raw data files are hard to understand, have poor column and value labels, contain way too many columns and rows, and are just hard to work with. Data preparation is all about getting raw data files that are easy to work with.

```{r echo=FALSE, eval = TRUE, out.width='35%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_messy-tidy.png"))
```

The end product of the data preparation stage is *tidy* raw data files. Tidy raw data files are easy to understand, have sensible column and value labels, contain only relevant columns and rows, and are very easy to work with.

The **Data Preparation** stage is only required because the data files created from the E-Prime or other software program are usually not in a format that is easy to use or understand. I am referring to this format as a **messy raw data** file. Also, there are typically other preparation steps one needs to take before they can start looking at the data. These might include merging individual subject data files and exporting the data to a non-proprietary format so we can import the data into R. The purpose of the data preparation stage is simply to create **tidy raw data** files from the messy raw data files.

**Tidy raw data** files are easy to use and understand. There will be one row per trial, column labels that are easy to understand (e.g. Trial, Condition, RT, Accuracy, etc.), and values in columns that make sense. If values in a column are categorical, then the category names will be used rather than numerical values. Ideally, someone not involved in the research project should be able to look at a tidy raw data file and understand what each column represents, and what the values in the column refer to.

## Data Analysis

Okay, now say you are ready to analyze some data! It is tempting to do your analysis in the original *Data Collection* directory where the data are already stored. I highly suggest not doing this. You will be mixing up a *Data Collection* directory with a *Data Analysis* directory. This distinction is particularly important when we conduct large-scale studies with many data analysis projects for a single data collection study.

Instead, you should copy over the *tidy* raw data files from the data preparation stage to a separate *Data Analysis* directory.

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_copyover.png"))
```

You also might as well create an **Archival Backup** of the **Data Collection** directory on some other hard drive. That way you are at less risk of a hard drive crashing and losing all your precious data.

In the **Data Analysis** directory you have three main folders:

* Data Files

* R Scripts

* Results

**Data Files** is where you will store tidy raw data files, scored data files, and a single merged data file ready for statistical analysis.

It is advisable to store all your **R scripts** in one single place. I also like to prefix them with numbers corresponding to the order they need to be executed - that way they will be organized in an easy to find way.

Finally, you should create a separate folder to hold all your outputs from statistical analysis and data visualization in a **Results** folder.

There are three main stages to data analysis; **1**) scoring, cleaning, and merging data **2**) data visualization, and **3**) statistical analyses. Data analysis tends to be more cyclic and iterative therefore you may end up going back and forth between these stages.

```{r echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_dataanalysis.png"))
```

The first stage takes the tidy raw data files from the data preparation stage and converts them into a scored data file, usually by aggregating performance across trials. Data cleaning procedures (such as removing outliers) also occurs during this stage. The format of the scored data file will depend on the type of statistical analysis one plans on performing.

The data analysis stages usually occurs in tandem with one another. Visualizing our data, running statistical analyses, visualizing our statistical models, etc. If we have more exploratory data, then based on these visualizations and analyses we may decide that we want to use different scoring or cleaning procedures. Or we want to explore our data to further understand our findings. We may then go back to the scoring and cleaning stage, and on and on. 

You may have other folders in your **Data Analysis** directory:

* Figures

* Manuscript

* Presentations

**Figures** is where any image files, that are used in a manuscript or presentations, are stored. You may also have a **PowerPoint** file stored here.

**Manuscript** is where the manuscript and any drafts for this project are stored.

**Presentations** is where any **PowerPoint** presentation files related to this project can be stored.

These other folders are more optional.

----

## `workflow` package

I will show you how to automatically create **Data Collection** and **Data Analysis** directories using RStudio Projects and my [**workflow** package](https://dr-jt.github.io/workflow/){target="_blank"}

The most important thing to remember is that you need to copy and paste the *tidy* **task_raw.csv** data files from a **Data Collection** directory to a *Data Analysis* directory - but never copy and paste data files from one _Data Analysis_ directory to another.

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_copyover.png"))
```

When should you create a separate **Data Analysis** directory? Basically, if the set of analyses is going to be it's own **Manuscript** then create a new directory. If the set of analyses (whether exploratory or supplemental) is part of a larger set of analyses already in the works for a manuscript then no need to create a separate directory. 

### Install

Install the `workflow` package

```{r eval = FALSE}
devtools::install_github("dr-JT/workflow")
```

### Create a New R Project

One of the features this package allows is for you to automatically setup the organization of a **Data Collection** or **Data Analysis** project.

Navigate to __File -> New Project... -> New Directory 

And browse until you see the option: __Research Study__

Click on that and you will see a dialogue box like this

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/workflow_proj_template.png"))
```

Here are what the different options mean:

* __Directory Name__: This will be the name of the folder for the study

* __Create project as subdirecotry of__: Select Browse and choose where the folder (Directory Name) should be located.

* __Repository Type__: **data collection** or **data analysis**. Depending on which one you choose it will create the corresponding directories and files:

```{r echo=FALSE, eval = TRUE, out.width='60%', fig.align='center'}
knitr::include_graphics(rep("images/workflows/repository_type.png"))
```

Notice that if you choose the **data collection** repository it will download a generic template for *converting "messy" raw data files to "tidy" raw data files*. And if you choose the **data analysis** repository it will download generic templates for *creating scored data files from "tidy" raw data files* and to *merge* the Scored data files into one final data file. 

* __# of Sessions__: How many sessions will the study have? This will create folders in the `Tasks` folder for each session. For instance, if there will be 4 sessions it will create the the folders "Session 1", "Session 2", "Session 3", and "Session 4". Obviously this is not needed for a **data analysis** repository.

* __Other Directories__: I talked earlier about some other folders you may want to include in a **Data Analysis** repository. Well you can automatically add them here. 

----

```{r eval = TRUE, echo=FALSE}
rm(list=ls())
```

